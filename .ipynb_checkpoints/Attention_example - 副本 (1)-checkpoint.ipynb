{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  seq2seq + Luong 注意力 进行中英文翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T07:51:42.718150Z",
     "start_time": "2022-05-15T07:51:42.330746Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "from collections import Counter#计数器\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence ,pack_padded_sequence,pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer#分词器\n",
    "#from autonotebook import tqdm as notebook_tqdm\n",
    "import gc\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = 0 #未知\n",
    "PAD_IDX = 1  #\n",
    "BATCH_SIZE = 64   \n",
    "EPOCHS  = 30\n",
    "DROPOUT = 0.2\n",
    "ENC_HIDDEN_SIZE = DEC_HIDDEN_SIZE = 100\n",
    "EMBED_SIZE = 100\n",
    "## 数据集文件\n",
    "train_en_file = 'en-zh/train.en'\n",
    "train_cn_file = 'en-zh/train.zh'\n",
    "test_en_file = 'en-zh/test.en'\n",
    "test_cn_file = 'en-zh/test.zh'\n",
    "save_file = 'model.pt'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T08:07:29.961999Z",
     "start_time": "2022-05-15T08:07:29.946390Z"
    }
   },
   "outputs": [],
   "source": [
    "#分词器\n",
    "tokenizer_en = get_tokenizer('basic_english')#按空格进行分割\n",
    "tokenizer_cn = get_tokenizer(jieba.lcut)#进行结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T08:19:00.252379Z",
     "start_time": "2022-05-15T08:19:00.242402Z"
    }
   },
   "outputs": [],
   "source": [
    "#加载文件\n",
    "\n",
    "\n",
    "def load_data(path,language):\n",
    "    text = []\n",
    "    # 语言为中文\n",
    "    if language == 'cn':\n",
    "        i = 0\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                i = i+1\n",
    "                \n",
    "                if i%1000000 == 0:\n",
    "                    print(str(i))\n",
    "                    \n",
    "                try:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    text.append([\"BOS\"] + tokenizer_cn(line[0]) + [\"EOS\"])\n",
    "                    \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"error raise!\")\n",
    "                    \n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # 语言为英文\n",
    "    elif language == 'en':\n",
    "        i = 0\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                i = i+1\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    text.append([\"BOS\"] + tokenizer_en(line[0].lower()) + [\"EOS\"])#小写\n",
    "                    \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"error raise!\")\n",
    "                if i%1000000 == 0:\n",
    "                    print(str(i))\n",
    "                    \n",
    "                    \n",
    "       \n",
    "        return text\n",
    "    \n",
    "    # 语言非中文和非英文\n",
    "    else:\n",
    "        print(\"Can't handle the language!\")\n",
    "        return \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n"
     ]
    }
   ],
   "source": [
    "train_en = load_data(train_en_file, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-15T08:20:46.371Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.690 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_zh = load_data(train_cn_file, 'cn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_zh.pickle', 'wb') as file:\n",
    "    pickle.dump(train_zh, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_en.pickle', 'wb') as file:\n",
    "    pickle.dump(train_en, file)\n",
    "#with open('train_zh.pickle', 'wb') as file:\n",
    " #   pickle.dump(train_zh, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 8s, sys: 30.8 s, total: 2min 39s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "with open('train_en.pickle', 'rb') as file:\n",
    "    train_en = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['BOS', 'resolution', '918', '(', '1994', ')', 'EOS'], 15886041)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[0], len(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('train_zh.pickle', 'rb') as file:\n",
    "    train_zh = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en = load_data(test_en_file, 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS',\n",
       " '7439th',\n",
       " 'meeting',\n",
       " ',',\n",
       " 'held',\n",
       " 'on',\n",
       " '11',\n",
       " 'may',\n",
       " '2015',\n",
       " '.',\n",
       " 'EOS']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8c71cc36a149d7a9b7dd9ebbbab446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7966fb614c84450c85a3337d6af6a52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e910d66db06a411abcbb769fcc28cc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7ae19b0a2b486dbf6ed318465e851e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[100, 100, 2309, 117, 1316, 1113, 1429, 1336, 1410, 119, 100]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "ids = tokenizer.convert_tokens_to_ids(test_en[0])\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (894 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n",
      "CPU times: user 1h 44min 59s, sys: 3min 53s, total: 1h 48min 53s\n",
      "Wall time: 1h 49min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "i = 0\n",
    "train_text_en = []\n",
    "train_en_encode = []\n",
    "with open(train_en_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        i = i+1\n",
    "\n",
    "        if i%1000000 == 0:\n",
    "            print(str(i))\n",
    "\n",
    "        try:\n",
    "            line = line.strip().split('\\t')\n",
    "            train_text_en.append(tokenizer_en.tokenize(line[0]))\n",
    "            train_en_encode.append(tokenizer_en.encode(line[0], max_length=1000))\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error raise!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 155, 9919, 13901, 16830, 24805, 5539, 1604, 113, 1898, 114, 102]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en_encode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_en_encode.pickle', 'wb') as file:\n",
    "    pickle.dump(train_en_encode, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer_zh = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "i = 0\n",
    "train_text_zh = []\n",
    "train_zh_encode = []\n",
    "with open(train_zh_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        i = i+1\n",
    "\n",
    "        if i%1000000 == 0:\n",
    "            print(str(i))\n",
    "\n",
    "        try:\n",
    "            line = line.strip().split('\\t')\n",
    "            train_text_zh.append(tokenizer_zh.tokenize(line[0]))\n",
    "            train_zh_encode.append(tokenizer_zh.encode(line[0], max_length=1000))\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error raise!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_en_file, 'r', encoding='utf-8') as f:\n",
    "    tmp = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"7439th meeting, held on 11 May 2015.\\nISIL itself has published videos depicting people being subjected to a range of abhorrent punishments, including stoning, being pushed-off buildings, decapitation and crucifixion.\\nUNICEF disbursed emergency cash assistance to tens of thousands of displaced families in camps and UNHCR distributed cash assistance to vulnerable families which had been internally displaced.\\n31. Recognizes the important contribution of the African Peer Review Mechanism since its inception in improving governance and supporting socioeconomic development in African countries, and recalls in this regard the high-level panel discussion held on 21 October 2013 on Africa's innovation in governance through 10 years of the African Peer Review Mechanism, organized during the sixty-eighth session of the General Assembly to commemorate the tenth anniversary of the Mechanism;\\nSpreads between sovereign bonds in Germany and those in other countries were relatively unaffected by politi\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['74', '##39', '##th', 'meeting', ',', 'held', 'on', '11', 'May', '2015', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 5692, 24786, 1582, 2309, 117, 1316, 1113, 1429, 1318, 1410, 119, 102]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en_encode[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISIL itself has published videos depicting people being subjected to a range of abhorrent punishments, including stoning, being pushed - off buildings, decapitation and crucifixion.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode = tokenizer.decode(id[1])\n",
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7439th meeting, held on 11 May 2015.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.723 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "test_zh = load_data(test_cn_file, 'cn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 647. GiB for an array with shape (452561528,) and data type <U384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14462/1143556178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mid_to_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mword_to_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mid_to_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0men_wtoi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_itow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#zh_wtoi, zh_itow = build_dict(train_zh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14462/1143556178.py\u001b[0m in \u001b[0;36mbuild_dict\u001b[0;34m(sentences, max_words)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#最大单词数是50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mword_to_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mword_to_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'UNK'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNK_IDX\u001b[0m  \u001b[0;31m#0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 647. GiB for an array with shape (452561528,) and data type <U384"
     ]
    }
   ],
   "source": [
    "#构建词汇表\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "\n",
    "    vocab = Counter(np.concatenate(sentences)).most_common(max_words)#最大单词数是50000\n",
    "    word_to_id = {w[0]: index + 2 for index, w in enumerate(vocab)}\n",
    "    word_to_id['UNK'] = UNK_IDX  #0\n",
    "    word_to_id['PAD'] = PAD_IDX  #1\n",
    "    id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "    return word_to_id,id_to_word\n",
    "en_wtoi, en_itow = build_dict(train_en)\n",
    "#zh_wtoi, zh_itow = build_dict(train_zh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 到此为止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "en_itow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用词典对原始句子编码 单词->数字\n",
    "def encode(en_sentences, ch_sentences, en_wtoi, zh_wtoi, sort_by_len=True):\n",
    "\n",
    "\n",
    "    out_en_sentences = [[en_wtoi.get(w, UNK_IDX) for w in sent] for sent in en_sentences]\n",
    "    out_ch_sentences = [[zh_wtoi.get(w, UNK_IDX) for w in sent] for sent in ch_sentences]\n",
    "    #返回w对应的值，否则返回UNK_IDX\n",
    "    def len_argsort(seq):#按照长度进行排序\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_ch_sentences = [out_ch_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_ch_sentences\n",
    "\n",
    "train_en_encode, train_zh_encode = encode(train_en, train_zh, en_wtoi, zh_wtoi)\n",
    "#dev_en_encode, dev_zh_encode = encode(dev_en, dev_zh, en_wtoi, zh_wtoi)\n",
    "test_en_encode, test_zh_encode = encode(test_en, test_zh, en_wtoi, zh_wtoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(sentences, wtoi, sort_by_len=True):\n",
    "\n",
    "\n",
    "    out_sentences = [[wtoi.get(w, UNK_IDX) for w in sent] for sent in sentences]\n",
    "    #返回w对应的值，否则返回UNK_IDX\n",
    "    def len_argsort(seq):#按照长度进行排序\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_ch_sentences = [out_ch_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_ch_sentences\n",
    "\n",
    "train_en_encode, train_zh_encode = encode(train_en, train_zh, en_wtoi, zh_wtoi)\n",
    "#dev_en_encode, dev_zh_encode = encode(dev_en, dev_zh, en_wtoi, zh_wtoi)\n",
    "test_en_encode, test_zh_encode = encode(test_en, test_zh, en_wtoi, zh_wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#返回每个batch的id\n",
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size) \n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_minibatches(50, 10, shuffle=True)#得到每一个batch对应的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将句子对划分到batch\n",
    "def get_batches(en_encode, ch_encode):\n",
    "    batch_indexs = get_minibatches(len(en_encode),BATCH_SIZE)\n",
    "\n",
    "    batches = []\n",
    "    for batch_index in batch_indexs:\n",
    "        batch_en = [torch.tensor(en_encode[index]).long() for index in batch_index] #每一个idx对应的句子，转为tensor格式\n",
    "        batch_zh = [torch.tensor(ch_encode[index]).long() for index in batch_index]\n",
    "        length_en = torch.tensor([len(en)  for en in batch_en]).long()#每一个句子的长度\n",
    "        length_zh = torch.tensor([len(zh)  for zh in batch_zh]).long()\n",
    "\n",
    "        batch_en = pad_sequence(batch_en, padding_value=PAD_IDX, batch_first=True)#讲一个batch中的句子padding为相同长度\n",
    "        batch_zh = pad_sequence(batch_zh, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "        batches.append((batch_en, batch_zh,length_en,length_zh))\n",
    "    return batches\n",
    "train_data = get_batches(train_en_encode, train_zh_encode)\n",
    "dev_data = get_batches(dev_en_encode, dev_zh_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cdn.mathpix.com/snip/images/8Es5WLO-mn8kY7GO-T7o1IxWUBPbZeLrz3JbqY5U2Vw.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $\\bar{h}_{s}$ 表示encoder每个hidden_state的输出， $h_{t}$ 表示decoder每个hidden_state的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{t}(s) \n",
    "&=\\frac{\\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s}\\right)\\right)}{\\sum_{s^{\\prime}} \\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s^{\\prime}}\\right)\\right)} \\\\\n",
    "c_{t} &= \\sum a_{t} \\bar{h}_{s} \\\\\n",
    "\\tilde{h}_{t} &=\\tanh \\left(W_{c}\\left[c_{t} ; h_{t}\\right]\\right)\\\\\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\operatorname{score}\\left(\\boldsymbol{h}_{t}, \\overline{\\boldsymbol{h}}_{s}\\right)= \\begin{cases}\\boldsymbol{h}_{t}^{\\top} \\overline{\\boldsymbol{h}}_{s} & \\text { dot } \\\\ \\boldsymbol{h}_{t}^{\\top} \\boldsymbol{W}_{\\boldsymbol{a}} \\overline{\\boldsymbol{h}}_{s} & \\text { general } \\\\ \\boldsymbol{v}_{a}^{\\top} \\tanh \\left(\\boldsymbol{W}_{\\boldsymbol{a}}\\left[\\boldsymbol{h}_{t} ; \\overline{\\boldsymbol{h}}_{s}\\right]\\right) & \\text { concat }\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size,enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongEncoder,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, bidirectional=True)#双向GRU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size*2 , dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        \"\"\"\n",
    "        input_seqs : batch_size,max(x_lengths)\n",
    "        input_lengths: batch_size\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))#batch_size,max(x_lengths),embed_size\n",
    "        packed = pack_padded_sequence(embedded, x_lengths.long().cpu().data.numpy(),batch_first=True,enforce_sorted=False)\n",
    "        #压缩填充张量,压缩掉无效的填充值\n",
    "        #enforce_sorted：如果是 True ，则输入应该是按长度降序排序的序列。如果是 False ，会在函数内部进行排序 \n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs,padding_value =PAD_IDX,batch_first=True)#还原\n",
    "        \n",
    "        #hidden (2, batch_size, enc_hidden_size)\n",
    "        #outputs (batch_size,seq_len, 2 * enc_hidden_size)\n",
    "        \n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        hidden = torch.tanh(self.fc(hidden)).unsqueeze(0)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, enc_hidden_size,dec_hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "        #general attention\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, output, encoder_out,mask):\n",
    "        \"\"\"\n",
    "        output:batch_size, max(y_lengths), dec_hidden_size  #(h_t)\n",
    "        encoder_out:batch_size, max(x_lengths), 2 * enc_hidden_size  #(h_s)\n",
    "        \"\"\"\n",
    "        batch_size = output.shape[0]\n",
    "        output_len = output.shape[1]\n",
    "        input_len = encoder_out.shape[1]\n",
    "\n",
    "        encoder_out1 = self.linear_in(encoder_out.view(batch_size*input_len, -1)).view(batch_size, input_len, -1) \n",
    "        #Wh_s \n",
    "        #batch_size,max(x_lengths),dec_hidden_size\n",
    "        score = torch.bmm(output, encoder_out1.transpose(1,2))#实现三维数组的乘法，而不用拆成二维数组使用for循环解决\n",
    "        #[batch_size,max(y_lengths),dec_hidden_size] * [batch_size,dec_hidden_size,max(x_lengths)]\n",
    "        #batch_size,max(y_lengths),max(x_lengths)#score = h_t W h_s\n",
    "        score.data.masked_fill(mask,-1e16)\n",
    "        attn = F.softmax(score, dim=2) #attention系数矩阵\n",
    "\n",
    "        ct = torch.bmm(attn, encoder_out) #ct = aths\n",
    "        #[batch_size,max(y_lengths),max(x_lengths)] * [batch_size, max(x_lengths), 2 * enc_hidden_size]\n",
    "        #batch_size, max(y_lengths), enc_hidden_size*2\n",
    "        output = torch.cat((ct, output), dim=2) \n",
    "\n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        #batch_size, max(y_lengths), dec_hidden_size\n",
    "\n",
    "        return output,attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongDecoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.attention = Attn(enc_hidden_size,dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size,dec_hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "    def creat_mask(self,x,y):\n",
    "\n",
    "        x_mask = x.data != PAD_IDX #batch_size,max(x_lengths)\n",
    "        y_mask = y.data != PAD_IDX #batch_size,max(y_lengths)\n",
    "        mask = (1 - (x_mask.unsqueeze(2) * y_mask.unsqueeze(1)).float()).bool()\n",
    "        #batch_size,max(x_lengths),max(y_lengths)\n",
    "        return mask \n",
    "    \n",
    "    def forward(self,encoder_out,x,y,y_lengths,hid):\n",
    "        mask = self.creat_mask(y,x)\n",
    "        y = self.dropout(self.embedding(y))\n",
    "        packed = pack_padded_sequence(y, y_lengths.long().cpu().data.numpy(),batch_first=True,enforce_sorted=False)\n",
    "        out, hid = self.rnn(packed,hid)\n",
    "       \n",
    "        out, _ = pad_packed_sequence(out,padding_value =PAD_IDX,batch_first=True)\n",
    "        \n",
    "        output,attn = self.attention(out,encoder_out,mask)\n",
    "        output = self.out(output)\n",
    "        #batch_size, max(y_lengths), vocab_size\n",
    "        return output,hid,attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self,x,x_lengths,y,y_lengths):\n",
    "        encoder_out,hid = self.encoder(x,x_lengths)\n",
    "        output,hid,attn = self.decoder(encoder_out,  #这里输出的hid是decoder_rnn的hid\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)#encoder的hid\n",
    "        return output,attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=15):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for _ in range(max_length):\n",
    "            output,hid,attn = self.decoder(encoder_out, \n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1),torch.cat(attns, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define Model\n",
    "encoder = LuongEncoder(vocab_size = len(en_itow), embed_size = EMBED_SIZE, enc_hidden_size = ENC_HIDDEN_SIZE, dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "decoder = LuongDecoder(vocab_size = len(zh_itow), embed_size = EMBED_SIZE, enc_hidden_size  = ENC_HIDDEN_SIZE, dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "model = seq2seq(encoder, decoder)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)#忽略padding位置的损失\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer,train_data):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:,:-1]#将前seq-1个单词作为输入\n",
    "        y_output = y[:,1:]#将后seq-1个单词作为输出，相当于前一个单词预测后一个单词\n",
    "        y_lengths = (y_lengths-1).to(DEVICE)\n",
    "\n",
    "\n",
    "        logits, _ = model(x,x_lengths,y_input,y_lengths)#batch_size, max(y_lengths), vocab_size\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)\n",
    "\n",
    "\n",
    "def evaluate(model,dev_data):\n",
    "\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:,:-1]\n",
    "        y_output = y[:,1:]\n",
    "        y_lengths = (y_lengths-1).to(DEVICE)\n",
    "        logits, _ = model(x,x_lengths,y_input,y_lengths)\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer,train_data)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model,dev_data)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dev(i):\n",
    "    model.eval()\n",
    "    \n",
    "    en_sent = \" \".join([en_itow[word] for word in test_en_encode[i]])\n",
    "    print('英文原句：',en_sent)\n",
    "    print('标准中文翻译：',\" \".join([zh_itow[word] for word in test_zh_encode[i]]))\n",
    "\n",
    "    bos = torch.Tensor([[zh_wtoi[\"BOS\"]]]).long().to(DEVICE)\n",
    "    x = torch.Tensor(test_en_encode[i]).long().to(DEVICE).reshape(1, -1)\n",
    "    x_len = torch.Tensor([len(test_en_encode[i])]).long().to(DEVICE)\n",
    "    \n",
    "    translation,_ = model.translate(x, x_len, bos)\n",
    "    translation = [zh_itow[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print('模型翻译结果：',\" \".join(trans))\n",
    "for i in range(50,100):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 练习一：Bi-LSTM + attention 用于情感分类\n",
    "\n",
    "补全代码：我们使用构建一个Bi-LSTM + attention模型完成文本分类任务，数据使用IMDb电影评论数据集，检测一段文字的情感是正面还是负面。\n",
    "\n",
    "[论文](https://aclanthology.org/P16-2034.pdf)\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.insert_token(\"<pad>\",1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x : 0 if x == 'neg' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):#自定义的batch输出\n",
    "    label_list, text_list,lengths = [], [],[]\n",
    "    batch.sort(key=lambda x: len(text_pipeline(x[1])), reverse=True)#按照长度的大小进行排序\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text))\n",
    "    text_list = pad_sequence(text_list, padding_value=vocab.get_stoi()[\"<pad>\"], batch_first=True)#进行填充，每个batch中的句子需要有相同的长度\n",
    "    return torch.tensor(label_list), text_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter,test_iter = IMDB(root='data', split=('train', 'test'))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ =  random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=128,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=128,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]#标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]#batch_size,max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[2]#lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型分为五个部分\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)\n",
    "\n",
    "- 输入层（Input layer）：将句子输入模型\n",
    "- 嵌入层（Embedding layer）：将每个词映射到一个低维向量\n",
    "- LSTM层（LSTM layer）：利用BiLSTM从词向量中获得特征\n",
    "- Attention层（Attention layer）：生成权重向量，将每个时间步长的单词级特征与权重向量相乘，合并成句子级特征向量（补全代码）\n",
    "- 输出层（Output layer）： 对句子进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bilstm_attn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 dropout_rate, pad_id):\n",
    "        super(bilstm_attn,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = True,\n",
    "                            dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \"\"\"\n",
    "            Write your code here.\n",
    "        \"\"\"\n",
    "        \n",
    "    def forward(self, x,lengths):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))# [batch size,seq len] -> [batch size,seq len,embedding_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden = [n layers *2, batch size, hidden dim]最后一个step的hidden\n",
    "        # cell = [n layers * 2, batch size, hidden dim]最终一个step的cell\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # output = [batch size, seq len, hidden dim * 2]#每一个step下的最后一层的output\n",
    "        output = output.reshape(output.shape[0],output.shape[1],2,-1)\n",
    "        # output = [batch size, seq len, 2,hidden dim]\n",
    "        output =  torch.sum(output, dim=2)\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        \"\"\"\n",
    "            Write your code here.\n",
    "        \"\"\"\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5\n",
    "pad_id = vocab.get_stoi()[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = bilstm_attn(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,dropout_rate, pad_id).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "    model.train() #model.train()代表了训练模式    \n",
    "    for label, text, lengths in train_loader: \n",
    "        label = label.to(device)\n",
    "        text = text.to(device)  \n",
    "        \n",
    "        out = model(text,lengths)\n",
    "        loss = loss_fn(out,label)\n",
    "        \n",
    "        _,pred = torch.max(out.data,1)       \n",
    "        corrects += (pred == label).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad() #加这步防止梯度叠加\n",
    "        loss.backward() #反向传播\n",
    "        optimizer.step() #梯度下降\n",
    "        \n",
    "        epoch_loss += loss.item() * len(label)\n",
    "        #loss.item()已经本身除以了len(batch.label)\n",
    "        #所以得再乘一次，得到一个batch的损失，累加得到所有样本损失。\n",
    "\n",
    "        \n",
    "        total_len += len(label)\n",
    "        #计算train_iterator所有样本的数量，不出意外应该是17500\n",
    "        \n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "     \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "    \n",
    "    model.eval()\n",
    "    #转换成测试模式，冻结dropout层或其他层。\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for label, text, lengths in valid_loader: \n",
    "            #iterator为valid_iterator\n",
    "            label = label.to(device)\n",
    "            text = text.to(device) \n",
    "            \n",
    "            out = model(text,lengths)\n",
    "            loss = loss_fn(out,label)\n",
    "\n",
    "            _,pred = torch.max(out.data,1)       \n",
    "            corrects += (pred == label).sum().item()\n",
    "            \n",
    "            \n",
    "            epoch_loss += loss.item() * len(label)\n",
    "            total_len += len(label)\n",
    "    model.train() #调回训练模式   \n",
    "    \n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, loss_fn)\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss,\"train_acc\",train_acc)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader)  \n",
    "    print(\"epoch:\",epoch,\"valid_loss:\",valid_loss,\"valid_acc\",valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bilstm_attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    text = text_pipeline(text)\n",
    "    \n",
    "    length = torch.LongTensor([len(text)])\n",
    "    tensor = torch.LongTensor(text).unsqueeze(0).to(device)\n",
    "    \n",
    "    out = model(tensor, length)\n",
    "    _,pred = torch.max(out.data,1)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sentiment(\"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
