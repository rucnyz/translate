{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  seq2seq + Luong 注意力 进行中英文翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "from collections import Counter  #计数器\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer  #分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 练习一：Bi-LSTM + attention 用于情感分类\n",
    "\n",
    "补全代码：我们使用构建一个Bi-LSTM + attention模型完成文本分类任务，数据使用IMDb电影评论数据集，检测一段文字的情感是正面还是负面。\n",
    "\n",
    "[论文](https://aclanthology.org/P16-2034.pdf)\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split = 'train')\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.insert_token(\"<pad>\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: 0 if x == 'neg' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):  #自定义的batch输出\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    batch.sort(key = lambda x: len(text_pipeline(x[1])), reverse = True)  #按照长度的大小进行排序\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text))\n",
    "    text_list = pad_sequence(text_list, padding_value = vocab.get_stoi()[\"<pad>\"],\n",
    "                             batch_first = True)  #进行填充，每个batch中的句子需要有相同的长度\n",
    "    return torch.tensor(label_list), text_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = IMDB(root = 'data', split = ('train', 'test'))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size = 128,\n",
    "                              shuffle = True, collate_fn = collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size = 128,\n",
    "                              shuffle = True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128,\n",
    "                             shuffle = True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "        1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "        1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "        1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        1, 0, 1, 1, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]  #标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11133,   512,    18,  ...,  4904,    24,     3],\n",
       "        [   94,   139,     8,  ...,     1,     1,     1],\n",
       "        [  288,   725,     6,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    6,  1126,    21,  ...,     1,     1,     1],\n",
       "        [   19,   154,     7,  ...,     1,     1,     1],\n",
       "        [ 1196,    80,   100,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]  #batch_size,max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[944,\n",
       " 864,\n",
       " 777,\n",
       " 695,\n",
       " 653,\n",
       " 544,\n",
       " 531,\n",
       " 522,\n",
       " 499,\n",
       " 497,\n",
       " 496,\n",
       " 491,\n",
       " 485,\n",
       " 484,\n",
       " 479,\n",
       " 457,\n",
       " 453,\n",
       " 440,\n",
       " 439,\n",
       " 433,\n",
       " 429,\n",
       " 422,\n",
       " 419,\n",
       " 407,\n",
       " 404,\n",
       " 397,\n",
       " 395,\n",
       " 383,\n",
       " 372,\n",
       " 369,\n",
       " 352,\n",
       " 342,\n",
       " 331,\n",
       " 323,\n",
       " 320,\n",
       " 312,\n",
       " 312,\n",
       " 312,\n",
       " 311,\n",
       " 311,\n",
       " 295,\n",
       " 295,\n",
       " 294,\n",
       " 294,\n",
       " 294,\n",
       " 280,\n",
       " 275,\n",
       " 272,\n",
       " 263,\n",
       " 260,\n",
       " 259,\n",
       " 253,\n",
       " 253,\n",
       " 243,\n",
       " 234,\n",
       " 224,\n",
       " 220,\n",
       " 220,\n",
       " 216,\n",
       " 211,\n",
       " 210,\n",
       " 209,\n",
       " 204,\n",
       " 203,\n",
       " 201,\n",
       " 200,\n",
       " 193,\n",
       " 191,\n",
       " 191,\n",
       " 188,\n",
       " 187,\n",
       " 186,\n",
       " 186,\n",
       " 185,\n",
       " 184,\n",
       " 183,\n",
       " 181,\n",
       " 179,\n",
       " 173,\n",
       " 171,\n",
       " 168,\n",
       " 164,\n",
       " 163,\n",
       " 163,\n",
       " 162,\n",
       " 156,\n",
       " 156,\n",
       " 156,\n",
       " 153,\n",
       " 150,\n",
       " 150,\n",
       " 148,\n",
       " 141,\n",
       " 141,\n",
       " 138,\n",
       " 134,\n",
       " 133,\n",
       " 133,\n",
       " 133,\n",
       " 132,\n",
       " 132,\n",
       " 130,\n",
       " 128,\n",
       " 128,\n",
       " 127,\n",
       " 126,\n",
       " 120,\n",
       " 120,\n",
       " 119,\n",
       " 118,\n",
       " 111,\n",
       " 110,\n",
       " 108,\n",
       " 106,\n",
       " 93,\n",
       " 93,\n",
       " 91,\n",
       " 83,\n",
       " 81,\n",
       " 79,\n",
       " 77,\n",
       " 67,\n",
       " 67,\n",
       " 63,\n",
       " 57,\n",
       " 50,\n",
       " 47,\n",
       " 46]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]  #lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "模型分为五个部分\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)\n",
    "\n",
    "- 输入层（Input layer）：将句子输入模型\n",
    "- 嵌入层（Embedding layer）：将每个词映射到一个低维向量\n",
    "- LSTM层（LSTM layer）：利用BiLSTM从词向量中获得特征\n",
    "- Attention层（Attention layer）：生成权重向量，将每个时间步长的单词级特征与权重向量相乘，合并成句子级特征向量（补全代码）\n",
    "- 输出层（Output layer）： 对句子进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class bilstm_attn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 dropout_rate, pad_id):\n",
    "        super(bilstm_attn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_id)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = True,\n",
    "                            dropout = dropout_rate, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.attn = MultiheadAttention(hidden_dim, 1, batch_first = True)\n",
    "        self.Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.dropout(self.embedding(x))  # [batch size,seq len] -> [batch size,seq len,embedding_dim]\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first = True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden = [n layers *2, batch size, hidden dim]最后一个step的hidden\n",
    "        # cell = [n layers * 2, batch size, hidden dim]最终一个step的cell\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first = True)\n",
    "        # output = [batch size, seq len, hidden dim * 2]#每一个step下的最后一层的output\n",
    "        output = output.reshape(output.shape[0], output.shape[1], 2, -1)\n",
    "        # output = [batch size, seq len, 2,hidden dim]\n",
    "        output = torch.sum(output, dim = 2)\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        output = self.dropout(output)\n",
    "        q_vector = self.Q(output)\n",
    "        k_vector = self.K(output)\n",
    "        v_vector = self.V(output)\n",
    "        attn_output, attn_output_weights = self.attn(q_vector, k_vector, v_vector)\n",
    "        prediction = self.output(attn_output.sum(dim = 1))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5\n",
    "pad_id = vocab.get_stoi()[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = bilstm_attn(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate, pad_id).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "    model.train()  #model.train()代表了训练模式\n",
    "    for label, text, lengths in tqdm(train_loader):\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "\n",
    "        out = model(text, lengths)\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "        _, pred = torch.max(out.data, 1)\n",
    "        corrects += (pred == label).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()  #加这步防止梯度叠加\n",
    "        loss.backward()  #反向传播\n",
    "        optimizer.step()  #梯度下降\n",
    "\n",
    "        epoch_loss += loss.item() * len(label)\n",
    "        #loss.item()已经本身除以了len(batch.label)\n",
    "        #所以得再乘一次，得到一个batch的损失，累加得到所有样本损失。\n",
    "\n",
    "        total_len += len(label)\n",
    "        #计算train_iterator所有样本的数量，不出意外应该是17500\n",
    "\n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    #转换成测试模式，冻结dropout层或其他层。\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text, lengths in tqdm(valid_loader):\n",
    "            #iterator为valid_iterator\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            out = model(text, lengths)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            _, pred = torch.max(out.data, 1)\n",
    "            corrects += (pred == label).sum().item()\n",
    "\n",
    "            epoch_loss += loss.item() * len(label)\n",
    "            total_len += len(label)\n",
    "    model.train()  #调回训练模式\n",
    "\n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: 3.335827901147541 train_acc 0.5699368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 valid_loss: 0.6690802865028381 valid_acc 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:33<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 train_loss: 0.5631066594475194 train_acc 0.7155789473684211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 valid_loss: 0.5004681632995606 valid_acc 0.7728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 train_loss: 0.45125067443345723 train_acc 0.7912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 valid_loss: 0.3808725685119629 valid_acc 0.8264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 train_loss: 0.4035148923246484 train_acc 0.8285052631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 valid_loss: 0.38572630796432494 valid_acc 0.8448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 train_loss: 0.34146378761341695 train_acc 0.857178947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 valid_loss: 0.4886480527877808 valid_acc 0.8088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 train_loss: 0.28406720405628805 train_acc 0.8827368421052632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 valid_loss: 0.3373931920051575 valid_acc 0.868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 train_loss: 0.24964638517279375 train_acc 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 valid_loss: 0.3452787638664246 valid_acc 0.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:33<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 train_loss: 0.229890008506022 train_acc 0.9080842105263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 valid_loss: 0.31069177660942077 valid_acc 0.8816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 train_loss: 0.19923359266958737 train_acc 0.9213473684210526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 valid_loss: 0.3836130741596222 valid_acc 0.8736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:32<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 train_loss: 0.18384257307429064 train_acc 0.926778947368421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 valid_loss: 0.34496309649944307 valid_acc 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, loss_fn)\n",
    "    print(\"epoch:\", epoch, \"train_loss:\", train_loss, \"train_acc\", train_acc)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader)\n",
    "    print(\"epoch:\", epoch, \"valid_loss:\", valid_loss, \"valid_acc\", valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'attn_data/bilstm_attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    text = text_pipeline(text)\n",
    "\n",
    "    length = torch.LongTensor([len(text)])\n",
    "    tensor = torch.LongTensor(text).unsqueeze(0).to(device)\n",
    "\n",
    "    out = model(tensor, length)\n",
    "    _, pred = torch.max(out.data, 1)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
