{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  seq2seq + Luong 注意力 进行中英文翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "from collections import Counter  #计数器\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer  #分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用Luong 注意力进行中英翻译训练，[数据来源](https://github.com/cuicaihao/Annotated-Transformer-English-to-Chinese-Translator/tree/master/data/nmt/en-cn)，包含成对的中英文翻译句子，包含7个文件，cmn.txt是全体数据集；train.txt, dev.txt, test.txt为对全体数据集的80%、10%、\n",
    "10%划分。train_mini.txt, dev_mini.txt, test_mini.txt为小样本数据，分别包含1000、200、200条数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "UNK_IDX = 0  #未知\n",
    "PAD_IDX = 1  #\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "DROPOUT = 0.2\n",
    "ENC_HIDDEN_SIZE = DEC_HIDDEN_SIZE = 100\n",
    "EMBED_SIZE = 100\n",
    "DEBUG = True\n",
    "\n",
    "if DEBUG:\n",
    "\n",
    "    train_file = 'attn_data/train_mini.txt'\n",
    "    dev_file = 'attn_data/dev_mini.txt'\n",
    "    test_file = 'attn_data/test_mini.txt'\n",
    "    save_file = 'model.pt'\n",
    "else:\n",
    "\n",
    "    train_file = 'attn_data/train.txt'\n",
    "    dev_file = 'attn_data/dev.txt'\n",
    "    test_file = 'attn_data/test.txt'\n",
    "    save_file = 'large_model.pt'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#分词器\n",
    "tokenizer_en = get_tokenizer('basic_english')  #按空格进行分割\n",
    "tokenizer_cn = get_tokenizer(jieba.lcut)  #进行结巴分词\n",
    "\n",
    "\n",
    "#加载文件\n",
    "def load_data(path):\n",
    "    en = []\n",
    "    cn = []\n",
    "    with open(path, 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            en.append([\"BOS\"] + tokenizer_en(line[0].lower()) + [\"EOS\"])  #小写\n",
    "            cn.append([\"BOS\"] + tokenizer_cn(line[1]) + [\"EOS\"])\n",
    "    return en, cn\n",
    "\n",
    "\n",
    "train_en, train_zh = load_data(train_file)\n",
    "dev_en, dev_zh = load_data(dev_file)\n",
    "test_en, test_zh = load_data(test_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_en[0], train_zh[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#构建词汇表\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "    vocab = Counter(np.concatenate(sentences)).most_common(max_words)  #最大单词数是50000\n",
    "    word_to_id = {w[0]: index + 2 for index, w in enumerate(vocab)}\n",
    "    word_to_id['UNK'] = UNK_IDX  #0\n",
    "    word_to_id['PAD'] = PAD_IDX  #1\n",
    "    id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "en_wtoi, en_itow = build_dict(train_en)\n",
    "zh_wtoi, zh_itow = build_dict(train_zh)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "en_itow[2], en_itow[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 利用词典对原始句子编码 单词->数字\n",
    "def encode(en_sentences, ch_sentences, en_wtoi, zh_wtoi, sort_by_len = True):\n",
    "    out_en_sentences = [[en_wtoi.get(w, UNK_IDX) for w in sent] for sent in en_sentences]\n",
    "    out_ch_sentences = [[zh_wtoi.get(w, UNK_IDX) for w in sent] for sent in ch_sentences]\n",
    "\n",
    "    #返回w对应的值，否则返回UNK_IDX\n",
    "    def len_argsort(seq):  #按照长度进行排序\n",
    "        return sorted(range(len(seq)), key = lambda x: len(seq[x]))\n",
    "\n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_ch_sentences = [out_ch_sentences[i] for i in sorted_index]\n",
    "\n",
    "    return out_en_sentences, out_ch_sentences\n",
    "\n",
    "\n",
    "train_en_encode, train_zh_encode = encode(train_en, train_zh, en_wtoi, zh_wtoi)\n",
    "dev_en_encode, dev_zh_encode = encode(dev_en, dev_zh, en_wtoi, zh_wtoi)\n",
    "test_en_encode, test_zh_encode = encode(test_en, test_zh, en_wtoi, zh_wtoi)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_en_encode[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#返回每个batch的id\n",
    "def get_minibatches(n, minibatch_size, shuffle = True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_minibatches(50, 10, shuffle = True)  #得到每一个batch对应的id"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#将句子对划分到batch\n",
    "def get_batches(en_encode, ch_encode):\n",
    "    batch_indexs = get_minibatches(len(en_encode), BATCH_SIZE)\n",
    "\n",
    "    batches = []\n",
    "    for batch_index in batch_indexs:\n",
    "        batch_en = [torch.tensor(en_encode[index]).long() for index in batch_index]  #每一个idx对应的句子，转为tensor格式\n",
    "        batch_zh = [torch.tensor(ch_encode[index]).long() for index in batch_index]\n",
    "        length_en = torch.tensor([len(en) for en in batch_en]).long()  #每一个句子的长度\n",
    "        length_zh = torch.tensor([len(zh) for zh in batch_zh]).long()\n",
    "\n",
    "        batch_en = pad_sequence(batch_en, padding_value = PAD_IDX, batch_first = True)  #讲一个batch中的句子padding为相同长度\n",
    "        batch_zh = pad_sequence(batch_zh, padding_value = PAD_IDX, batch_first = True)\n",
    "\n",
    "        batches.append((batch_en, batch_zh, length_en, length_zh))\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = get_batches(train_en_encode, train_zh_encode)\n",
    "dev_data = get_batches(dev_en_encode, dev_zh_encode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[0][3], len(train_data[0][3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data[0][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 建立模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/8Es5WLO-mn8kY7GO-T7o1IxWUBPbZeLrz3JbqY5U2Vw.original.fullsize.png\" width=\"40%\"> "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "其中 $\\bar{h}_{s}$ 表示encoder每个hidden_state的输出， $h_{t}$ 表示decoder每个hidden_state的输出。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "a_{t}(s) =\\frac{\\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s}\\right)\\right)}{\\sum_{s^{\\prime}} \\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s^{\\prime}}\\right)\\right)}\\\\\n",
    "c_{t} = \\sum a_{t} \\bar{h}_{s}\\\\\n",
    "\\tilde{h}_{t} = \\tanh \\left(W_{c}\\left[c_{t} ; h_{t}\\right]\\right)\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "$$\n",
    "\\operatorname{score}\\left(\\boldsymbol{h}_{t}, \\overline{\\boldsymbol{h}}_{s}\\right)= \\begin{cases}\\boldsymbol{h}_{t}^{\\top} \\overline{\\boldsymbol{h}}_{s} & \\text { dot } \\\\ \\boldsymbol{h}_{t}^{\\top} \\boldsymbol{W}_{\\boldsymbol{a}} \\overline{\\boldsymbol{h}}_{s} & \\text { general } \\\\ \\boldsymbol{v}_{a}^{\\top} \\tanh \\left(\\boldsymbol{W}_{\\boldsymbol{a}}\\left[\\boldsymbol{h}_{t} ; \\overline{\\boldsymbol{h}}_{s}\\right]\\right) & \\text { concat }\\end{cases}\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LuongEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongEncoder, self).__init__()\n",
    "        \n",
    "        # 随机初始化词向量，词向量值在正态分布N(0,1)中随机取值: vocab_size 词典的大小尺寸, embed_size 嵌入向量的维度\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) \n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, bidirectional = True)  #双向GRU（embed_size输入特征维度，enc_hidden_size输出特征维度）\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, x_lengths):  # x_lengths: 输入句子长度\n",
    "        \"\"\"\n",
    "        input_seqs : batch_size,max(x_lengths)\n",
    "        input_lengths: batch_size\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))  #batch_size,max(x_lengths),embed_size\n",
    "        packed = pack_padded_sequence(embedded, x_lengths.long().cpu().data.numpy(), batch_first = True,\n",
    "                                      enforce_sorted = False)\n",
    "        # batch_first = False (seq, batch, feature)  batch_first = True (batch, seq, feature)\n",
    "        \n",
    "        #压缩填充张量,压缩掉无效的填充值\n",
    "        #enforce_sorted：如果是 True ，则输入应该是按长度降序排序的序列。如果是 False ，会在函数内部进行排序 \n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs, padding_value = PAD_IDX, batch_first = True)  #还原\n",
    "\n",
    "        #hidden (2, batch_size, enc_hidden_size)  # 2:双向\n",
    "        #outputs (batch_size,seq_len, 2 * enc_hidden_size)  h_s\n",
    "\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim = 1)  # 变成一维\n",
    "        hidden = torch.tanh(self.fc(hidden)).unsqueeze(0)  # 修改成decoder可接受hidden size维度\n",
    "        return outputs, hidden  # outputs为每一个time stamp的输出，hidden为最后一个time stamp的输出"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        #general attention\n",
    "        self.linear_in = nn.Linear(enc_hidden_size * 2, dec_hidden_size, bias = False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size * 2 + dec_hidden_size, dec_hidden_size)\n",
    "\n",
    "    def forward(self, output, encoder_out, mask):\n",
    "        \"\"\"\n",
    "        output:batch_size, max(y_lengths), dec_hidden_size  #(h_t)\n",
    "        encoder_out:batch_size, max(x_lengths), 2 * enc_hidden_size  #(h_s)\n",
    "        \"\"\"\n",
    "        batch_size = output.shape[0]\n",
    "        output_len = output.shape[1]\n",
    "        input_len = encoder_out.shape[1]\n",
    "\n",
    "        encoder_out1 = self.linear_in(encoder_out.view(batch_size * input_len, -1)).view(batch_size, input_len, -1)\n",
    "        #Wh_s \n",
    "        #batch_size,max(x_lengths),dec_hidden_size\n",
    "        score = torch.bmm(output, encoder_out1.transpose(1, 2))  #实现三维数组的乘法，而不用拆成二维数组使用for循环解决\n",
    "        #[batch_size,max(y_lengths),dec_hidden_size] * [batch_size,dec_hidden_size,max(x_lengths)]\n",
    "        #batch_size,max(y_lengths),max(x_lengths)  #score = h_t W h_s\n",
    "        score.data.masked_fill(mask, -1e16)\n",
    "        attn = F.softmax(score, dim = 2)  #attention系数矩阵, mask均为0\n",
    "        # mask的size是(batch_size,n,m)\n",
    "        # mask中，若该位置为True，就表明score中该位置要被mask掉，用-1e6来代替。\n",
    "        # PS mask中，若某个位置K_sub(b,i,j)为True，表明这个batch中的第b句话的中文的第i个字是padding or 英文的第j个单词是padding or 两个都是padding\n",
    "        # 若某个位置K_sub(b,i,j)为False，表明这个batch中的第b句话的中文的第i个字不是padding且英文的第j个单词也不是padding\n",
    "\n",
    "        ct = torch.bmm(attn, encoder_out)  #ct = aths\n",
    "        #[batch_size,max(y_lengths),max(x_lengths)] * [batch_size, max(x_lengths), 2 * enc_hidden_size]\n",
    "        #batch_size, max(y_lengths), enc_hidden_size*2\n",
    "        output = torch.cat((ct, output), dim = 2)  #batch_size, max(y_lengths), enc_hidden_size*2 + dec_hidden_size\n",
    "\n",
    "        output = output.view(batch_size * output_len, -1)  #batch_size * max(y_lengths), enc_hidden_size*2 + dec_hidden_size\n",
    "        output = torch.tanh(self.linear_out(output))  #batch_size * max(y_lengths), dec_hidden_size\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        #batch_size, max(y_lengths), dec_hidden_size\n",
    "\n",
    "        return output, attn\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LuongDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attn(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, dec_hidden_size, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "\n",
    "    def creat_mask(self, x, y):\n",
    "        x_mask = x.data != PAD_IDX  #batch_size,max(x_lengths)  # 不等于为true，不是padding\n",
    "        y_mask = y.data != PAD_IDX  #batch_size,max(y_lengths)\n",
    "        mask = (1 - (x_mask.unsqueeze(2) * y_mask.unsqueeze(1)).float()).bool()  # true为padding\n",
    "        # unsqueeze增加维度\n",
    "        #batch_size,max(x_lengths),max(y_lengths)\n",
    "        #attn为batch_size,max(y_lengths),max(x_lengths)，因此y与x对调\n",
    "        return mask\n",
    "\n",
    "    def forward(self, encoder_out, x, y, y_lengths, hid):  ## (encoder_out. hid)对应LuongEncoder的输出(outputs, hidden)\n",
    "        mask = self.creat_mask(y, x)\n",
    "        y = self.dropout(self.embedding(y))\n",
    "        packed = pack_padded_sequence(y, y_lengths.long().cpu().data.numpy(), batch_first = True,\n",
    "                                      enforce_sorted = False)\n",
    "        out, hid = self.rnn(packed, hid)  # x的 hid和y同时输入到decoder\n",
    "\n",
    "        out, _ = pad_packed_sequence(out, padding_value = PAD_IDX, batch_first = True)\n",
    "\n",
    "        output, attn = self.attention(out, encoder_out, mask)  # 输入enc和dec的hidden\n",
    "        output = self.out(output)\n",
    "        #batch_size, max(y_lengths), dec_hidden_size --> batch_size, max(y_lengths), vocab_size\n",
    "        return output, hid, attn  # output为每一个time stamp的输出，hid为最后一个time stamp的输出(attention前)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 2, 3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a.data,PAD_IDX"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(encoder_out,  #这里输出的hid是decoder_rnn的hid\n",
    "                                         x = x,\n",
    "                                         y = y,\n",
    "                                         y_lengths = y_lengths,\n",
    "                                         hid = hid)  #encoder的hid\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length = 15):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for _ in range(max_length):\n",
    "            output, hid, attn = self.decoder(encoder_out,\n",
    "                                             x = x,\n",
    "                                             y = y,\n",
    "                                             y_lengths = torch.ones(batch_size).long().to(y.device),  # 逐字翻译！\n",
    "                                             hid = hid)\n",
    "\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3.]), 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data,PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(encoder_out,  #这里输出的hid是decoder_rnn的hid\n",
    "                                         x = x,\n",
    "                                         y = y,\n",
    "                                         y_lengths = y_lengths,\n",
    "                                         hid = hid)  #encoder的hid\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length = 15):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for _ in range(max_length):\n",
    "            output, hid, attn = self.decoder(encoder_out,\n",
    "                                             x = x,\n",
    "                                             y = y,\n",
    "                                             y_lengths = torch.ones(batch_size).long().to(y.device),  # 逐字翻译！\n",
    "                                             hid = hid)\n",
    "\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define Model\n",
    "encoder = LuongEncoder(vocab_size = len(en_itow), embed_size = EMBED_SIZE, enc_hidden_size = ENC_HIDDEN_SIZE,\n",
    "                       dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "decoder = LuongDecoder(vocab_size = len(zh_itow), embed_size = EMBED_SIZE, enc_hidden_size = ENC_HIDDEN_SIZE,\n",
    "                       dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "model = seq2seq(encoder, decoder)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index = PAD_IDX)  #忽略padding位置的损失\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_data):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:, :-1]  #将前seq-1个单词作为输入\n",
    "        y_output = y[:, 1:]  #将后seq-1个单词作为输出，相当于前一个单词预测后一个单词\n",
    "        y_lengths = (y_lengths - 1).to(DEVICE)\n",
    "\n",
    "        logits, _ = model(x, x_lengths, y_input, y_lengths)  #batch_size, max(y_lengths), vocab_size\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)\n",
    "\n",
    "\n",
    "def evaluate(model, dev_data):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:, :-1]\n",
    "        y_output = y[:, 1:]\n",
    "        y_lengths = (y_lengths - 1).to(DEVICE)\n",
    "        logits, _ = model(x, x_lengths, y_input, y_lengths)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 5.773, Val loss: 5.144, Epoch time = 4.815s\n",
      "Epoch: 2, Train loss: 5.007, Val loss: 4.760, Epoch time = 4.802s\n",
      "Epoch: 3, Train loss: 4.663, Val loss: 4.447, Epoch time = 4.790s\n",
      "Epoch: 4, Train loss: 4.389, Val loss: 4.218, Epoch time = 4.812s\n",
      "Epoch: 5, Train loss: 4.168, Val loss: 4.007, Epoch time = 5.031s\n",
      "Epoch: 6, Train loss: 3.967, Val loss: 3.824, Epoch time = 4.705s\n",
      "Epoch: 7, Train loss: 3.784, Val loss: 3.655, Epoch time = 4.880s\n",
      "Epoch: 8, Train loss: 3.616, Val loss: 3.505, Epoch time = 5.031s\n",
      "Epoch: 9, Train loss: 3.462, Val loss: 3.369, Epoch time = 4.778s\n",
      "Epoch: 10, Train loss: 3.316, Val loss: 3.241, Epoch time = 4.814s\n",
      "Epoch: 11, Train loss: 3.179, Val loss: 3.110, Epoch time = 4.890s\n",
      "Epoch: 12, Train loss: 3.051, Val loss: 2.978, Epoch time = 4.578s\n",
      "Epoch: 13, Train loss: 2.928, Val loss: 2.866, Epoch time = 4.710s\n",
      "Epoch: 14, Train loss: 2.812, Val loss: 2.754, Epoch time = 4.702s\n",
      "Epoch: 15, Train loss: 2.705, Val loss: 2.654, Epoch time = 4.913s\n",
      "Epoch: 16, Train loss: 2.599, Val loss: 2.560, Epoch time = 4.679s\n",
      "Epoch: 17, Train loss: 2.504, Val loss: 2.465, Epoch time = 4.621s\n",
      "Epoch: 18, Train loss: 2.407, Val loss: 2.379, Epoch time = 4.822s\n",
      "Epoch: 19, Train loss: 2.316, Val loss: 2.289, Epoch time = 4.827s\n",
      "Epoch: 20, Train loss: 2.229, Val loss: 2.205, Epoch time = 4.693s\n",
      "Epoch: 21, Train loss: 2.149, Val loss: 2.127, Epoch time = 4.756s\n",
      "Epoch: 22, Train loss: 2.071, Val loss: 2.052, Epoch time = 4.886s\n",
      "Epoch: 23, Train loss: 2.001, Val loss: 1.983, Epoch time = 4.881s\n",
      "Epoch: 24, Train loss: 1.930, Val loss: 1.920, Epoch time = 4.839s\n",
      "Epoch: 25, Train loss: 1.864, Val loss: 1.851, Epoch time = 4.588s\n",
      "Epoch: 26, Train loss: 1.800, Val loss: 1.789, Epoch time = 4.642s\n",
      "Epoch: 27, Train loss: 1.738, Val loss: 1.735, Epoch time = 4.562s\n",
      "Epoch: 28, Train loss: 1.683, Val loss: 1.678, Epoch time = 4.827s\n",
      "Epoch: 29, Train loss: 1.633, Val loss: 1.629, Epoch time = 4.411s\n",
      "Epoch: 30, Train loss: 1.582, Val loss: 1.574, Epoch time = 4.364s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer, train_data)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, dev_data)\n",
    "    print((\n",
    "              f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文原句： BOS they want more . EOS\n",
      "标准中文翻译： BOS 他們 想要 更 多 。 EOS\n",
      "模型翻译结果： 汤姆 想 吃 了 。\n",
      "\n",
      "英文原句： BOS tom was bullied . EOS\n",
      "标准中文翻译： BOS 汤姆 被 UNK 了 。 EOS\n",
      "模型翻译结果： 汤姆 是 蓝色 的 。\n",
      "\n",
      "英文原句： BOS see you around . EOS\n",
      "标准中文翻译： BOS 再见 ！ EOS\n",
      "模型翻译结果： 再见 ！\n",
      "\n",
      "英文原句： BOS now i remember . EOS\n",
      "标准中文翻译： BOS 现在 我 想 起来 了 。 EOS\n",
      "模型翻译结果： 但願 我 吃 飽 。\n",
      "\n",
      "英文原句： BOS she hated him . EOS\n",
      "标准中文翻译： BOS 她 恨 他 。 EOS\n",
      "模型翻译结果： 她 愛 他 。\n",
      "\n",
      "英文原句： BOS tell me again . EOS\n",
      "标准中文翻译： BOS 重新 告訴 我 。 EOS\n",
      "模型翻译结果： 再 再 再 再 再 再 再 再 來 了 。\n",
      "\n",
      "英文原句： BOS tom follows orders . EOS\n",
      "标准中文翻译： BOS 汤姆 UNK 。 EOS\n",
      "模型翻译结果： 汤姆 走得 很慢 。\n",
      "\n",
      "英文原句： BOS do it now . EOS\n",
      "标准中文翻译： BOS 現在 就 做 。 EOS\n",
      "模型翻译结果： 现在 在 做 。\n",
      "\n",
      "英文原句： BOS foxes eat UNK . EOS\n",
      "标准中文翻译： BOS 狐狸 吃 UNK 。 EOS\n",
      "模型翻译结果： 下午 都 来 。\n",
      "\n",
      "英文原句： BOS i almost won . EOS\n",
      "标准中文翻译： BOS 我 几乎 赢 了 。 EOS\n",
      "模型翻译结果： 我 轉動 58 了 。\n",
      "\n",
      "英文原句： BOS shake my hand . EOS\n",
      "标准中文翻译： BOS 和 我 握手 。 EOS\n",
      "模型翻译结果： 别进 我 的 手 。\n",
      "\n",
      "英文原句： BOS they hated tom . EOS\n",
      "标准中文翻译： BOS 他們 恨 湯姆 。 EOS\n",
      "模型翻译结果： 他们 原谅 了 。\n",
      "\n",
      "英文原句： BOS prices went up . EOS\n",
      "标准中文翻译： BOS UNK 上 UNK 。 EOS\n",
      "模型翻译结果： 下 点 。\n",
      "\n",
      "英文原句： BOS no one knows . EOS\n",
      "标准中文翻译： BOS 沒 有人 知道 。 EOS\n",
      "模型翻译结果： 没有 人 都 不 知道 。\n",
      "\n",
      "英文原句： BOS words express thoughts . EOS\n",
      "标准中文翻译： BOS 語 UNK 達 UNK 。 EOS\n",
      "模型翻译结果： 說 人 很 難 。\n",
      "\n",
      "英文原句： BOS he wears glasses . EOS\n",
      "标准中文翻译： BOS 他 戴 眼鏡 。 EOS\n",
      "模型翻译结果： 他 站 在 天花板 。\n",
      "\n",
      "英文原句： BOS the system worked . EOS\n",
      "标准中文翻译： BOS 系统 UNK 起来 了 。 EOS\n",
      "模型翻译结果： 鐘停 了 。\n",
      "\n",
      "英文原句： BOS take your time . EOS\n",
      "标准中文翻译： BOS 你 可以 慢慢来 。 EOS\n",
      "模型翻译结果： 小心 你 的 。\n",
      "\n",
      "英文原句： BOS he UNK me . EOS\n",
      "标准中文翻译： BOS 他 向 我 UNK 。 EOS\n",
      "模型翻译结果： 他 让 我 跑 。\n",
      "\n",
      "英文原句： BOS i know tom . EOS\n",
      "标准中文翻译： BOS 我 认识 汤姆 。 EOS\n",
      "模型翻译结果： 我 知道 汤姆 。\n",
      "\n",
      "英文原句： BOS everything went black . EOS\n",
      "标准中文翻译： BOS 一切 都 變成 了 黑色 。 EOS\n",
      "模型翻译结果： 一些 人 要 離開 。\n",
      "\n",
      "英文原句： BOS let them decide . EOS\n",
      "标准中文翻译： BOS 讓 他們 決定 。 EOS\n",
      "模型翻译结果： 讓 我們 開始 吧 。\n",
      "\n",
      "英文原句： BOS are you finished ? EOS\n",
      "标准中文翻译： BOS 你 结束 了 吗 ？ EOS\n",
      "模型翻译结果： 你 生气 了 吗 ？\n",
      "\n",
      "英文原句： BOS i feel lonely . EOS\n",
      "标准中文翻译： BOS 我 觉得 很 孤独 。 EOS\n",
      "模型翻译结果： 我 需要 一些 。\n",
      "\n",
      "英文原句： BOS has anything changed ? EOS\n",
      "标准中文翻译： BOS 有 任何 事情 改變 了 嗎 ？ EOS\n",
      "模型翻译结果： 有 什么 新鲜事 吗 ？\n",
      "\n",
      "英文原句： BOS every minute UNK . EOS\n",
      "标准中文翻译： BOS UNK 。 EOS\n",
      "模型翻译结果： 盡 都 来 。\n",
      "\n",
      "英文原句： BOS what a surprise ! EOS\n",
      "标准中文翻译： BOS 太 惊喜 了 ! EOS\n",
      "模型翻译结果： 什么 啊 ！\n",
      "\n",
      "英文原句： BOS who built it ? EOS\n",
      "标准中文翻译： BOS 这 是 谁 建 的 ？ EOS\n",
      "模型翻译结果： 谁 是 谁 发明 的 ？\n",
      "\n",
      "英文原句： BOS he got angry . EOS\n",
      "标准中文翻译： BOS 他 生气 了 。 EOS\n",
      "模型翻译结果： 他 有 了 。\n",
      "\n",
      "英文原句： BOS she UNK him . EOS\n",
      "标准中文翻译： BOS 她 看不起 他 。 EOS\n",
      "模型翻译结果： 她 說 他 。\n",
      "\n",
      "英文原句： BOS i hate mosquitoes . EOS\n",
      "标准中文翻译： BOS 我 恨 蚊子 。 EOS\n",
      "模型翻译结果： 我 讨厌 快 。\n",
      "\n",
      "英文原句： BOS he acts quickly . EOS\n",
      "标准中文翻译： BOS 他 行动 迅速 。 EOS\n",
      "模型翻译结果： 他 很 容易 。\n",
      "\n",
      "英文原句： BOS are they students ? EOS\n",
      "标准中文翻译： BOS 他們 是 學生 嗎 ？ EOS\n",
      "模型翻译结果： 他们 是 吗 ？\n",
      "\n",
      "英文原句： BOS do you understand ? EOS\n",
      "标准中文翻译： BOS 你 明白 了 嗎 ? EOS\n",
      "模型翻译结果： 你 生气 了 吗 ？\n",
      "\n",
      "英文原句： BOS knowledge is power . EOS\n",
      "标准中文翻译： BOS 知识 就是 力量 。 EOS\n",
      "模型翻译结果： 凡事 是 个 骗子 。\n",
      "\n",
      "英文原句： BOS what should i bring ? EOS\n",
      "标准中文翻译： BOS 我该 UNK 什么 ？ EOS\n",
      "模型翻译结果： 我 应该 什麼 ？\n",
      "\n",
      "英文原句： BOS he is no fool . EOS\n",
      "标准中文翻译： BOS 他 没 疯 。 EOS\n",
      "模型翻译结果： 他 不是 傻子 。\n",
      "\n",
      "英文原句： BOS she went there yesterday . EOS\n",
      "标准中文翻译： BOS 她 昨天 去 那裡 。 EOS\n",
      "模型翻译结果： 她 昨天 去 了 那裡 。\n",
      "\n",
      "英文原句： BOS he climbed the stairs . EOS\n",
      "标准中文翻译： BOS 他 爬 上 了 樓梯 。 EOS\n",
      "模型翻译结果： 他 目睹 了 謀 。\n",
      "\n",
      "英文原句： BOS i just heard something . EOS\n",
      "标准中文翻译： BOS 我 只是 听到 了 一些 消息 。 EOS\n",
      "模型翻译结果： 我 希望 我 的 記憶力 。\n",
      "\n",
      "英文原句： BOS i am from UNK . EOS\n",
      "标准中文翻译： BOS 我來 UNK 。 EOS\n",
      "模型翻译结果： 我 完全 等 。\n",
      "\n",
      "英文原句： BOS you broke the rules . EOS\n",
      "标准中文翻译： BOS 你 触犯 了 规则 。 EOS\n",
      "模型翻译结果： 你 白費力氣 了 。\n",
      "\n",
      "英文原句： BOS i called her up . EOS\n",
      "标准中文翻译： BOS 我 叫 她 起来 。 EOS\n",
      "模型翻译结果： 我 愛 她 。\n",
      "\n",
      "英文原句： BOS he cleared his throat . EOS\n",
      "标准中文翻译： BOS 他 UNK 喉嚨 。 EOS\n",
      "模型翻译结果： 他 舉起 了 他 的 使命 。\n",
      "\n",
      "英文原句： BOS those are their books . EOS\n",
      "标准中文翻译： BOS 这是 他们 的 书 。 EOS\n",
      "模型翻译结果： 這是 他們 的 學生 。\n",
      "\n",
      "英文原句： BOS speaking english is useful . EOS\n",
      "标准中文翻译： BOS 说 英语 很 有用 。 EOS\n",
      "模型翻译结果： 英語 很 難 。\n",
      "\n",
      "英文原句： BOS where are my books ? EOS\n",
      "标准中文翻译： BOS 我 的 書 在 哪 ？ EOS\n",
      "模型翻译结果： 我 的 傘在 哪 ？\n",
      "\n",
      "英文原句： BOS he UNK the lady . EOS\n",
      "标准中文翻译： BOS 他 向 那位 女士 问好 。 EOS\n",
      "模型翻译结果： 他 打開 了 這個 旁邊 。\n",
      "\n",
      "英文原句： BOS he does speak well . EOS\n",
      "标准中文翻译： BOS 他 真的 說 得 很 好 。 EOS\n",
      "模型翻译结果： 他 說 得 很 好 。\n",
      "\n",
      "英文原句： BOS tom watched tv yesterday . EOS\n",
      "标准中文翻译： BOS Tom 昨天 看 了 电视 。 EOS\n",
      "模型翻译结果： 汤姆 昨天 有 很多 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    model.eval()\n",
    "\n",
    "    en_sent = \" \".join([en_itow[word] for word in test_en_encode[i]])\n",
    "    print('英文原句：', en_sent)\n",
    "    print('标准中文翻译：', \" \".join([zh_itow[word] for word in test_zh_encode[i]]))\n",
    "\n",
    "    bos = torch.Tensor([[zh_wtoi[\"BOS\"]]]).long().to(DEVICE)\n",
    "    x = torch.Tensor(test_en_encode[i]).long().to(DEVICE).reshape(1, -1)\n",
    "    x_len = torch.Tensor([len(test_en_encode[i])]).long().to(DEVICE)\n",
    "\n",
    "    translation, _ = model.translate(x, x_len, bos)\n",
    "    translation = [zh_itow[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print('模型翻译结果：', \" \".join(trans))\n",
    "\n",
    "\n",
    "for i in range(50, 100):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 练习一：Bi-LSTM + attention 用于情感分类\n",
    "\n",
    "补全代码：我们使用构建一个Bi-LSTM + attention模型完成文本分类任务，数据使用IMDb电影评论数据集，检测一段文字的情感是正面还是负面。\n",
    "\n",
    "[论文](https://aclanthology.org/P16-2034.pdf)\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limengyuan/anaconda3/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/home/limengyuan/anaconda3/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split = 'train')\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.insert_token(\"<pad>\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: 0 if x == 'neg' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):  #自定义的batch输出\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    batch.sort(key = lambda x: len(text_pipeline(x[1])), reverse = True)  #按照长度的大小进行排序\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text))\n",
    "    text_list = pad_sequence(text_list, padding_value = vocab.get_stoi()[\"<pad>\"],\n",
    "                             batch_first = True)  #进行填充，每个batch中的句子需要有相同的长度\n",
    "    return torch.tensor(label_list), text_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/limengyuan/anaconda3/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/home/limengyuan/anaconda3/lib/python3.9/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n"
     ]
    }
   ],
   "source": [
    "train_iter, test_iter = IMDB(root = 'data', split = ('train', 'test'))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size = 128,\n",
    "                              shuffle = True, collate_fn = collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size = 128,\n",
    "                              shuffle = True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128,\n",
    "                             shuffle = True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "        0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "        1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]  #标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 13, 163,   9,  ..., 131, 117,   3],\n",
       "        [822,  15,  12,  ...,   1,   1,   1],\n",
       "        [ 87, 637,  18,  ...,   1,   1,   1],\n",
       "        ...,\n",
       "        [ 13, 438,   8,  ...,   1,   1,   1],\n",
       "        [115, 361,   4,  ...,   1,   1,   1],\n",
       "        [ 14,  21,  52,  ...,   1,   1,   1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]  #batch_size,max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1112,\n",
       " 941,\n",
       " 910,\n",
       " 899,\n",
       " 897,\n",
       " 800,\n",
       " 759,\n",
       " 759,\n",
       " 738,\n",
       " 729,\n",
       " 728,\n",
       " 718,\n",
       " 655,\n",
       " 645,\n",
       " 583,\n",
       " 575,\n",
       " 569,\n",
       " 568,\n",
       " 512,\n",
       " 496,\n",
       " 486,\n",
       " 469,\n",
       " 464,\n",
       " 454,\n",
       " 422,\n",
       " 408,\n",
       " 405,\n",
       " 404,\n",
       " 388,\n",
       " 386,\n",
       " 383,\n",
       " 383,\n",
       " 380,\n",
       " 378,\n",
       " 365,\n",
       " 362,\n",
       " 339,\n",
       " 332,\n",
       " 328,\n",
       " 319,\n",
       " 317,\n",
       " 314,\n",
       " 310,\n",
       " 287,\n",
       " 282,\n",
       " 278,\n",
       " 264,\n",
       " 258,\n",
       " 254,\n",
       " 252,\n",
       " 252,\n",
       " 246,\n",
       " 245,\n",
       " 242,\n",
       " 237,\n",
       " 236,\n",
       " 227,\n",
       " 227,\n",
       " 222,\n",
       " 222,\n",
       " 220,\n",
       " 218,\n",
       " 216,\n",
       " 216,\n",
       " 215,\n",
       " 214,\n",
       " 209,\n",
       " 209,\n",
       " 206,\n",
       " 203,\n",
       " 201,\n",
       " 201,\n",
       " 198,\n",
       " 196,\n",
       " 192,\n",
       " 190,\n",
       " 189,\n",
       " 186,\n",
       " 185,\n",
       " 183,\n",
       " 173,\n",
       " 172,\n",
       " 169,\n",
       " 168,\n",
       " 167,\n",
       " 167,\n",
       " 165,\n",
       " 165,\n",
       " 165,\n",
       " 158,\n",
       " 158,\n",
       " 158,\n",
       " 157,\n",
       " 153,\n",
       " 152,\n",
       " 151,\n",
       " 150,\n",
       " 148,\n",
       " 147,\n",
       " 146,\n",
       " 146,\n",
       " 144,\n",
       " 144,\n",
       " 143,\n",
       " 142,\n",
       " 141,\n",
       " 141,\n",
       " 140,\n",
       " 135,\n",
       " 133,\n",
       " 131,\n",
       " 128,\n",
       " 128,\n",
       " 126,\n",
       " 116,\n",
       " 114,\n",
       " 110,\n",
       " 108,\n",
       " 105,\n",
       " 103,\n",
       " 95,\n",
       " 88,\n",
       " 77,\n",
       " 76,\n",
       " 75,\n",
       " 71,\n",
       " 61,\n",
       " 56]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]  #lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "模型分为五个部分\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)\n",
    "\n",
    "- 输入层（Input layer）：将句子输入模型\n",
    "- 嵌入层（Embedding layer）：将每个词映射到一个低维向量\n",
    "- LSTM层（LSTM layer）：利用BiLSTM从词向量中获得特征\n",
    "- Attention层（Attention layer）：生成权重向量，将每个时间步长的单词级特征与权重向量相乘，合并成句子级特征向量（补全代码）\n",
    "- 输出层（Output layer）： 对句子进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class bilstm_attn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 dropout_rate, pad_id):\n",
    "        super(bilstm_attn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_id)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = True,\n",
    "                            dropout = dropout_rate, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \"\"\"\n",
    "            Write your code here.\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.dropout(self.embedding(x))  # [batch size,seq len] -> [batch size,seq len,embedding_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first = True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden = [n layers *2, batch size, hidden dim]最后一个step的hidden\n",
    "        # cell = [n layers * 2, batch size, hidden dim]最终一个step的cell\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first = True)\n",
    "        # output = [batch size, seq len, hidden dim * 2]#每一个step下的最后一层的output\n",
    "        output = output.reshape(output.shape[0], output.shape[1], 2, -1)\n",
    "        # output = [batch size, seq len, 2,hidden dim]\n",
    "        output = torch.sum(output, dim = 2)\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        \"\"\"\n",
    "            Write your code here.\n",
    "        \"\"\"\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5\n",
    "pad_id = vocab.get_stoi()[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = bilstm_attn(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate, pad_id).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "    model.train()  #model.train()代表了训练模式\n",
    "    for label, text, lengths in train_loader:\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "\n",
    "        out = model(text, lengths)\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "        _, pred = torch.max(out.data, 1)\n",
    "        corrects += (pred == label).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()  #加这步防止梯度叠加\n",
    "        loss.backward()  #反向传播\n",
    "        optimizer.step()  #梯度下降\n",
    "\n",
    "        epoch_loss += loss.item() * len(label)\n",
    "        #loss.item()已经本身除以了len(batch.label)\n",
    "        #所以得再乘一次，得到一个batch的损失，累加得到所有样本损失。\n",
    "\n",
    "        total_len += len(label)\n",
    "        #计算train_iterator所有样本的数量，不出意外应该是17500\n",
    "\n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    #转换成测试模式，冻结dropout层或其他层。\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text, lengths in valid_loader:\n",
    "            #iterator为valid_iterator\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            out = model(text, lengths)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            _, pred = torch.max(out.data, 1)\n",
    "            corrects += (pred == label).sum().item()\n",
    "\n",
    "            epoch_loss += loss.item() * len(label)\n",
    "            total_len += len(label)\n",
    "    model.train()  #调回训练模式\n",
    "\n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 train_loss: 0.6832094741921676 train_acc 0.5885052631578948\n",
      "epoch: 0 valid_loss: 0.5922406481742859 valid_acc 0.6976\n",
      "epoch: 1 train_loss: 0.5618076030480235 train_acc 0.709978947368421\n",
      "epoch: 1 valid_loss: 0.47752310166358947 valid_acc 0.772\n",
      "epoch: 2 train_loss: 0.46739850202108685 train_acc 0.7832842105263158\n",
      "epoch: 2 valid_loss: 0.45280455932617186 valid_acc 0.8024\n",
      "epoch: 3 train_loss: 0.405346336911854 train_acc 0.8201263157894737\n",
      "epoch: 3 valid_loss: 0.41220421171188354 valid_acc 0.8248\n",
      "epoch: 4 train_loss: 0.3586794477236898 train_acc 0.844378947368421\n",
      "epoch: 4 valid_loss: 0.35476437602043154 valid_acc 0.8536\n",
      "epoch: 5 train_loss: 0.3227162823275516 train_acc 0.8635368421052632\n",
      "epoch: 5 valid_loss: 0.4248126239776611 valid_acc 0.8312\n",
      "epoch: 6 train_loss: 0.2943850700114903 train_acc 0.8784421052631579\n",
      "epoch: 6 valid_loss: 0.3186738593578339 valid_acc 0.8664\n",
      "epoch: 7 train_loss: 0.27103296057676013 train_acc 0.8895157894736843\n",
      "epoch: 7 valid_loss: 0.33145447697639463 valid_acc 0.8664\n",
      "epoch: 8 train_loss: 0.25032097199716064 train_acc 0.8989894736842106\n",
      "epoch: 8 valid_loss: 0.32759395637512206 valid_acc 0.8784\n",
      "epoch: 9 train_loss: 0.23404318929973403 train_acc 0.9073263157894736\n",
      "epoch: 9 valid_loss: 0.4192514950275421 valid_acc 0.8552\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, loss_fn)\n",
    "    print(\"epoch:\", epoch, \"train_loss:\", train_loss, \"train_acc\", train_acc)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader)\n",
    "    print(\"epoch:\", epoch, \"valid_loss:\", valid_loss, \"valid_acc\", valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bilstm_attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    text = text_pipeline(text)\n",
    "\n",
    "    length = torch.LongTensor([len(text)])\n",
    "    tensor = torch.LongTensor(text).unsqueeze(0).to(device)\n",
    "\n",
    "    out = model(tensor, length)\n",
    "    _, pred = torch.max(out.data, 1)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}