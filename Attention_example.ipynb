{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  seq2seq + Luong 注意力 进行中英文翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "from collections import Counter  #计数器\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer  #分词器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用Luong 注意力进行中英翻译训练，[数据来源](https://github.com/cuicaihao/Annotated-Transformer-English-to-Chinese-Translator/tree/master/data/nmt/en-cn)，包含成对的中英文翻译句子，包含7个文件，cmn.txt是全体数据集；train.txt, dev.txt, test.txt为对全体数据集的80%、10%、\n",
    "10%划分。train_mini.txt, dev_mini.txt, test_mini.txt为小样本数据，分别包含1000、200、200条数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNK_IDX = 0  #未知\n",
    "PAD_IDX = 1  #\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30\n",
    "DROPOUT = 0.2\n",
    "ENC_HIDDEN_SIZE = DEC_HIDDEN_SIZE = 100\n",
    "EMBED_SIZE = 100\n",
    "DEBUG = True\n",
    "\n",
    "if DEBUG:\n",
    "\n",
    "    train_file = 'attn_data/train_mini.txt'\n",
    "    dev_file = 'attn_data/dev_mini.txt'\n",
    "    test_file = 'attn_data/test_mini.txt'\n",
    "    save_file = 'attn_data/model.pt'\n",
    "else:\n",
    "\n",
    "    train_file = 'attn_data/train.txt'\n",
    "    dev_file = 'attn_data/dev.txt'\n",
    "    test_file = 'attn_data/test.txt'\n",
    "    save_file = 'attn_data/large_model.pt'\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\NIEYUZ~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.684 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#分词器\n",
    "tokenizer_en = get_tokenizer('basic_english')  #按空格进行分割\n",
    "tokenizer_cn = get_tokenizer(jieba.lcut)  #进行结巴分词\n",
    "\n",
    "\n",
    "#加载文件\n",
    "def load_data(path):\n",
    "    en = []\n",
    "    cn = []\n",
    "    with open(path, 'r', encoding = 'utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split('\\t')\n",
    "            en.append([\"BOS\"] + tokenizer_en(line[0].lower()) + [\"EOS\"])  #小写\n",
    "            cn.append([\"BOS\"] + tokenizer_cn(line[1]) + [\"EOS\"])\n",
    "    return en, cn\n",
    "\n",
    "\n",
    "train_en, train_zh = load_data(train_file)\n",
    "dev_en, dev_zh = load_data(dev_file)\n",
    "test_en, test_zh = load_data(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOS', 'anyone', 'can', 'do', 'that', '.', 'EOS'] ['BOS', '任何人', '都', '可以', '做到', '。', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "print(train_en[0], train_zh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#构建词汇表\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "    vocab = Counter(np.concatenate(sentences)).most_common(max_words)  #最大单词数是50000\n",
    "    word_to_id = {w[0]: index + 2 for index, w in enumerate(vocab)}\n",
    "    word_to_id['UNK'] = UNK_IDX  #0\n",
    "    word_to_id['PAD'] = PAD_IDX  #1\n",
    "    id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "\n",
    "en_wtoi, en_itow = build_dict(train_en)\n",
    "zh_wtoi, zh_itow = build_dict(train_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "{2: 'BOS',\n 3: 'EOS',\n 4: '.',\n 5: 'i',\n 6: 'the',\n 7: \"'\",\n 8: 'you',\n 9: 'to',\n 10: 'a',\n 11: '?',\n 12: 'is',\n 13: 'he',\n 14: 't',\n 15: 'in',\n 16: 'it',\n 17: 'of',\n 18: 'she',\n 19: 's',\n 20: 'have',\n 21: 'me',\n 22: 'tom',\n 23: ',',\n 24: 'do',\n 25: 'that',\n 26: 'for',\n 27: 'my',\n 28: 'don',\n 29: 'are',\n 30: 'her',\n 31: 'what',\n 32: 'this',\n 33: 'at',\n 34: 'with',\n 35: 'your',\n 36: 'we',\n 37: 'was',\n 38: 'not',\n 39: 'like',\n 40: 'on',\n 41: 'his',\n 42: 'can',\n 43: 'has',\n 44: 'm',\n 45: 'be',\n 46: 'go',\n 47: 'him',\n 48: 'please',\n 49: 'they',\n 50: 'will',\n 51: 'there',\n 52: 'from',\n 53: 'want',\n 54: 'and',\n 55: 'know',\n 56: 'how',\n 57: 'very',\n 58: 'll',\n 59: 'here',\n 60: 'an',\n 61: 'didn',\n 62: 'about',\n 63: 'should',\n 64: 'did',\n 65: 'get',\n 66: 'had',\n 67: 'up',\n 68: 've',\n 69: 'by',\n 70: 'out',\n 71: 'all',\n 72: 'need',\n 73: 'time',\n 74: 'why',\n 75: 'many',\n 76: 'no',\n 77: 'could',\n 78: 'some',\n 79: 'good',\n 80: 'one',\n 81: 'day',\n 82: 'give',\n 83: 'made',\n 84: 'would',\n 85: 'school',\n 86: 'last',\n 87: 'too',\n 88: 'car',\n 89: 'now',\n 90: 'see',\n 91: 'when',\n 92: 'am',\n 93: 'must',\n 94: 'house',\n 95: 'think',\n 96: 'told',\n 97: 'much',\n 98: 'us',\n 99: 'more',\n 100: 'english',\n 101: 'may',\n 102: 'going',\n 103: 'something',\n 104: 'father',\n 105: 'anything',\n 106: 'come',\n 107: 'new',\n 108: 'love',\n 109: 'every',\n 110: 'after',\n 111: 'always',\n 112: 'got',\n 113: 'our',\n 114: 'as',\n 115: 'just',\n 116: 'money',\n 117: 'if',\n 118: 'were',\n 119: 'tomorrow',\n 120: 'been',\n 121: 'book',\n 122: 'before',\n 123: 'people',\n 124: 'leave',\n 125: 'drink',\n 126: 'little',\n 127: 'asked',\n 128: 'mary',\n 129: 'wish',\n 130: 'home',\n 131: 'd',\n 132: 'went',\n 133: 'party',\n 134: 'off',\n 135: 'room',\n 136: 'two',\n 137: 'speak',\n 138: 'really',\n 139: 'around',\n 140: 'does',\n 141: 'or',\n 142: 'take',\n 143: 'so',\n 144: 'where',\n 145: 'never',\n 146: 'look',\n 147: 'understand',\n 148: 'read',\n 149: 'say',\n 150: 'man',\n 151: 'night',\n 152: 'mother',\n 153: 'who',\n 154: 'today',\n 155: 're',\n 156: 'than',\n 157: 'week',\n 158: 'teacher',\n 159: 'letter',\n 160: 'children',\n 161: 'swimming',\n 162: 'any',\n 163: 'still',\n 164: 'sick',\n 165: 'train',\n 166: 'problem',\n 167: 'door',\n 168: 'tired',\n 169: 'arrived',\n 170: 'yesterday',\n 171: 'doing',\n 172: 'right',\n 173: 'coffee',\n 174: 'into',\n 175: 'but',\n 176: 'ask',\n 177: 'them',\n 178: 'morning',\n 179: 'show',\n 180: 'live',\n 181: 'enough',\n 182: 'anyone',\n 183: 'another',\n 184: 'baby',\n 185: 'play',\n 186: 'tennis',\n 187: 'baseball',\n 188: 'cold',\n 189: 'often',\n 190: 'used',\n 191: 'keep',\n 192: 'heard',\n 193: 'fun',\n 194: 'boy',\n 195: 'help',\n 196: 'pass',\n 197: 'same',\n 198: 'those',\n 199: 'reading',\n 200: 'back',\n 201: 'wouldn',\n 202: 'alone',\n 203: 'said',\n 204: 'class',\n 205: 'fell',\n 206: 'quit',\n 207: 'lost',\n 208: 'three',\n 209: 'questions',\n 210: 'wanted',\n 211: 'afraid',\n 212: 'first',\n 213: 'playing',\n 214: 'tv',\n 215: 'let',\n 216: 'looks',\n 217: 'other',\n 218: 'flowers',\n 219: 'garden',\n 220: 'stand',\n 221: 'write',\n 222: 'accident',\n 223: 'sorry',\n 224: 'friends',\n 225: 'wants',\n 226: 'yet',\n 227: 'doesn',\n 228: 'knew',\n 229: 'old',\n 230: 'dog',\n 231: 'saw',\n 232: 'stay',\n 233: 'work',\n 234: 'married',\n 235: 'whole',\n 236: 'game',\n 237: 'best',\n 238: 'bad',\n 239: 'away',\n 240: 'water',\n 241: 'hard',\n 242: 'left',\n 243: 'milk',\n 244: 'friend',\n 245: 'apples',\n 246: 'wrong',\n 247: 'lives',\n 248: 'age',\n 249: 'eating',\n 250: 'open',\n 251: 'hand',\n 252: 'finished',\n 253: 'isn',\n 254: 'only',\n 255: 'goes',\n 256: 'study',\n 257: 'spent',\n 258: 'french',\n 259: 'late',\n 260: 'able',\n 261: 'miss',\n 262: 'hope',\n 263: 'dollars',\n 264: 'meeting',\n 265: 'gave',\n 266: 'young',\n 267: 'again',\n 268: 'smoking',\n 269: 'drive',\n 270: 'answer',\n 271: 'might',\n 272: 'soon',\n 273: 'swim',\n 274: 'year',\n 275: 'living',\n 276: 'years',\n 277: 'tell',\n 278: 'juice',\n 279: 'better',\n 280: 'long',\n 281: 'lock',\n 282: 'these',\n 283: 'way',\n 284: 'plays',\n 285: 'until',\n 286: 'box',\n 287: 'well',\n 288: 'everyone',\n 289: 'looking',\n 290: 'turn',\n 291: 'uncle',\n 292: 'question',\n 293: 'important',\n 294: 'charge',\n 295: 'haven',\n 296: 'quiet',\n 297: 'over',\n 298: 'knows',\n 299: 'down',\n 300: 'happened',\n 301: 'name',\n 302: 'red',\n 303: 'japan',\n 304: 'talk',\n 305: 'rich',\n 306: 'being',\n 307: 'between',\n 308: 'offer',\n 309: 'fire',\n 310: 'feelings',\n 311: 'bought',\n 312: 'camera',\n 313: 'loves',\n 314: 'learned',\n 315: 'make',\n 316: 'big',\n 317: '!',\n 318: 'days',\n 319: 'getting',\n 320: 'doctor',\n 321: 'window',\n 322: 'fox',\n 323: 'smoke',\n 324: 'making',\n 325: 'heavy',\n 326: 'snow',\n 327: 'person',\n 328: 'chance',\n 329: 'language',\n 330: 'asleep',\n 331: 'started',\n 332: 'aunt',\n 333: 'music',\n 334: 'bird',\n 335: 'office',\n 336: 'near',\n 337: 'station',\n 338: 'dinner',\n 339: 'phone',\n 340: 'size',\n 341: 'building',\n 342: 'light',\n 343: 'sometimes',\n 344: 'control',\n 345: 'everything',\n 346: 'easy',\n 347: 'under',\n 348: 'dress',\n 349: 'clock',\n 350: 'bus',\n 351: 'busy',\n 352: 'recently',\n 353: 'their',\n 354: 'since',\n 355: 'takes',\n 356: 'sun',\n 357: 'concert',\n 358: 'eat',\n 359: 'brother',\n 360: 'rent',\n 361: 'birthday',\n 362: 'oranges',\n 363: 'seen',\n 364: 'feel',\n 365: 'kept',\n 366: 'waiting',\n 367: 'answered',\n 368: 'died',\n 369: 'fever',\n 370: 'broken',\n 371: 'tokyo',\n 372: 'real',\n 373: 'himself',\n 374: 'careful',\n 375: 'small',\n 376: 'london',\n 377: 'head',\n 378: 'umbrella',\n 379: 'mean',\n 380: 'showed',\n 381: 'most',\n 382: 'ago',\n 383: 'salt',\n 384: 'picture',\n 385: 'christmas',\n 386: 'bicycle',\n 387: 'seats',\n 388: 'earlier',\n 389: 'ticket',\n 390: 'lot',\n 391: 'coming',\n 392: 'tonight',\n 393: 'put',\n 394: 'thank',\n 395: 'wrote',\n 396: 'everybody',\n 397: 'river',\n 398: 'care',\n 399: 'five',\n 400: 'idea',\n 401: 'rather',\n 402: 'next',\n 403: 'lunch',\n 404: 'hokkaido',\n 405: 'likes',\n 406: 'pen',\n 407: 'both',\n 408: 'dead',\n 409: 'end',\n 410: 'hungry',\n 411: 'which',\n 412: 'news',\n 413: 'radio',\n 414: 'running',\n 415: 'war',\n 416: 'succeed',\n 417: 'hot',\n 418: 'library',\n 419: 'second',\n 420: 'floor',\n 421: 'books',\n 422: 'closed',\n 423: 'try',\n 424: 'truth',\n 425: 'heartless',\n 426: 'thing',\n 427: 'suits',\n 428: 'carry',\n 429: 'bag',\n 430: 'matter',\n 431: 'exam',\n 432: 'taken',\n 433: 'flower',\n 434: 'homework',\n 435: 'felt',\n 436: 'done',\n 437: 'happy',\n 438: 'sunday',\n 439: 'novel',\n 440: 'far',\n 441: 'cut',\n 442: '3',\n 443: 'listening',\n 444: 'because',\n 445: 'minutes',\n 446: 'use',\n 447: 'target',\n 448: 'unlikely',\n 449: 'speaker',\n 450: 'level',\n 451: 'dark',\n 452: 'none',\n 453: 'find',\n 454: 'church',\n 455: 'thousand',\n 456: 'cover',\n 457: 'dangerous',\n 458: 'called',\n 459: 'beautiful',\n 460: 'ran',\n 461: 'having',\n 462: 'hold',\n 463: 'six',\n 464: 'common',\n 465: 'exactly',\n 466: 'happen',\n 467: 'while',\n 468: 'bed',\n 469: 'across',\n 470: 'walk',\n 471: 'front',\n 472: 'pretty',\n 473: 'each',\n 474: 'stopped',\n 475: 'america',\n 476: 'grow',\n 477: 'interesting',\n 478: 'story',\n 479: 'slept',\n 480: 'city',\n 481: 'orbit',\n 482: 'surprised',\n 483: 'switch',\n 484: 'promise',\n 485: 'place',\n 486: 'curtain',\n 487: 'expect',\n 488: 'shouldn',\n 489: 'eaten',\n 490: 'ice',\n 491: 'point',\n 492: 'strange',\n 493: 'optimist',\n 494: 'nature',\n 495: 'returned',\n 496: 'month',\n 497: 'blame',\n 498: 'deal',\n 499: 'hurts',\n 500: 'studying',\n 501: 'student',\n 502: 'storm',\n 503: 'earth',\n 504: 'took',\n 505: 'forgot',\n 506: 'animals',\n 507: 'became',\n 508: 'pictures',\n 509: 'glass',\n 510: 'passport',\n 511: 'lung',\n 512: 'cancer',\n 513: 'enjoy',\n 514: 'missed',\n 515: 'rotten',\n 516: 'changed',\n 517: 'kitchen',\n 518: 'mind',\n 519: 'high',\n 520: 'street',\n 521: 'noon',\n 522: 'whether',\n 523: 'either',\n 524: 'full',\n 525: 'believe',\n 526: 'foreign',\n 527: 'weight',\n 528: 'complaints',\n 529: 'movies',\n 530: 'sundays',\n 531: 'taste',\n 532: 'agreed',\n 533: 'proposal',\n 534: 'ended',\n 535: 'television',\n 536: 'boys',\n 537: 'telephone',\n 538: 'number',\n 539: 'invited',\n 540: 'china',\n 541: 'snack',\n 542: 'cook',\n 543: 'computers',\n 544: 'life',\n 545: 'recover',\n 546: 'zoo',\n 547: 'park',\n 548: 'death',\n 549: 'states',\n 550: 'check',\n 551: 'tire',\n 552: 'words',\n 553: 'students',\n 554: 'rome',\n 555: 'paris',\n 556: 'things',\n 557: 'soup',\n 558: 'broke',\n 559: 'prevented',\n 560: 'airport',\n 561: 'met',\n 562: 'few',\n 563: 'sure',\n 564: 'menu',\n 565: 'forget',\n 566: 'plan',\n 567: 'ten',\n 568: 'girl',\n 569: 'greeted',\n 570: 'smile',\n 571: 'table',\n 572: 'own',\n 573: 'men',\n 574: 'maybe',\n 575: 'draw',\n 576: 'apple',\n 577: 'belongs',\n 578: 'borrow',\n 579: 'carried',\n 580: 'send',\n 581: 'newspaper',\n 582: 'ever',\n 583: 'shoes',\n 584: 'aren',\n 585: 'picked',\n 586: 'satisfied',\n 587: 'winter',\n 588: 'older',\n 589: 'fall',\n 590: 'stayed',\n 591: 'remember',\n 592: 'talks',\n 593: 'meat',\n 594: 'hurry',\n 595: 'weather',\n 596: 'wedding',\n 597: 'fruit',\n 598: 'hasn',\n 599: 'passengers',\n 600: 'university',\n 601: 'bring',\n 602: 'present',\n 603: 'pay',\n 604: 'against',\n 605: 'chemistry',\n 606: 'parents',\n 607: 'begin',\n 608: 'afford',\n 609: 'less',\n 610: 'cannot',\n 611: 'losing',\n 612: 'probably',\n 613: 'beef',\n 614: 'country',\n 615: 'early',\n 616: 'hawaii',\n 617: 'throw',\n 618: 'start',\n 619: 'taking',\n 620: 'excuse',\n 621: 'sister',\n 622: 'cost',\n 623: 'face',\n 624: 'famous',\n 625: 'absent',\n 626: 'foot',\n 627: 'call',\n 628: 'eight',\n 629: 'planning',\n 630: 'taxi',\n 631: 'stone',\n 632: 'somewhere',\n 633: 'once',\n 634: 'currently',\n 635: 'strong',\n 636: 'catch',\n 637: 'brothers',\n 638: 'sisters',\n 639: 'buy',\n 640: 'watching',\n 641: 'date',\n 642: 'already',\n 643: 'learn',\n 644: 'fifty',\n 645: 'woman',\n 646: 'breakfast',\n 647: 'lived',\n 648: 'cruel',\n 649: 'police',\n 650: 'mt',\n 651: 'fuji',\n 652: 'tall',\n 653: 'piece',\n 654: 'cake',\n 655: 'learning',\n 656: 'irregular',\n 657: 'verbs',\n 658: 'ball',\n 659: 'sleeping',\n 660: 'cancel',\n 661: 'hike',\n 662: 'dealing',\n 663: 'cooking',\n 664: 'recovered',\n 665: 'rejected',\n 666: 'quotes',\n 667: 'milton',\n 668: 'mommy',\n 669: 'miyazaki',\n 670: 'pain',\n 671: 'oil',\n 672: 'button',\n 673: 'push',\n 674: 'opposed',\n 675: 'type',\n 676: 'throne',\n 677: 'low',\n 678: 'salary',\n 679: 'prevents',\n 680: 'buying',\n 681: 'hang',\n 682: 'coat',\n 683: 'hook',\n 684: 'hide',\n 685: 'dad',\n 686: 'irresistible',\n 687: 'such',\n 688: 'calls',\n 689: 'tailor',\n 690: 'drove',\n 691: 'truck',\n 692: 'dallas',\n 693: 'smiled',\n 694: 'propose',\n 695: 'cheese',\n 696: 'housework',\n 697: 'spoke',\n 698: 'thought',\n 699: 'failed',\n 700: 'dare',\n 701: 'solve',\n 702: 'longer',\n 703: 'industry',\n 704: 'exist',\n 705: 'drop',\n 706: 'difficulty',\n 707: 'driver',\n 708: 'license',\n 709: 'run',\n 710: 'line',\n 711: 'large',\n 712: 'eggs',\n 713: 'pale',\n 714: 'counting',\n 715: 'honor',\n 716: 'stake',\n 717: 'matters',\n 718: 'misunderstanding',\n 719: 'hunter',\n 720: 'shot',\n 721: 'quietly',\n 722: 'spoken',\n 723: 'spanish',\n 724: 'nothing',\n 725: 'fries',\n 726: 'hours',\n 727: 'boasts',\n 728: 'plane',\n 729: 'beijing',\n 730: '20',\n 731: 'vodka',\n 732: 'acquire',\n 733: 'adult',\n 734: 'reach',\n 735: 'native',\n 736: 'hurried',\n 737: 'safely',\n 738: 'useful',\n 739: 'stupid',\n 740: 'fishing',\n 741: 'expenses',\n 742: 'welcomed',\n 743: 'warmly',\n 744: 'pulled',\n 745: 'wishes',\n 746: 'guilty',\n 747: 'fly',\n 748: 'swarm',\n 749: 'hornets',\n 750: 'attacked',\n 751: 'stirred',\n 752: 'teaspoon',\n 753: 'cell',\n 754: 'built-in',\n 755: 'digital',\n 756: 'horses',\n 757: 'divided',\n 758: 'equals',\n 759: 'bookkeeping',\n 760: 'hamburgers',\n 761: 'liked',\n 762: 'academic',\n 763: 'fraud',\n 764: 'lake',\n 765: 'charles',\n 766: 'lindbergh',\n 767: 'solo',\n 768: 'flight',\n 769: 'atlantic',\n 770: 'ocean',\n 771: '1927',\n 772: 'shirt',\n 773: 'stained',\n 774: 'sauce',\n 775: 'found',\n 776: 'locked',\n 777: 'held',\n 778: 'package',\n 779: 'arm',\n 780: 'thanks',\n 781: 'argued',\n 782: 'slipped',\n 783: 'classroom',\n 784: 'ought',\n 785: 'permission',\n 786: 'received',\n 787: 'aside',\n 788: '2020',\n 789: 'population',\n 790: 'doubled',\n 791: 'swimmers',\n 792: 'numb',\n 793: 'moons',\n 794: 'saturn',\n 795: 'volcanic',\n 796: 'ash',\n 797: 'disrupted',\n 798: 'air',\n 799: 'travel',\n 800: 'marriage',\n 801: 'mom',\n 802: 'earned',\n 803: 'chocolate',\n 804: 'starving',\n 805: 'mysterious',\n 806: 'atmosphere',\n 807: '165',\n 808: 'neptune',\n 809: 'punctual',\n 810: 'cream',\n 811: 'gambling',\n 812: 'cows',\n 813: 'provide',\n 814: 'enjoyed',\n 815: 'herself',\n 816: 'erased',\n 817: 'speech',\n 818: 'tape',\n 819: 'larger',\n 820: 'halley',\n 821: 'comet',\n 822: '2061',\n 823: 'anymore',\n 824: 'brought',\n 825: 'computer',\n 826: 'difference',\n 827: 'needed',\n 828: 'treatment',\n 829: 'twenties',\n 830: 'younger',\n 831: 'stingy',\n 832: 'bottle',\n 833: 'wine',\n 834: 'fluently',\n 835: 'oscar',\n 836: 'ceremonies',\n 837: 'hollywood',\n 838: 'biggest',\n 839: 'extravaganza',\n 840: 'per',\n 841: 'bike',\n 842: 'squeezed',\n 843: 'guy',\n 844: 'obvious',\n 845: 'stomach',\n 846: 'memory',\n 847: 'roof',\n 848: 'damaged',\n 849: 'artificial',\n 850: 'satellites',\n 851: 'revolving',\n 852: 'teaches',\n 853: 'turned',\n 854: 'report',\n 855: 'climate',\n 856: 'cough',\n 857: 'managed',\n 858: 'half',\n 859: 'trains',\n 860: 'planting',\n 861: 'roses',\n 862: 'guests',\n 863: 'appreciate',\n 864: 'cooperation',\n 865: 'weatherman',\n 866: 'says',\n 867: 'graduated',\n 868: 'straight',\n 869: 'prohibited',\n 870: 'poured',\n 871: 'cup',\n 872: 'awful',\n 873: 'orange',\n 874: 'continued',\n 875: 'singing',\n 876: 'rescued',\n 877: 'drowning',\n 878: 'traffic',\n 879: 'cross',\n 880: 'golf',\n 881: 'times',\n 882: 'whatever',\n 883: 'interfere',\n 884: 'affairs',\n 885: 'alert',\n 886: 'mysteries',\n 887: 'barking',\n 888: 'tree',\n 889: 'letting',\n 890: 'theater',\n 891: 'hunting',\n 892: 'agency',\n 893: 'executive',\n 894: 'position',\n 895: 'great',\n 896: 'progress',\n 897: 'engaged',\n 898: 'trade',\n 899: 'operation',\n 900: 'color',\n 901: 'skirt',\n 902: 'beer',\n 903: 'acquired',\n 904: 'interested',\n 905: 'turkey',\n 906: 'hat',\n 907: 'intention',\n 908: 'asking',\n 909: 'needle',\n 910: 'haystack',\n 911: 'summer',\n 912: 'sad',\n 913: 'stock',\n 914: 'concentrate',\n 915: 'health',\n 916: 'farm',\n 917: 'watering',\n 918: 'loved',\n 919: 'distance',\n 920: 'seemed',\n 921: 'opinion',\n 922: 'forward',\n 923: 'japanese',\n 924: 'foods',\n 925: 'different',\n 926: 'certainly',\n 927: 'role',\n 928: 'worn',\n 929: 'worked',\n 930: 'abroad',\n 931: 'burst',\n 932: 'tears',\n 933: 'deleted',\n 934: 'file',\n 935: 'novels',\n 936: 'without',\n 937: 'heroes',\n 938: 'suggested',\n 939: 'change',\n 940: 'hairdressers',\n 941: 'gay',\n 942: 'elderberry',\n 943: 'studied',\n 944: 'penalty',\n 945: 'usa',\n 946: 'perfectly',\n 947: 'extraordinary',\n 948: 'ringing',\n 949: 'pressure',\n 950: 'row',\n 951: '5',\n 952: 'obey',\n 953: 'teachers',\n 954: 'express',\n 955: 'deep',\n 956: 'sorrow',\n 957: 'response',\n 958: 'forty',\n 959: 'shell',\n 960: 'egg',\n 961: 'easily',\n 962: 'guts',\n 963: 'period',\n 964: 'profession',\n 965: 'guitarist',\n 966: 'starting',\n 967: 'flat',\n 968: 'course',\n 969: 'seriously',\n 970: 'pillow',\n 971: 'blanket',\n 972: 'stir',\n 973: 'almost',\n 974: 'final',\n 975: 'law',\n 976: 'applicable',\n 977: 'cases',\n 978: 'healthy',\n 979: 'breathe',\n 980: 'deeply',\n 981: 'schools',\n 982: 'medicine',\n 983: 'couple',\n 984: 'walking',\n 985: 'chocolates',\n 986: 'displeased',\n 987: 'stick',\n 988: 'stamp',\n 989: 'envelope',\n 990: 'app',\n 991: 'folds',\n 992: 'paper',\n 993: 'cranes',\n 994: 'true',\n 995: '1988',\n 996: 'works',\n 997: 'factory',\n 998: 'usually',\n 999: 'inviting',\n 1000: 'entering',\n 1001: 'tripped',\n ...}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_itow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 利用词典对原始句子编码 单词->数字\n",
    "def encode(en_sentences, ch_sentences, en_wtoi, zh_wtoi, sort_by_len = True):\n",
    "    out_en_sentences = [[en_wtoi.get(w, UNK_IDX) for w in sent] for sent in en_sentences]\n",
    "    out_ch_sentences = [[zh_wtoi.get(w, UNK_IDX) for w in sent] for sent in ch_sentences]\n",
    "\n",
    "    #返回w对应的值，否则返回UNK_IDX\n",
    "    def len_argsort(seq):  #按照长度进行排序\n",
    "        return sorted(range(len(seq)), key = lambda x: len(seq[x]))\n",
    "\n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_ch_sentences = [out_ch_sentences[i] for i in sorted_index]\n",
    "\n",
    "    return out_en_sentences, out_ch_sentences\n",
    "\n",
    "\n",
    "train_en_encode, train_zh_encode = encode(train_en, train_zh, en_wtoi, zh_wtoi)\n",
    "dev_en_encode, dev_zh_encode = encode(dev_en, dev_zh, en_wtoi, zh_wtoi)\n",
    "test_en_encode, test_zh_encode = encode(test_en, test_zh, en_wtoi, zh_wtoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[[2, 423, 16, 4, 3],\n [2, 18, 435, 1162, 4, 3],\n [2, 56, 29, 8, 11, 3],\n [2, 1272, 35, 135, 4, 3],\n [2, 215, 21, 95, 4, 3],\n [2, 6, 339, 0, 4, 3],\n [2, 42, 182, 269, 11, 3],\n [2, 0, 313, 8, 4, 3],\n [2, 22, 0, 0, 4, 3],\n [2, 31, 63, 5, 601, 11, 3],\n [2, 13, 12, 76, 0, 4, 3],\n [2, 18, 132, 51, 170, 4, 3],\n [2, 13, 0, 6, 0, 4, 3],\n [2, 5, 115, 192, 103, 4, 3],\n [2, 5, 92, 52, 0, 4, 3],\n [2, 8, 558, 6, 0, 4, 3],\n [2, 5, 458, 30, 67, 4, 3],\n [2, 13, 0, 41, 0, 4, 3],\n [2, 198, 29, 353, 421, 4, 3],\n [2, 0, 100, 12, 738, 4, 3],\n [2, 144, 29, 27, 421, 11, 3],\n [2, 13, 0, 6, 0, 4, 3],\n [2, 13, 140, 137, 287, 4, 3],\n [2, 22, 0, 214, 170, 4, 3],\n [2, 0, 17, 6, 230, 317, 3],\n [2, 24, 8, 137, 0, 11, 3],\n [2, 6, 672, 1420, 134, 4, 3],\n [2, 6, 309, 0, 0, 4, 3],\n [2, 411, 12, 35, 121, 11, 3],\n [2, 43, 103, 79, 300, 11, 3],\n [2, 18, 247, 15, 107, 1344, 4, 3],\n [2, 28, 7, 14, 45, 0, 4, 3],\n [2, 5, 92, 10, 0, 59, 4, 3],\n [2, 22, 585, 67, 6, 1013, 4, 3],\n [2, 13, 93, 45, 62, 958, 4, 3],\n [2, 5, 37, 0, 69, 0, 4, 3],\n [2, 215, 7, 19, 38, 0, 4, 3],\n [2, 32, 159, 12, 0, 0, 4, 3],\n [2, 16, 7, 19, 35, 290, 4, 3],\n [2, 5, 115, 441, 27, 1027, 4, 3],\n [2, 13, 127, 21, 136, 209, 4, 3],\n [2, 49, 83, 193, 17, 128, 4, 3],\n [2, 13, 0, 62, 6, 1137, 4, 3],\n [2, 13, 504, 134, 41, 0, 4, 3],\n [2, 51, 29, 973, 76, 421, 4, 3],\n [2, 5, 20, 10, 316, 230, 4, 3],\n [2, 20, 8, 252, 646, 226, 11, 3],\n [2, 36, 29, 33, 85, 1307, 4, 3],\n [2, 49, 458, 134, 353, 0, 4, 3],\n [2, 22, 12, 10, 994, 150, 4, 3],\n [2, 16, 7, 19, 38, 0, 4, 3],\n [2, 36, 20, 10, 895, 1267, 4, 3],\n [2, 32, 12, 10, 492, 1207, 4, 3],\n [2, 24, 8, 256, 109, 81, 11, 3],\n [2, 25, 7, 19, 6, 491, 4, 3],\n [2, 10, 0, 12, 10, 0, 4, 3],\n [2, 22, 190, 9, 233, 59, 4, 3],\n [2, 8, 93, 1327, 6, 571, 4, 3],\n [2, 5, 7, 44, 38, 563, 4, 3],\n [2, 5, 7, 68, 0, 279, 4, 3],\n [2, 6, 1120, 12, 858, 436, 4, 3],\n [2, 0, 12, 0, 15, 540, 4, 3],\n [2, 0, 8, 7, 155, 21, 4, 3],\n [2, 253, 7, 14, 25, 0, 11, 3],\n [2, 13, 395, 9, 21, 170, 4, 3],\n [2, 18, 7, 19, 15, 6, 0, 4, 3],\n [2, 1116, 12, 87, 229, 9, 643, 4, 3],\n [2, 5, 96, 22, 5, 37, 0, 4, 3],\n [2, 118, 8, 15, 475, 86, 496, 11, 3],\n [2, 215, 7, 19, 0, 113, 0, 4, 3],\n [2, 36, 814, 0, 33, 6, 133, 4, 3],\n [2, 13, 7, 19, 27, 107, 244, 4, 3],\n [2, 6, 341, 12, 1197, 0, 519, 4, 3],\n [2, 56, 62, 102, 70, 26, 403, 11, 3],\n [2, 5, 7, 44, 0, 27, 1340, 4, 3],\n [2, 43, 18, 252, 6, 121, 226, 11, 3],\n [2, 5, 53, 47, 9, 148, 32, 4, 3],\n [2, 22, 228, 56, 9, 24, 25, 4, 3],\n [2, 48, 269, 6, 88, 99, 1264, 4, 3],\n [2, 48, 0, 6, 230, 109, 81, 4, 3],\n [2, 31, 7, 19, 246, 23, 0, 11, 3],\n [2, 28, 7, 14, 939, 35, 518, 4, 3],\n [2, 13, 647, 930, 26, 75, 276, 4, 3],\n [2, 183, 832, 17, 833, 23, 48, 4, 3],\n [2, 5, 20, 252, 0, 27, 135, 4, 3],\n [2, 364, 1155, 9, 176, 162, 209, 4, 3],\n [2, 8, 7, 58, 145, 45, 202, 4, 3],\n [2, 13, 577, 9, 6, 312, 1166, 4, 3],\n [2, 13, 996, 52, 0, 9, 0, 4, 3],\n [2, 91, 64, 8, 607, 500, 100, 11, 3],\n [2, 25, 0, 0, 57, 0, 0, 4, 3],\n [2, 77, 8, 24, 21, 10, 0, 11, 3],\n [2, 85, 0, 15, 0, 15, 0, 4, 3],\n [2, 27, 0, 12, 163, 10, 184, 4, 3],\n [2, 13, 66, 41, 0, 441, 0, 4, 3],\n [2, 13, 0, 21, 39, 41, 0, 4, 3],\n [2, 8, 101, 0, 162, 17, 177, 4, 3],\n [2, 101, 5, 393, 16, 299, 59, 11, 3],\n [2, 27, 104, 368, 1168, 276, 382, 4, 3],\n [2, 5, 28, 7, 14, 39, 0, 4, 3],\n [2, 5, 1330, 35, 0, 172, 239, 4, 3],\n [2, 6, 85, 827, 10, 107, 158, 4, 3],\n [2, 42, 8, 24, 25, 69, 1076, 11, 3],\n [2, 5, 55, 25, 18, 12, 723, 4, 3],\n [2, 8, 20, 642, 489, 6, 654, 4, 3],\n [2, 215, 7, 19, 65, 1307, 267, 317, 3],\n [2, 10, 0, 15, 73, 0, 0, 4, 3],\n [2, 5, 53, 9, 639, 32, 0, 4, 3],\n [2, 22, 37, 0, 34, 6, 0, 4, 3],\n [2, 13, 368, 272, 110, 6, 222, 4, 3],\n [2, 84, 8, 48, 250, 6, 167, 11, 3],\n [2, 32, 1407, 12, 0, 26, 208, 4, 3],\n [2, 5, 295, 7, 14, 252, 32, 4, 3],\n [2, 5, 7, 58, 90, 9, 16, 4, 3],\n [2, 22, 0, 461, 0, 6, 116, 4, 3],\n [2, 13, 298, 279, 156, 9, 0, 30, 4, 3],\n [2, 13, 203, 25, 16, 84, 612, 0, 4, 3],\n [2, 8, 0, 32, 1120, 1076, 23, 172, 11, 3],\n [2, 5, 231, 16, 34, 27, 572, 1115, 4, 3],\n [2, 31, 7, 19, 6, 0, 26, 119, 11, 3],\n [2, 56, 97, 116, 24, 5, 1269, 8, 11, 3],\n [2, 144, 64, 8, 65, 35, 312, 0, 11, 3],\n [2, 152, 83, 98, 695, 0, 26, 403, 4, 3],\n [2, 144, 7, 19, 6, 0, 165, 337, 11, 3],\n [2, 18, 365, 21, 366, 26, 1415, 445, 4, 3],\n [2, 5, 7, 44, 163, 1071, 62, 25, 4, 3],\n [2, 36, 53, 9, 1327, 67, 32, 166, 4, 3],\n [2, 13, 0, 67, 9, 45, 60, 0, 4, 3],\n [2, 18, 1420, 52, 1300, 9, 90, 21, 4, 3],\n [2, 24, 8, 358, 16, 15, 6, 783, 11, 3],\n [2, 13, 112, 67, 33, 399, 114, 0, 4, 3],\n [2, 5, 92, 0, 25, 8, 20, 0, 4, 3],\n [2, 28, 7, 14, 215, 47, 0, 8, 4, 3],\n [2, 5, 127, 47, 31, 41, 301, 37, 4, 3],\n [2, 18, 61, 7, 14, 39, 30, 0, 4, 3],\n [2, 441, 6, 0, 54, 65, 9, 233, 4, 3],\n [2, 358, 35, 557, 467, 16, 12, 417, 4, 3],\n [2, 5, 7, 44, 10, 316, 0, 0, 4, 3],\n [2, 51, 12, 126, 240, 15, 6, 0, 4, 3],\n [2, 0, 83, 17, 79, 0, 0, 287, 4, 3],\n [2, 5, 7, 58, 90, 8, 402, 1385, 4, 3],\n [2, 5, 7, 44, 138, 0, 17, 0, 4, 3],\n [2, 10, 0, 0, 0, 6, 1216, 1217, 4, 3],\n [2, 5, 72, 9, 55, 31, 8, 95, 4, 3],\n [2, 28, 7, 14, 0, 35, 572, 0, 4, 3],\n [2, 13, 365, 10, 0, 1096, 6, 1312, 4, 3],\n [2, 109, 80, 17, 30, 0, 37, 10, 0, 4, 3],\n [2, 32, 12, 6, 237, 1358, 25, 5, 55, 4, 3],\n [2, 75, 0, 20, 100, 114, 10, 464, 329, 4, 3],\n [2, 6, 0, 0, 0, 39, 10, 711, 0, 4, 3],\n [2, 5, 92, 319, 134, 33, 6, 402, 337, 4, 3],\n [2, 16, 37, 451, 91, 5, 0, 6, 0, 4, 3],\n [2, 18, 7, 19, 0, 0, 26, 10, 121, 4, 3],\n [2, 5, 0, 5, 61, 7, 14, 24, 105, 4, 3],\n [2, 5, 95, 8, 63, 142, 6, 0, 267, 4, 3],\n [2, 27, 291, 66, 21, 1231, 114, 10, 0, 4, 3],\n [2, 5, 28, 7, 14, 55, 144, 13, 247, 4, 3],\n [2, 5, 20, 76, 0, 0, 9, 24, 143, 4, 3],\n [2, 18, 227, 7, 14, 180, 51, 162, 99, 4, 3],\n [2, 18, 7, 19, 38, 114, 229, 114, 128, 4, 3],\n [2, 5, 0, 5, 7, 131, 279, 339, 22, 4, 3],\n [2, 5, 20, 567, 0, 99, 156, 8, 24, 4, 3],\n [2, 22, 227, 7, 14, 20, 162, 583, 40, 4, 3],\n [2, 31, 5, 53, 12, 78, 0, 54, 296, 4, 3],\n [2, 29, 8, 102, 9, 0, 162, 217, 0, 11, 3],\n [2, 32, 12, 6, 0, 1043, 5, 20, 582, 363, 4, 3],\n [2, 5, 0, 117, 5, 1266, 22, 7, 19, 310, 4, 3],\n [2, 5, 7, 131, 39, 9, 233, 33, 6, 0, 4, 3],\n [2, 13, 1350, 7, 14, 462, 41, 0, 162, 702, 4, 3],\n [2, 5, 53, 9, 46, 0, 8, 7, 155, 102, 4, 3],\n [2, 22, 7, 19, 1115, 118, 0, 9, 6, 0, 4, 3],\n [2, 282, 208, 472, 0, 29, 71, 0, 17, 0, 4, 3],\n [2, 6, 0, 0, 37, 60, 0, 73, 15, 0, 4, 3],\n [2, 5, 7, 44, 38, 0, 9, 319, 67, 615, 4, 3],\n [2, 808, 12, 6, 0, 0, 17, 6, 0, 0, 4, 3],\n [2, 13, 0, 1270, 23, 1271, 0, 26, 32, 121, 4, 3],\n [2, 5, 64, 38, 487, 16, 9, 45, 25, 316, 4, 3],\n [2, 6, 649, 0, 10, 390, 17, 0, 604, 47, 4, 3],\n [2, 31, 73, 64, 13, 149, 13, 7, 131, 106, 11, 3],\n [2, 22, 1420, 9, 90, 117, 128, 827, 162, 195, 4, 3],\n [2, 8, 0, 9, 21, 23, 61, 7, 14, 8, 11, 3],\n [2, 5, 7, 44, 111, 0, 123, 7, 19, 1402, 4, 3],\n [2, 117, 8, 594, 23, 8, 50, 636, 67, 34, 47, 4, 3],\n [2, 71, 17, 27, 224, 112, 127, 9, 0, 0, 21, 4, 3],\n [2, 36, 20, 9, 393, 134, 6, 236, 0, 402, 438, 4, 3],\n [2, 10, 994, 0, 84, 38, 0, 6, 292, 32, 283, 4, 3],\n [2, 13, 66, 0, 9, 416, 23, 175, 13, 61, 7, 14, 4, 3],\n [2, 150, 12, 6, 254, 0, 25, 42, 315, 446, 17, 309, 4, 3],\n [2, 13, 690, 6, 88, 23, 443, 9, 333, 40, 6, 413, 4, 3],\n [2, 5, 92, 15, 6, 0, 17, 619, 78, 0, 122, 646, 4, 3],\n [2, 99, 189, 156, 38, 23, 18, 66, 9, 46, 15, 327, 4, 3],\n [2, 5, 95, 16, 7, 19, 73, 26, 8, 9, 476, 67, 4, 3],\n [2, 76, 609, 156, 208, 0, 263, 37, 827, 26, 6, 233, 4, 3],\n [2, 117, 8, 196, 32, 0, 23, 8, 77, 0, 402, 496, 4, 3],\n [2, 22, 96, 21, 25, 13, 227, 7, 14, 39, 0, 0, 4, 3],\n [2, 8, 488, 7, 14, 304, 200, 9, 35, 606, 39, 25, 4, 3],\n [2, 100, 12, 10, 0, 329, 54, 12, 190, 71, 297, 6, 1241, 4, 3],\n [2, 0, 9, 581, 0, 23, 51, 37, 60, 0, 222, 86, 0, 4, 3],\n [2, 28, 7, 14, 617, 70, 32, 0, 4, 5, 295, 7, 14, 148, 16, 226, 4, 3],\n [2, 6, 79, 0, 266, 12, 60, 229, 1134, 411, 101, 141, 101, 38, 45, 994, 4, 3]]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#返回每个batch的id\n",
    "def get_minibatches(n, minibatch_size, shuffle = True):\n",
    "    idx_list = np.arange(0, n, minibatch_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49]),\n array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_minibatches(50, 10, shuffle = True)  #得到每一个batch对应的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#将句子对划分到batch\n",
    "def get_batches(en_encode, ch_encode):\n",
    "    batch_indexs = get_minibatches(len(en_encode), BATCH_SIZE)\n",
    "\n",
    "    batches = []\n",
    "    for batch_index in batch_indexs:\n",
    "        batch_en = [torch.tensor(en_encode[index]).long() for index in batch_index]  #每一个idx对应的句子，转为tensor格式\n",
    "        batch_zh = [torch.tensor(ch_encode[index]).long() for index in batch_index]\n",
    "        length_en = torch.tensor([len(en) for en in batch_en]).long()  #每一个句子的长度\n",
    "        length_zh = torch.tensor([len(zh) for zh in batch_zh]).long()\n",
    "\n",
    "        batch_en = pad_sequence(batch_en, padding_value = PAD_IDX, batch_first = True)  #讲一个batch中的句子padding为相同长度\n",
    "        batch_zh = pad_sequence(batch_zh, padding_value = PAD_IDX, batch_first = True)\n",
    "\n",
    "        batches.append((batch_en, batch_zh, length_en, length_zh))\n",
    "    return batches\n",
    "\n",
    "\n",
    "train_data = get_batches(train_en_encode, train_zh_encode)\n",
    "dev_data = get_batches(dev_en_encode, dev_zh_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 8,  8,  8, 10,  9,  7,  9, 10,  8,  5,  8,  8,  7, 10,  9,  8,  7,  9,\n        15,  9,  9,  9,  9,  8,  9, 11,  9,  8,  8,  6, 12,  8,  9,  8,  8,  9,\n         9,  8, 10, 10,  9,  8,  8, 11, 10, 10,  7,  9,  8, 10,  8,  9, 11,  8,\n         6,  8,  9,  7, 10, 11, 11,  8,  9,  7])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![](https://cdn.mathpix.com/snip/images/8Es5WLO-mn8kY7GO-T7o1IxWUBPbZeLrz3JbqY5U2Vw.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "其中 $\\bar{h}_{s}$ 表示encoder每个hidden_state的输出， $h_{t}$ 表示decoder每个hidden_state的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{t}(s) \n",
    "&=\\frac{\\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s}\\right)\\right)}{\\sum_{s^{\\prime}} \\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s^{\\prime}}\\right)\\right)} \\\\\n",
    "c_{t} &= \\sum a_{t} \\bar{h}_{s} \\\\\n",
    "\\tilde{h}_{t} &=\\tanh \\left(W_{c}\\left[c_{t} ; h_{t}\\right]\\right)\\\\\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$\\operatorname{score}\\left(\\boldsymbol{h}_{t}, \\overline{\\boldsymbol{h}}_{s}\\right)= \\begin{cases}\\boldsymbol{h}_{t}^{\\top} \\overline{\\boldsymbol{h}}_{s} & \\text { dot } \\\\ \\boldsymbol{h}_{t}^{\\top} \\boldsymbol{W}_{\\boldsymbol{a}} \\overline{\\boldsymbol{h}}_{s} & \\text { general } \\\\ \\boldsymbol{v}_{a}^{\\top} \\tanh \\left(\\boldsymbol{W}_{\\boldsymbol{a}}\\left[\\boldsymbol{h}_{t} ; \\overline{\\boldsymbol{h}}_{s}\\right]\\right) & \\text { concat }\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LuongEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongEncoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, bidirectional = True)  #双向GRU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        \"\"\"\n",
    "        input_seqs : batch_size,max(x_lengths)\n",
    "        input_lengths: batch_size\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))  #batch_size,max(x_lengths),embed_size\n",
    "        packed = pack_padded_sequence(embedded, x_lengths.long().cpu().data.numpy(), batch_first = True,\n",
    "                                      enforce_sorted = False)\n",
    "        #压缩填充张量,压缩掉无效的填充值\n",
    "        #enforce_sorted：如果是 True ，则输入应该是按长度降序排序的序列。如果是 False ，会在函数内部进行排序 \n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs, padding_value = PAD_IDX, batch_first = True)  #还原\n",
    "\n",
    "        #hidden (2, batch_size, enc_hidden_size)\n",
    "        #outputs (batch_size,seq_len, 2 * enc_hidden_size)\n",
    "\n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim = 1)\n",
    "        hidden = torch.tanh(self.fc(hidden)).unsqueeze(0)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        #general attention\n",
    "        self.linear_in = nn.Linear(enc_hidden_size * 2, dec_hidden_size, bias = False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size * 2 + dec_hidden_size, dec_hidden_size)\n",
    "\n",
    "    def forward(self, output, encoder_out, mask):\n",
    "        \"\"\"\n",
    "        output:batch_size, max(y_lengths), dec_hidden_size  #(h_t)\n",
    "        encoder_out:batch_size, max(x_lengths), 2 * enc_hidden_size  #(h_s)\n",
    "        \"\"\"\n",
    "        batch_size = output.shape[0]\n",
    "        output_len = output.shape[1]\n",
    "        input_len = encoder_out.shape[1]\n",
    "\n",
    "        encoder_out1 = self.linear_in(encoder_out.view(batch_size * input_len, -1)).view(batch_size, input_len, -1)\n",
    "        #Wh_s \n",
    "        #batch_size,max(x_lengths),dec_hidden_size\n",
    "        score = torch.bmm(output, encoder_out1.transpose(1, 2))  #实现三维数组的乘法，而不用拆成二维数组使用for循环解决\n",
    "        #[batch_size,max(y_lengths),dec_hidden_size] * [batch_size,dec_hidden_size,max(x_lengths)]\n",
    "        #batch_size,max(y_lengths),max(x_lengths)#score = h_t W h_s\n",
    "        score.data.masked_fill(mask, -1e16)\n",
    "        attn = F.softmax(score, dim = 2)  #attention系数矩阵\n",
    "\n",
    "        ct = torch.bmm(attn, encoder_out)  #ct = aths\n",
    "        #[batch_size,max(y_lengths),max(x_lengths)] * [batch_size, max(x_lengths), 2 * enc_hidden_size]\n",
    "        #batch_size, max(y_lengths), enc_hidden_size*2\n",
    "        output = torch.cat((ct, output), dim = 2)\n",
    "\n",
    "        output = output.view(batch_size * output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        #batch_size, max(y_lengths), dec_hidden_size\n",
    "\n",
    "        return output, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LuongDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attn(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, dec_hidden_size, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "\n",
    "    def creat_mask(self, x, y):\n",
    "        x_mask = x.data != PAD_IDX  #batch_size,max(x_lengths)\n",
    "        y_mask = y.data != PAD_IDX  #batch_size,max(y_lengths)\n",
    "        mask = (1 - (x_mask.unsqueeze(2) * y_mask.unsqueeze(1)).float()).bool()\n",
    "        #batch_size,max(x_lengths),max(y_lengths)\n",
    "        return mask\n",
    "\n",
    "    def forward(self, encoder_out, x, y, y_lengths, hid):\n",
    "        mask = self.creat_mask(y, x)\n",
    "        y = self.dropout(self.embedding(y))\n",
    "        packed = pack_padded_sequence(y, y_lengths.long().cpu().data.numpy(), batch_first = True,\n",
    "                                      enforce_sorted = False)\n",
    "        out, hid = self.rnn(packed, hid)\n",
    "\n",
    "        out, _ = pad_packed_sequence(out, padding_value = PAD_IDX, batch_first = True)\n",
    "\n",
    "        output, attn = self.attention(out, encoder_out, mask)\n",
    "        output = self.out(output)\n",
    "        #batch_size, max(y_lengths), vocab_size\n",
    "        return output, hid, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(encoder_out,  #这里输出的hid是decoder_rnn的hid\n",
    "                                         x = x,\n",
    "                                         y = y,\n",
    "                                         y_lengths = y_lengths,\n",
    "                                         hid = hid)  #encoder的hid\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length = 15):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for _ in range(max_length):\n",
    "            output, hid, attn = self.decoder(encoder_out,\n",
    "                                             x = x,\n",
    "                                             y = y,\n",
    "                                             y_lengths = torch.ones(batch_size).long().to(y.device),\n",
    "                                             hid = hid)\n",
    "\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define Model\n",
    "encoder = LuongEncoder(vocab_size = len(en_itow), embed_size = EMBED_SIZE, enc_hidden_size = ENC_HIDDEN_SIZE,\n",
    "                       dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "decoder = LuongDecoder(vocab_size = len(zh_itow), embed_size = EMBED_SIZE, enc_hidden_size = ENC_HIDDEN_SIZE,\n",
    "                       dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "model = seq2seq(encoder, decoder)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index = PAD_IDX)  #忽略padding位置的损失\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_data):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:, :-1]  #将前seq-1个单词作为输入\n",
    "        y_output = y[:, 1:]  #将后seq-1个单词作为输出，相当于前一个单词预测后一个单词\n",
    "        y_lengths = (y_lengths - 1).to(DEVICE)\n",
    "\n",
    "        logits, _ = model(x, x_lengths, y_input, y_lengths)  #batch_size, max(y_lengths), vocab_size\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)\n",
    "\n",
    "\n",
    "def evaluate(model, dev_data):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:, :-1]\n",
    "        y_output = y[:, 1:]\n",
    "        y_lengths = (y_lengths - 1).to(DEVICE)\n",
    "        logits, _ = model(x, x_lengths, y_input, y_lengths)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 7.124, Val loss: 6.198, Epoch time = 1.174s\n",
      "Epoch: 2, Train loss: 5.706, Val loss: 5.289, Epoch time = 1.160s\n",
      "Epoch: 3, Train loss: 5.157, Val loss: 5.006, Epoch time = 1.130s\n",
      "Epoch: 4, Train loss: 4.969, Val loss: 4.882, Epoch time = 1.135s\n",
      "Epoch: 5, Train loss: 4.865, Val loss: 4.786, Epoch time = 1.124s\n",
      "Epoch: 6, Train loss: 4.779, Val loss: 4.710, Epoch time = 1.121s\n",
      "Epoch: 7, Train loss: 4.697, Val loss: 4.653, Epoch time = 1.132s\n",
      "Epoch: 8, Train loss: 4.619, Val loss: 4.564, Epoch time = 1.122s\n",
      "Epoch: 9, Train loss: 4.537, Val loss: 4.466, Epoch time = 1.138s\n",
      "Epoch: 10, Train loss: 4.440, Val loss: 4.361, Epoch time = 1.118s\n",
      "Epoch: 11, Train loss: 4.344, Val loss: 4.282, Epoch time = 1.152s\n",
      "Epoch: 12, Train loss: 4.262, Val loss: 4.194, Epoch time = 1.118s\n",
      "Epoch: 13, Train loss: 4.182, Val loss: 4.114, Epoch time = 1.106s\n",
      "Epoch: 14, Train loss: 4.107, Val loss: 4.031, Epoch time = 1.115s\n",
      "Epoch: 15, Train loss: 4.028, Val loss: 3.969, Epoch time = 1.119s\n",
      "Epoch: 16, Train loss: 3.956, Val loss: 3.929, Epoch time = 1.125s\n",
      "Epoch: 17, Train loss: 3.881, Val loss: 3.815, Epoch time = 1.095s\n",
      "Epoch: 18, Train loss: 3.799, Val loss: 3.730, Epoch time = 1.111s\n",
      "Epoch: 19, Train loss: 3.731, Val loss: 3.675, Epoch time = 1.125s\n",
      "Epoch: 20, Train loss: 3.653, Val loss: 3.620, Epoch time = 1.115s\n",
      "Epoch: 21, Train loss: 3.589, Val loss: 3.523, Epoch time = 1.102s\n",
      "Epoch: 22, Train loss: 3.526, Val loss: 3.452, Epoch time = 1.103s\n",
      "Epoch: 23, Train loss: 3.445, Val loss: 3.399, Epoch time = 1.093s\n",
      "Epoch: 24, Train loss: 3.377, Val loss: 3.323, Epoch time = 1.117s\n",
      "Epoch: 25, Train loss: 3.307, Val loss: 3.236, Epoch time = 1.102s\n",
      "Epoch: 26, Train loss: 3.241, Val loss: 3.182, Epoch time = 1.104s\n",
      "Epoch: 27, Train loss: 3.167, Val loss: 3.112, Epoch time = 1.093s\n",
      "Epoch: 28, Train loss: 3.104, Val loss: 3.041, Epoch time = 1.090s\n",
      "Epoch: 29, Train loss: 3.039, Val loss: 2.980, Epoch time = 1.110s\n",
      "Epoch: 30, Train loss: 2.970, Val loss: 2.911, Epoch time = 1.108s\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer, train_data)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model, dev_data)\n",
    "    print((\n",
    "        f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文原句： BOS it ' s not UNK . EOS\n",
      "标准中文翻译： BOS 这 不 UNK 。 EOS\n",
      "模型翻译结果： 我们 在 這裡 的 朋友 。\n",
      "\n",
      "英文原句： BOS we have a great team . EOS\n",
      "标准中文翻译： BOS 我们 有个 UNK 的 UNK 。 EOS\n",
      "模型翻译结果： 我們 我們 的 时候 。\n",
      "\n",
      "英文原句： BOS this is a strange sentence . EOS\n",
      "标准中文翻译： BOS 這是 一個 奇怪 的 句子 。 EOS\n",
      "模型翻译结果： 这 是 个 。\n",
      "\n",
      "英文原句： BOS do you study every day ? EOS\n",
      "标准中文翻译： BOS 你 每天 都 学习 吗 ？ EOS\n",
      "模型翻译结果： 你 要 多少 ？\n",
      "\n",
      "英文原句： BOS that ' s the point . EOS\n",
      "标准中文翻译： BOS 这 UNK 问题 的 UNK 。 EOS\n",
      "模型翻译结果： 他 是 个 个 。\n",
      "\n",
      "英文原句： BOS a UNK is a UNK . EOS\n",
      "标准中文翻译： BOS UNK 是 UNK UNK 。 EOS\n",
      "模型翻译结果： 这 是 个 的 故事 。\n",
      "\n",
      "英文原句： BOS tom used to work here . EOS\n",
      "标准中文翻译： BOS 汤姆 UNK 在 这里 工作 。 EOS\n",
      "模型翻译结果： 汤姆 在 這裡 了 。\n",
      "\n",
      "英文原句： BOS you must clear the table . EOS\n",
      "标准中文翻译： BOS 你 必须 把 桌子 UNK UNK 。 EOS\n",
      "模型翻译结果： 你 是 个 。\n",
      "\n",
      "英文原句： BOS i ' m not sure . EOS\n",
      "标准中文翻译： BOS 我 不 确定 。 EOS\n",
      "模型翻译结果： 我 不 明白 的 。\n",
      "\n",
      "英文原句： BOS i ' ve UNK better . EOS\n",
      "标准中文翻译： BOS 我 已 UNK 得 UNK 了 。 EOS\n",
      "模型翻译结果： 我 不 想 ， 我 不 在 這裡 做 。\n",
      "\n",
      "英文原句： BOS the job is half done . EOS\n",
      "标准中文翻译： BOS 這項 工作 已經 完成 了 一半 。 EOS\n",
      "模型翻译结果： 這個 於 。\n",
      "\n",
      "英文原句： BOS UNK is UNK in china . EOS\n",
      "标准中文翻译： BOS UNK 在 中國 是 被 UNK 的 。 EOS\n",
      "模型翻译结果： 我们 在 這裡 上 了 。\n",
      "\n",
      "英文原句： BOS UNK you ' re me . EOS\n",
      "标准中文翻译： BOS UNK 你 是 我 。 EOS\n",
      "模型翻译结果： 你 我 的 。\n",
      "\n",
      "英文原句： BOS isn ' t that UNK ? EOS\n",
      "标准中文翻译： BOS 那 是 我 的 吗 ？ EOS\n",
      "模型翻译结果： 我们 在 哪里 ？\n",
      "\n",
      "英文原句： BOS he wrote to me yesterday . EOS\n",
      "标准中文翻译： BOS 昨天 他 寫 信給 我 。 EOS\n",
      "模型翻译结果： 他 是 我 的 機會 。\n",
      "\n",
      "英文原句： BOS she ' s in the UNK . EOS\n",
      "标准中文翻译： BOS 她 在 UNK 。 EOS\n",
      "模型翻译结果： 她 在 上 了 。\n",
      "\n",
      "英文原句： BOS nobody is too old to learn . EOS\n",
      "标准中文翻译： BOS 沒 有人 會 UNK 太 老 而 不能 學習 。 EOS\n",
      "模型翻译结果： 这 於 於 。\n",
      "\n",
      "英文原句： BOS i told tom i was UNK . EOS\n",
      "标准中文翻译： BOS 我 对 汤姆 说 我 很 好 。 EOS\n",
      "模型翻译结果： 我 想 我 了 。\n",
      "\n",
      "英文原句： BOS were you in america last month ? EOS\n",
      "标准中文翻译： BOS 你 上 個 月 在 UNK 嗎 ? EOS\n",
      "模型翻译结果： 你 在 哪里 ？\n",
      "\n",
      "英文原句： BOS let ' s UNK our UNK . EOS\n",
      "标准中文翻译： BOS 讓 我們 UNK 一下 UNK 吧 。 EOS\n",
      "模型翻译结果： 我們 的 房子 是 个 的 。\n",
      "\n",
      "英文原句： BOS we enjoyed UNK at the party . EOS\n",
      "标准中文翻译： BOS 我們 在 派 對 上 玩 得 很 開心 。 EOS\n",
      "模型翻译结果： 我們 我們 的 时候 隻 上 上 上 上 上 上 了 。\n",
      "\n",
      "英文原句： BOS he ' s my new friend . EOS\n",
      "标准中文翻译： BOS 他 是 我 UNK 的 朋友 。 EOS\n",
      "模型翻译结果： 他 的 是 我 的 。\n",
      "\n",
      "英文原句： BOS the building is seven UNK high . EOS\n",
      "标准中文翻译： BOS 這棟 UNK UNK UNK 高 。 EOS\n",
      "模型翻译结果： 這個 於 是 个 。\n",
      "\n",
      "英文原句： BOS how about going out for lunch ? EOS\n",
      "标准中文翻译： BOS 出去 吃 UNK UNK ？ EOS\n",
      "模型翻译结果： 我们 在 哪里 ？\n",
      "\n",
      "英文原句： BOS i ' m UNK my prices . EOS\n",
      "标准中文翻译： BOS 我 正在 把 UNK UNK UNK 。 EOS\n",
      "模型翻译结果： 我 不 的 時候 我 的 时候 。\n",
      "\n",
      "英文原句： BOS has she finished the book yet ? EOS\n",
      "标准中文翻译： BOS 她 讀 完 UNK 了 嗎 ？ EOS\n",
      "模型翻译结果： 她 在 哪里 ？\n",
      "\n",
      "英文原句： BOS i want him to read this . EOS\n",
      "标准中文翻译： BOS 想 让 他 读 这个 。 EOS\n",
      "模型翻译结果： 我 想 他 的 朋友 。\n",
      "\n",
      "英文原句： BOS tom knew how to do that . EOS\n",
      "标准中文翻译： BOS 汤姆 知道 怎么 做 那事 。 EOS\n",
      "模型翻译结果： 汤姆 在 這裡 上 上 上 上 上 上 的 时候 。\n",
      "\n",
      "英文原句： BOS please drive the car more slowly . EOS\n",
      "标准中文翻译： BOS 請 開車 UNK 一點 。 EOS\n",
      "模型翻译结果： 請 不要 人 。\n",
      "\n",
      "英文原句： BOS please UNK the dog every day . EOS\n",
      "标准中文翻译： BOS 请 每天 UNK 一下 狗 。 EOS\n",
      "模型翻译结果： 請 在 玩 上 了 。\n",
      "\n",
      "英文原句： BOS what ' s wrong , UNK ? EOS\n",
      "标准中文翻译： BOS 出 什么 事 了 ， UNK ？ EOS\n",
      "模型翻译结果： 你 在 這裡 做 ？\n",
      "\n",
      "英文原句： BOS don ' t change your mind . EOS\n",
      "标准中文翻译： BOS 不要 UNK 你 的 UNK 。 EOS\n",
      "模型翻译结果： 他 是 你 的 时候 。\n",
      "\n",
      "英文原句： BOS he lived abroad for many years . EOS\n",
      "标准中文翻译： BOS 他 UNK 在 UNK 多年 。 EOS\n",
      "模型翻译结果： 他 是 个 的 機會 。\n",
      "\n",
      "英文原句： BOS another bottle of wine , please . EOS\n",
      "标准中文翻译： BOS 麻煩 再 一瓶 葡萄酒 。 EOS\n",
      "模型翻译结果： 這些 巴士 取消 遠足 。\n",
      "\n",
      "英文原句： BOS i have finished UNK my room . EOS\n",
      "标准中文翻译： BOS 我 已經 打 UNK 完 我 的 房間 了 。 EOS\n",
      "模型翻译结果： 我 想 我 的 意思 。\n",
      "\n",
      "英文原句： BOS feel free to ask any questions . EOS\n",
      "标准中文翻译： BOS UNK 任何 問題 都 可以 。 EOS\n",
      "模型翻译结果： 請 在 玩 上 上 上 上 上 上 了 。\n",
      "\n",
      "英文原句： BOS you ' ll never be alone . EOS\n",
      "标准中文翻译： BOS 你们 永远 不会 一个 人 的 。 EOS\n",
      "模型翻译结果： 你 是 个 。\n",
      "\n",
      "英文原句： BOS he belongs to the camera club . EOS\n",
      "标准中文翻译： BOS 他 參加 UNK 。 EOS\n",
      "模型翻译结果： 他 从 隻 隻 隻 隻 隻 隻 隻 隻 隻 隻 隻 隻 隻\n",
      "\n",
      "英文原句： BOS he works from UNK to UNK . EOS\n",
      "标准中文翻译： BOS 他 從 早上 UNK 工作 到 UNK UNK UNK 。 EOS\n",
      "模型翻译结果： 他 从 正 那裡 。\n",
      "\n",
      "英文原句： BOS when did you begin studying english ? EOS\n",
      "标准中文翻译： BOS 您 什么 时候 开始 学 英语 的 ？ EOS\n",
      "模型翻译结果： 你 在 哪里 ？\n",
      "\n",
      "英文原句： BOS that UNK UNK very UNK UNK . EOS\n",
      "标准中文翻译： BOS 那个 UNK UNK 了 非常 UNK 的 UNK 。 EOS\n",
      "模型翻译结果： 那 是 個 的 故事 。\n",
      "\n",
      "英文原句： BOS could you do me a UNK ? EOS\n",
      "标准中文翻译： BOS 請 你 幫 我 一個 忙 好 嗎 ？ EOS\n",
      "模型翻译结果： 你 可以 我 的 意思 ？\n",
      "\n",
      "英文原句： BOS school UNK in UNK in UNK . EOS\n",
      "标准中文翻译： BOS UNK 的 學校 在 UNK UNK 。 EOS\n",
      "模型翻译结果： 我们 在 這裡 上 的 时候 。\n",
      "\n",
      "英文原句： BOS my UNK is still a baby . EOS\n",
      "标准中文翻译： BOS 我 的 UNK 還是 個 嬰兒 。 EOS\n",
      "模型翻译结果： 我 的 生日 了 。\n",
      "\n",
      "英文原句： BOS he had his UNK cut UNK . EOS\n",
      "标准中文翻译： BOS 他 把 UNK UNK 了 。 EOS\n",
      "模型翻译结果： 他 是 个 的 朋友 。\n",
      "\n",
      "英文原句： BOS he UNK me like his UNK . EOS\n",
      "标准中文翻译： BOS 他 對 待 我 就 像 他 的 UNK 。 EOS\n",
      "模型翻译结果： 他 我 的 时候 。\n",
      "\n",
      "英文原句： BOS you may UNK any of them . EOS\n",
      "标准中文翻译： BOS 你 可以 UNK 他們 UNK 的 任何 一個 。 EOS\n",
      "模型翻译结果： 你 是 個 的 朋友 。\n",
      "\n",
      "英文原句： BOS may i put it down here ? EOS\n",
      "标准中文翻译： BOS 我 可以 把 它 UNK 这儿 吗 ？ EOS\n",
      "模型翻译结果： 我 想 在 這裡 ？\n",
      "\n",
      "英文原句： BOS my father died four years ago . EOS\n",
      "标准中文翻译： BOS 我 的 父親 UNK 前 去世 了 。 EOS\n",
      "模型翻译结果： 我 的 腳踏車 上 上 了 。\n",
      "\n",
      "英文原句： BOS i don ' t like UNK . EOS\n",
      "标准中文翻译： BOS 我 不 喜欢 小孩 。 EOS\n",
      "模型翻译结果： 我 不 明白 的 时候 。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def translate_dev(i):\n",
    "    model.eval()\n",
    "\n",
    "    en_sent = \" \".join([en_itow[word] for word in test_en_encode[i]])\n",
    "    print('英文原句：', en_sent)\n",
    "    print('标准中文翻译：', \" \".join([zh_itow[word] for word in test_zh_encode[i]]))\n",
    "\n",
    "    bos = torch.Tensor([[zh_wtoi[\"BOS\"]]]).long().to(DEVICE)\n",
    "    x = torch.Tensor(test_en_encode[i]).long().to(DEVICE).reshape(1, -1)\n",
    "    x_len = torch.Tensor([len(test_en_encode[i])]).long().to(DEVICE)\n",
    "\n",
    "    translation, _ = model.translate(x, x_len, bos)\n",
    "    translation = [zh_itow[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print('模型翻译结果：', \" \".join(trans))\n",
    "\n",
    "\n",
    "for i in range(50, 100):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 练习一：Bi-LSTM + attention 用于情感分类\n",
    "\n",
    "补全代码：我们使用构建一个Bi-LSTM + attention模型完成文本分类任务，数据使用IMDb电影评论数据集，检测一段文字的情感是正面还是负面。\n",
    "\n",
    "[论文](https://aclanthology.org/P16-2034.pdf)\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split = 'train')\n",
    "\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials = [\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.insert_token(\"<pad>\", 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: 0 if x == 'neg' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):  #自定义的batch输出\n",
    "    label_list, text_list, lengths = [], [], []\n",
    "    batch.sort(key = lambda x: len(text_pipeline(x[1])), reverse = True)  #按照长度的大小进行排序\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text))\n",
    "    text_list = pad_sequence(text_list, padding_value = vocab.get_stoi()[\"<pad>\"],\n",
    "                             batch_first = True)  #进行填充，每个batch中的句子需要有相同的长度\n",
    "    return torch.tensor(label_list), text_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_iter, test_iter = IMDB(root = 'data', split = ('train', 'test'))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size = 128,\n",
    "                              shuffle = True, collate_fn = collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size = 128,\n",
    "                              shuffle = True, collate_fn = collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size = 128,\n",
    "                             shuffle = True, collate_fn = collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n        1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n        0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n        1, 0, 1, 1, 1, 1, 0, 0])"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]  #标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  458,  3141,  1552,  ...,    17,    57,     3],\n        [   20, 14053,     4,  ...,     1,     1,     1],\n        [    6,  8268,    12,  ...,     1,     1,     1],\n        ...,\n        [ 1070,     7,     6,  ...,     1,     1,     1],\n        [    2, 18679,   121,  ...,     1,     1,     1],\n        [  905,    14,    21,  ...,     1,     1,     1]])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]  #batch_size,max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[1185,\n 1162,\n 1144,\n 862,\n 842,\n 773,\n 762,\n 724,\n 662,\n 645,\n 631,\n 619,\n 598,\n 552,\n 535,\n 496,\n 491,\n 478,\n 477,\n 460,\n 435,\n 433,\n 429,\n 402,\n 398,\n 398,\n 398,\n 378,\n 367,\n 360,\n 349,\n 338,\n 336,\n 330,\n 329,\n 327,\n 319,\n 317,\n 313,\n 310,\n 306,\n 305,\n 301,\n 298,\n 284,\n 283,\n 276,\n 275,\n 272,\n 267,\n 258,\n 252,\n 247,\n 245,\n 242,\n 238,\n 235,\n 228,\n 227,\n 222,\n 219,\n 216,\n 207,\n 201,\n 198,\n 198,\n 196,\n 195,\n 192,\n 189,\n 187,\n 187,\n 184,\n 182,\n 179,\n 179,\n 176,\n 175,\n 174,\n 162,\n 161,\n 160,\n 158,\n 157,\n 157,\n 155,\n 155,\n 155,\n 155,\n 153,\n 150,\n 149,\n 148,\n 147,\n 146,\n 144,\n 143,\n 142,\n 141,\n 137,\n 135,\n 133,\n 133,\n 131,\n 131,\n 128,\n 126,\n 125,\n 124,\n 123,\n 123,\n 122,\n 121,\n 117,\n 117,\n 111,\n 105,\n 97,\n 97,\n 85,\n 84,\n 76,\n 65,\n 61,\n 58,\n 54,\n 37,\n 24]"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]  #lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "模型分为五个部分\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)\n",
    "\n",
    "- 输入层（Input layer）：将句子输入模型\n",
    "- 嵌入层（Embedding layer）：将每个词映射到一个低维向量\n",
    "- LSTM层（LSTM layer）：利用BiLSTM从词向量中获得特征\n",
    "- Attention层（Attention layer）：生成权重向量，将每个时间步长的单词级特征与权重向量相乘，合并成句子级特征向量（补全代码）\n",
    "- 输出层（Output layer）： 对句子进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class bilstm_attn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 dropout_rate, pad_id):\n",
    "        super(bilstm_attn, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_id)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = True,\n",
    "                            dropout = dropout_rate, batch_first = True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.attn = MultiheadAttention(hidden_dim, 8, batch_first = True)\n",
    "        self.Q = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.K = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.V = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.dropout(self.embedding(x))  # [batch size,seq len] -> [batch size,seq len,embedding_dim]\n",
    "\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths, batch_first = True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden = [n layers *2, batch size, hidden dim]最后一个step的hidden\n",
    "        # cell = [n layers * 2, batch size, hidden dim]最终一个step的cell\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first = True)\n",
    "        # output = [batch size, seq len, hidden dim * 2]#每一个step下的最后一层的output\n",
    "        output = output.reshape(output.shape[0], output.shape[1], 2, -1)\n",
    "        # output = [batch size, seq len, 2,hidden dim]\n",
    "        output = torch.sum(output, dim = 2)\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        output = self.dropout(output)\n",
    "        q_vector = self.Q(output)\n",
    "        k_vector = self.K(output)\n",
    "        v_vector = self.V(output)\n",
    "        attn_output, attn_output_weights = self.attn(q_vector, k_vector, v_vector)\n",
    "        prediction = self.output(attn_output.sum(dim = 1))\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5\n",
    "pad_id = vocab.get_stoi()[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nieyuzhou\\miniconda3\\envs\\dl\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = bilstm_attn(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout_rate, pad_id).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "    model.train()  #model.train()代表了训练模式\n",
    "    for label, text, lengths in tqdm(train_loader):\n",
    "        label = label.to(device)\n",
    "        text = text.to(device)\n",
    "\n",
    "        out = model(text, lengths)\n",
    "        loss = loss_fn(out, label)\n",
    "\n",
    "        _, pred = torch.max(out.data, 1)\n",
    "        corrects += (pred == label).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()  #加这步防止梯度叠加\n",
    "        loss.backward()  #反向传播\n",
    "        optimizer.step()  #梯度下降\n",
    "\n",
    "        epoch_loss += loss.item() * len(label)\n",
    "        #loss.item()已经本身除以了len(batch.label)\n",
    "        #所以得再乘一次，得到一个batch的损失，累加得到所有样本损失。\n",
    "\n",
    "        total_len += len(label)\n",
    "        #计算train_iterator所有样本的数量，不出意外应该是17500\n",
    "\n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "\n",
    "    model.eval()\n",
    "    #转换成测试模式，冻结dropout层或其他层。\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text, lengths in tqdm(valid_loader):\n",
    "            #iterator为valid_iterator\n",
    "            label = label.to(device)\n",
    "            text = text.to(device)\n",
    "\n",
    "            out = model(text, lengths)\n",
    "            loss = loss_fn(out, label)\n",
    "\n",
    "            _, pred = torch.max(out.data, 1)\n",
    "            corrects += (pred == label).sum().item()\n",
    "\n",
    "            epoch_loss += loss.item() * len(label)\n",
    "            total_len += len(label)\n",
    "    model.train()  #调回训练模式\n",
    "\n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/186 [02:08<6:36:36, 128.63s/it]"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, loss_fn)\n",
    "    print(\"epoch:\", epoch, \"train_loss:\", train_loss, \"train_acc\", train_acc)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader)\n",
    "    print(\"epoch:\", epoch, \"valid_loss:\", valid_loss, \"valid_acc\", valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'attn_data/bilstm_attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    text = text_pipeline(text)\n",
    "\n",
    "    length = torch.LongTensor([len(text)])\n",
    "    tensor = torch.LongTensor(text).unsqueeze(0).to(device)\n",
    "\n",
    "    out = model(tensor, length)\n",
    "    _, pred = torch.max(out.data, 1)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(\"This film is great\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}