{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#  seq2seq + Luong 注意力 进行中英文翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T07:51:42.718150Z",
     "start_time": "2022-05-15T07:51:42.330746Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "from collections import Counter#计数器\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence ,pack_padded_sequence,pad_packed_sequence\n",
    "from torchtext.data.utils import get_tokenizer#分词器\n",
    "#from autonotebook import tqdm as notebook_tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "UNK_IDX = 0 #未知\n",
    "PAD_IDX = 1  #\n",
    "BATCH_SIZE = 64   \n",
    "EPOCHS  = 30\n",
    "DROPOUT = 0.2\n",
    "ENC_HIDDEN_SIZE = DEC_HIDDEN_SIZE = 100\n",
    "EMBED_SIZE = 100\n",
    "## 数据集文件\n",
    "train_en_file = 'en-zh/train.en'\n",
    "train_cn_file = 'en-zh/train.zh'\n",
    "test_en_file = 'en-zh/test.en'\n",
    "test_cn_file = 'en-zh/test.zh'\n",
    "save_file = 'model.pt'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T08:07:29.961999Z",
     "start_time": "2022-05-15T08:07:29.946390Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#分词器\n",
    "tokenizer_en = get_tokenizer('basic_english')#按空格进行分割\n",
    "tokenizer_cn = get_tokenizer(jieba.lcut)#进行结巴分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-15T08:19:00.252379Z",
     "start_time": "2022-05-15T08:19:00.242402Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#加载文件\n",
    "\n",
    "\n",
    "def load_data(path,language):\n",
    "    text = []\n",
    "    # 语言为中文\n",
    "    if language == 'cn':\n",
    "        i = 0\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                i = i+1\n",
    "                \n",
    "                if i%1000000 == 0:\n",
    "                    print(str(i))\n",
    "                    \n",
    "                try:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    text.append([\"BOS\"] + tokenizer_cn(line[0]) + [\"EOS\"])\n",
    "                    \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"error raise!\")\n",
    "                    \n",
    "        \n",
    "        return text\n",
    "    \n",
    "    # 语言为英文\n",
    "    elif language == 'en':\n",
    "        i = 0\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                i = i+1\n",
    "                \n",
    "                \n",
    "                try:\n",
    "                    line = line.strip().split('\\t')\n",
    "                    text.append([\"BOS\"] + tokenizer_en(line[0].lower()) + [\"EOS\"])#小写\n",
    "                    \n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"error raise!\")\n",
    "                if i%1000000 == 0:\n",
    "                    print(str(i))\n",
    "                    \n",
    "                    \n",
    "       \n",
    "        return text\n",
    "    \n",
    "    # 语言非中文和非英文\n",
    "    else:\n",
    "        print(\"Can't handle the language!\")\n",
    "        return \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n"
     ]
    }
   ],
   "source": [
    "train_en = load_data(train_en_file, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(train_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-05-15T08:20:46.371Z"
    },
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.690 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "2000000\n",
      "3000000\n",
      "4000000\n",
      "5000000\n",
      "6000000\n",
      "7000000\n",
      "8000000\n",
      "9000000\n",
      "10000000\n",
      "11000000\n",
      "12000000\n",
      "13000000\n",
      "14000000\n",
      "15000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_zh = load_data(train_cn_file, 'cn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_zh.pickle', 'wb') as file:\n",
    "    pickle.dump(train_zh, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('train_en.pickle', 'wb') as file:\n",
    "    pickle.dump(train_en, file)\n",
    "#with open('train_zh.pickle', 'wb') as file:\n",
    " #   pickle.dump(train_zh, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 8s, sys: 30.8 s, total: 2min 39s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pickle\n",
    "with open('train_en.pickle', 'rb') as file:\n",
    "    train_en = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['BOS', 'resolution', '918', '(', '1994', ')', 'EOS'], 15886041)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_en[0], len(train_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('train_zh.pickle', 'rb') as file:\n",
    "    train_zh = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_en = load_data(test_en_file, 'en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOS',\n",
       " '7439th',\n",
       " 'meeting',\n",
       " ',',\n",
       " 'held',\n",
       " 'on',\n",
       " '11',\n",
       " 'may',\n",
       " '2015',\n",
       " '.',\n",
       " 'EOS']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae8c71cc36a149d7a9b7dd9ebbbab446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7966fb614c84450c85a3337d6af6a52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e910d66db06a411abcbb769fcc28cc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7ae19b0a2b486dbf6ed318465e851e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[100, 100, 2309, 117, 1316, 1113, 1429, 1336, 1410, 119, 100]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "ids = tokenizer.convert_tokens_to_ids(test_en[0])\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "text = []\n",
    "id = []\n",
    "with open(test_en_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        i = i+1\n",
    "\n",
    "        if i%1000000 == 0:\n",
    "            print(str(i))\n",
    "\n",
    "        try:\n",
    "            line = line.strip().split('\\t')\n",
    "            tokens = tokenizer.tokenize(line[0])\n",
    "            text.append(tokens)\n",
    "            id.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"error raise!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(test_en_file, 'r', encoding='utf-8') as f:\n",
    "    tmp = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"7439th meeting, held on 11 May 2015.\\nISIL itself has published videos depicting people being subjected to a range of abhorrent punishments, including stoning, being pushed-off buildings, decapitation and crucifixion.\\nUNICEF disbursed emergency cash assistance to tens of thousands of displaced families in camps and UNHCR distributed cash assistance to vulnerable families which had been internally displaced.\\n31. Recognizes the important contribution of the African Peer Review Mechanism since its inception in improving governance and supporting socioeconomic development in African countries, and recalls in this regard the high-level panel discussion held on 21 October 2013 on Africa's innovation in governance through 10 years of the African Peer Review Mechanism, organized during the sixty-eighth session of the General Assembly to commemorate the tenth anniversary of the Mechanism;\\nSpreads between sovereign bonds in Germany and those in other countries were relatively unaffected by politi\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['74',\n",
       "  '##39',\n",
       "  '##th',\n",
       "  'meeting',\n",
       "  ',',\n",
       "  'held',\n",
       "  'on',\n",
       "  '11',\n",
       "  'May',\n",
       "  '2015',\n",
       "  '.'],\n",
       " ['ISIL',\n",
       "  'itself',\n",
       "  'has',\n",
       "  'published',\n",
       "  'videos',\n",
       "  'depicting',\n",
       "  'people',\n",
       "  'being',\n",
       "  'subjected',\n",
       "  'to',\n",
       "  'a',\n",
       "  'range',\n",
       "  'of',\n",
       "  'a',\n",
       "  '##b',\n",
       "  '##hor',\n",
       "  '##rent',\n",
       "  'punishment',\n",
       "  '##s',\n",
       "  ',',\n",
       "  'including',\n",
       "  's',\n",
       "  '##ton',\n",
       "  '##ing',\n",
       "  ',',\n",
       "  'being',\n",
       "  'pushed',\n",
       "  '-',\n",
       "  'off',\n",
       "  'buildings',\n",
       "  ',',\n",
       "  'de',\n",
       "  '##cap',\n",
       "  '##itation',\n",
       "  'and',\n",
       "  'c',\n",
       "  '##ru',\n",
       "  '##ci',\n",
       "  '##fi',\n",
       "  '##xi',\n",
       "  '##on',\n",
       "  '.'],\n",
       " ['UN',\n",
       "  '##IC',\n",
       "  '##EF',\n",
       "  'di',\n",
       "  '##s',\n",
       "  '##bur',\n",
       "  '##sed',\n",
       "  'emergency',\n",
       "  'cash',\n",
       "  'assistance',\n",
       "  'to',\n",
       "  'tens',\n",
       "  'of',\n",
       "  'thousands',\n",
       "  'of',\n",
       "  'displaced',\n",
       "  'families',\n",
       "  'in',\n",
       "  'camps',\n",
       "  'and',\n",
       "  'UN',\n",
       "  '##HC',\n",
       "  '##R',\n",
       "  'distributed',\n",
       "  'cash',\n",
       "  'assistance',\n",
       "  'to',\n",
       "  'vulnerable',\n",
       "  'families',\n",
       "  'which',\n",
       "  'had',\n",
       "  'been',\n",
       "  'internally',\n",
       "  'displaced',\n",
       "  '.'],\n",
       " ['31',\n",
       "  '.',\n",
       "  'Re',\n",
       "  '##co',\n",
       "  '##gni',\n",
       "  '##zes',\n",
       "  'the',\n",
       "  'important',\n",
       "  'contribution',\n",
       "  'of',\n",
       "  'the',\n",
       "  'African',\n",
       "  'P',\n",
       "  '##eer',\n",
       "  'Review',\n",
       "  'Me',\n",
       "  '##chan',\n",
       "  '##ism',\n",
       "  'since',\n",
       "  'its',\n",
       "  'inception',\n",
       "  'in',\n",
       "  'improving',\n",
       "  'governance',\n",
       "  'and',\n",
       "  'supporting',\n",
       "  'socio',\n",
       "  '##economic',\n",
       "  'development',\n",
       "  'in',\n",
       "  'African',\n",
       "  'countries',\n",
       "  ',',\n",
       "  'and',\n",
       "  'recalls',\n",
       "  'in',\n",
       "  'this',\n",
       "  'regard',\n",
       "  'the',\n",
       "  'high',\n",
       "  '-',\n",
       "  'level',\n",
       "  'panel',\n",
       "  'discussion',\n",
       "  'held',\n",
       "  'on',\n",
       "  '21',\n",
       "  'October',\n",
       "  '2013',\n",
       "  'on',\n",
       "  'Africa',\n",
       "  \"'\",\n",
       "  's',\n",
       "  'innovation',\n",
       "  'in',\n",
       "  'governance',\n",
       "  'through',\n",
       "  '10',\n",
       "  'years',\n",
       "  'of',\n",
       "  'the',\n",
       "  'African',\n",
       "  'P',\n",
       "  '##eer',\n",
       "  'Review',\n",
       "  'Me',\n",
       "  '##chan',\n",
       "  '##ism',\n",
       "  ',',\n",
       "  'organized',\n",
       "  'during',\n",
       "  'the',\n",
       "  'sixty',\n",
       "  '-',\n",
       "  'eighth',\n",
       "  'session',\n",
       "  'of',\n",
       "  'the',\n",
       "  'General',\n",
       "  'Assembly',\n",
       "  'to',\n",
       "  'commemorate',\n",
       "  'the',\n",
       "  'tenth',\n",
       "  'anniversary',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Me',\n",
       "  '##chan',\n",
       "  '##ism',\n",
       "  ';'],\n",
       " ['S',\n",
       "  '##p',\n",
       "  '##read',\n",
       "  '##s',\n",
       "  'between',\n",
       "  'sovereign',\n",
       "  'bonds',\n",
       "  'in',\n",
       "  'Germany',\n",
       "  'and',\n",
       "  'those',\n",
       "  'in',\n",
       "  'other',\n",
       "  'countries',\n",
       "  'were',\n",
       "  'relatively',\n",
       "  'un',\n",
       "  '##af',\n",
       "  '##fected',\n",
       "  'by',\n",
       "  'political',\n",
       "  'and',\n",
       "  'market',\n",
       "  'uncertain',\n",
       "  '##ties',\n",
       "  'concerning',\n",
       "  'Greece',\n",
       "  'in',\n",
       "  'late',\n",
       "  '2014',\n",
       "  'and',\n",
       "  'early',\n",
       "  '2015',\n",
       "  '.'],\n",
       " ['The',\n",
       "  'removal',\n",
       "  'of',\n",
       "  'the',\n",
       "  'floor',\n",
       "  'with',\n",
       "  'respect',\n",
       "  'to',\n",
       "  'the',\n",
       "  'euro',\n",
       "  'was',\n",
       "  'accompanied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'further',\n",
       "  'move',\n",
       "  'into',\n",
       "  'negative',\n",
       "  'territory',\n",
       "  'of',\n",
       "  'the',\n",
       "  'interest',\n",
       "  'rate',\n",
       "  'on',\n",
       "  'sight',\n",
       "  'deposit',\n",
       "  'account',\n",
       "  'balance',\n",
       "  '##s',\n",
       "  'to',\n",
       "  '-',\n",
       "  '0',\n",
       "  '.',\n",
       "  '75',\n",
       "  'per',\n",
       "  'cent',\n",
       "  'in',\n",
       "  'order',\n",
       "  'to',\n",
       "  'reduce',\n",
       "  'app',\n",
       "  '##re',\n",
       "  '##ciating',\n",
       "  'pressures',\n",
       "  'and',\n",
       "  'the',\n",
       "  'resulting',\n",
       "  'tightening',\n",
       "  'of',\n",
       "  'monetary',\n",
       "  'conditions',\n",
       "  '.'],\n",
       " ['To',\n",
       "  'be',\n",
       "  'held',\n",
       "  'on',\n",
       "  'Thursday',\n",
       "  ',',\n",
       "  '2',\n",
       "  'April',\n",
       "  '2015',\n",
       "  ',',\n",
       "  'at',\n",
       "  '10',\n",
       "  '.',\n",
       "  '15',\n",
       "  'a',\n",
       "  '.',\n",
       "  'm',\n",
       "  '.'],\n",
       " ['Upon',\n",
       "  'instruction',\n",
       "  'from',\n",
       "  'my',\n",
       "  'Government',\n",
       "  ',',\n",
       "  'I',\n",
       "  'have',\n",
       "  'the',\n",
       "  'honour',\n",
       "  'to',\n",
       "  'attach',\n",
       "  'here',\n",
       "  '##with',\n",
       "  'a',\n",
       "  'list',\n",
       "  'containing',\n",
       "  'the',\n",
       "  'names',\n",
       "  'of',\n",
       "  '96',\n",
       "  'Syrian',\n",
       "  'civilians',\n",
       "  ',',\n",
       "  'including',\n",
       "  '41',\n",
       "  'children',\n",
       "  ',',\n",
       "  'killed',\n",
       "  'in',\n",
       "  'Aleppo',\n",
       "  'by',\n",
       "  'terrorist',\n",
       "  'groups',\n",
       "  'during',\n",
       "  'the',\n",
       "  'period',\n",
       "  'from',\n",
       "  '13',\n",
       "  'April',\n",
       "  '2015',\n",
       "  'to',\n",
       "  '7',\n",
       "  'May',\n",
       "  '2015',\n",
       "  '(',\n",
       "  'see',\n",
       "  'an',\n",
       "  '##nex',\n",
       "  ')',\n",
       "  '.'],\n",
       " ['12',\n",
       "  '.',\n",
       "  'Re',\n",
       "  '##iter',\n",
       "  '##ates',\n",
       "  'that',\n",
       "  'individuals',\n",
       "  'and',\n",
       "  'entities',\n",
       "  'determined',\n",
       "  'by',\n",
       "  'the',\n",
       "  'Committee',\n",
       "  'to',\n",
       "  'have',\n",
       "  'violated',\n",
       "  'the',\n",
       "  'provisions',\n",
       "  'of',\n",
       "  'resolution',\n",
       "  '1970',\n",
       "  '(',\n",
       "  '2011',\n",
       "  ')',\n",
       "  ',',\n",
       "  'including',\n",
       "  'the',\n",
       "  'arms',\n",
       "  'em',\n",
       "  '##bar',\n",
       "  '##go',\n",
       "  ',',\n",
       "  'or',\n",
       "  'assisted',\n",
       "  'others',\n",
       "  'in',\n",
       "  'doing',\n",
       "  'so',\n",
       "  ',',\n",
       "  'are',\n",
       "  'subject',\n",
       "  'to',\n",
       "  'designation',\n",
       "  ',',\n",
       "  'and',\n",
       "  'notes',\n",
       "  'that',\n",
       "  'this',\n",
       "  'includes',\n",
       "  'those',\n",
       "  'who',\n",
       "  'assist',\n",
       "  'in',\n",
       "  'the',\n",
       "  'violation',\n",
       "  'of',\n",
       "  'the',\n",
       "  'assets',\n",
       "  'freeze',\n",
       "  'and',\n",
       "  'travel',\n",
       "  'ban',\n",
       "  'in',\n",
       "  'resolution',\n",
       "  '1970',\n",
       "  '(',\n",
       "  '2011',\n",
       "  ')',\n",
       "  ';'],\n",
       " ['23',\n",
       "  '.',\n",
       "  'Support',\n",
       "  '##s',\n",
       "  'the',\n",
       "  'efforts',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Libyan',\n",
       "  'authorities',\n",
       "  'to',\n",
       "  'recover',\n",
       "  'funds',\n",
       "  'mi',\n",
       "  '##sa',\n",
       "  '##pp',\n",
       "  '##rop',\n",
       "  '##riated',\n",
       "  'under',\n",
       "  'the',\n",
       "  'Q',\n",
       "  '##ad',\n",
       "  '##ha',\n",
       "  '##fi',\n",
       "  'regime',\n",
       "  'and',\n",
       "  ',',\n",
       "  'in',\n",
       "  'this',\n",
       "  'regard',\n",
       "  ',',\n",
       "  'encourages',\n",
       "  'the',\n",
       "  'Libyan',\n",
       "  'authorities',\n",
       "  'and',\n",
       "  'Member',\n",
       "  'States',\n",
       "  'that',\n",
       "  'have',\n",
       "  'frozen',\n",
       "  'assets',\n",
       "  'pu',\n",
       "  '##rs',\n",
       "  '##uant',\n",
       "  'to',\n",
       "  'resolutions',\n",
       "  '1970',\n",
       "  '(',\n",
       "  '2011',\n",
       "  ')',\n",
       "  'and',\n",
       "  '1973',\n",
       "  '(',\n",
       "  '2011',\n",
       "  ')',\n",
       "  'as',\n",
       "  'modified',\n",
       "  'by',\n",
       "  'resolution',\n",
       "  '2009',\n",
       "  '(',\n",
       "  '2011',\n",
       "  ')',\n",
       "  'to',\n",
       "  'consult',\n",
       "  'with',\n",
       "  'each',\n",
       "  'other',\n",
       "  'regarding',\n",
       "  'claims',\n",
       "  'of',\n",
       "  'mi',\n",
       "  '##sa',\n",
       "  '##pp',\n",
       "  '##rop',\n",
       "  '##riated',\n",
       "  'funds',\n",
       "  'and',\n",
       "  'related',\n",
       "  'issues',\n",
       "  'of',\n",
       "  'ownership',\n",
       "  ';']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5692, 24786, 1582, 2309, 117, 1316, 1113, 1429, 1318, 1410, 119],\n",
       " [21388,\n",
       "  2111,\n",
       "  1144,\n",
       "  1502,\n",
       "  6581,\n",
       "  10726,\n",
       "  1234,\n",
       "  1217,\n",
       "  13927,\n",
       "  1106,\n",
       "  170,\n",
       "  2079,\n",
       "  1104,\n",
       "  170,\n",
       "  1830,\n",
       "  13252,\n",
       "  17759,\n",
       "  7703,\n",
       "  1116,\n",
       "  117,\n",
       "  1259,\n",
       "  188,\n",
       "  1633,\n",
       "  1158,\n",
       "  117,\n",
       "  1217,\n",
       "  2873,\n",
       "  118,\n",
       "  1228,\n",
       "  2275,\n",
       "  117,\n",
       "  1260,\n",
       "  25265,\n",
       "  12633,\n",
       "  1105,\n",
       "  172,\n",
       "  5082,\n",
       "  6617,\n",
       "  8702,\n",
       "  8745,\n",
       "  1320,\n",
       "  119],\n",
       " [7414,\n",
       "  9741,\n",
       "  14663,\n",
       "  4267,\n",
       "  1116,\n",
       "  19364,\n",
       "  5591,\n",
       "  5241,\n",
       "  5948,\n",
       "  5052,\n",
       "  1106,\n",
       "  17265,\n",
       "  1104,\n",
       "  4674,\n",
       "  1104,\n",
       "  13577,\n",
       "  2073,\n",
       "  1107,\n",
       "  7869,\n",
       "  1105,\n",
       "  7414,\n",
       "  15779,\n",
       "  2069,\n",
       "  4901,\n",
       "  5948,\n",
       "  5052,\n",
       "  1106,\n",
       "  8018,\n",
       "  2073,\n",
       "  1134,\n",
       "  1125,\n",
       "  1151,\n",
       "  19266,\n",
       "  13577,\n",
       "  119],\n",
       " [1955,\n",
       "  119,\n",
       "  11336,\n",
       "  2528,\n",
       "  22152,\n",
       "  11846,\n",
       "  1103,\n",
       "  1696,\n",
       "  6436,\n",
       "  1104,\n",
       "  1103,\n",
       "  2170,\n",
       "  153,\n",
       "  8284,\n",
       "  4960,\n",
       "  2508,\n",
       "  18546,\n",
       "  1863,\n",
       "  1290,\n",
       "  1157,\n",
       "  12548,\n",
       "  1107,\n",
       "  9248,\n",
       "  12711,\n",
       "  1105,\n",
       "  4374,\n",
       "  20650,\n",
       "  25726,\n",
       "  1718,\n",
       "  1107,\n",
       "  2170,\n",
       "  2182,\n",
       "  117,\n",
       "  1105,\n",
       "  19120,\n",
       "  1107,\n",
       "  1142,\n",
       "  7328,\n",
       "  1103,\n",
       "  1344,\n",
       "  118,\n",
       "  1634,\n",
       "  5962,\n",
       "  6145,\n",
       "  1316,\n",
       "  1113,\n",
       "  1626,\n",
       "  1357,\n",
       "  1381,\n",
       "  1113,\n",
       "  2201,\n",
       "  112,\n",
       "  188,\n",
       "  11279,\n",
       "  1107,\n",
       "  12711,\n",
       "  1194,\n",
       "  1275,\n",
       "  1201,\n",
       "  1104,\n",
       "  1103,\n",
       "  2170,\n",
       "  153,\n",
       "  8284,\n",
       "  4960,\n",
       "  2508,\n",
       "  18546,\n",
       "  1863,\n",
       "  117,\n",
       "  3366,\n",
       "  1219,\n",
       "  1103,\n",
       "  9229,\n",
       "  118,\n",
       "  6075,\n",
       "  4912,\n",
       "  1104,\n",
       "  1103,\n",
       "  1615,\n",
       "  2970,\n",
       "  1106,\n",
       "  13792,\n",
       "  1103,\n",
       "  8281,\n",
       "  5453,\n",
       "  1104,\n",
       "  1103,\n",
       "  2508,\n",
       "  18546,\n",
       "  1863,\n",
       "  132],\n",
       " [156,\n",
       "  1643,\n",
       "  11613,\n",
       "  1116,\n",
       "  1206,\n",
       "  14611,\n",
       "  10150,\n",
       "  1107,\n",
       "  1860,\n",
       "  1105,\n",
       "  1343,\n",
       "  1107,\n",
       "  1168,\n",
       "  2182,\n",
       "  1127,\n",
       "  3860,\n",
       "  8362,\n",
       "  9823,\n",
       "  21601,\n",
       "  1118,\n",
       "  1741,\n",
       "  1105,\n",
       "  2319,\n",
       "  9591,\n",
       "  4338,\n",
       "  6995,\n",
       "  4747,\n",
       "  1107,\n",
       "  1523,\n",
       "  1387,\n",
       "  1105,\n",
       "  1346,\n",
       "  1410,\n",
       "  119],\n",
       " [1109,\n",
       "  8116,\n",
       "  1104,\n",
       "  1103,\n",
       "  1837,\n",
       "  1114,\n",
       "  4161,\n",
       "  1106,\n",
       "  1103,\n",
       "  27772,\n",
       "  1108,\n",
       "  4977,\n",
       "  1118,\n",
       "  170,\n",
       "  1748,\n",
       "  1815,\n",
       "  1154,\n",
       "  4366,\n",
       "  3441,\n",
       "  1104,\n",
       "  1103,\n",
       "  2199,\n",
       "  2603,\n",
       "  1113,\n",
       "  3617,\n",
       "  14304,\n",
       "  3300,\n",
       "  5233,\n",
       "  1116,\n",
       "  1106,\n",
       "  118,\n",
       "  121,\n",
       "  119,\n",
       "  3453,\n",
       "  1679,\n",
       "  9848,\n",
       "  1107,\n",
       "  1546,\n",
       "  1106,\n",
       "  4851,\n",
       "  12647,\n",
       "  1874,\n",
       "  26346,\n",
       "  16390,\n",
       "  1105,\n",
       "  1103,\n",
       "  3694,\n",
       "  20896,\n",
       "  1104,\n",
       "  15997,\n",
       "  2975,\n",
       "  119],\n",
       " [1706,\n",
       "  1129,\n",
       "  1316,\n",
       "  1113,\n",
       "  9170,\n",
       "  117,\n",
       "  123,\n",
       "  1364,\n",
       "  1410,\n",
       "  117,\n",
       "  1120,\n",
       "  1275,\n",
       "  119,\n",
       "  1405,\n",
       "  170,\n",
       "  119,\n",
       "  182,\n",
       "  119],\n",
       " [4352,\n",
       "  8235,\n",
       "  1121,\n",
       "  1139,\n",
       "  2384,\n",
       "  117,\n",
       "  146,\n",
       "  1138,\n",
       "  1103,\n",
       "  6565,\n",
       "  1106,\n",
       "  25337,\n",
       "  1303,\n",
       "  22922,\n",
       "  170,\n",
       "  2190,\n",
       "  4051,\n",
       "  1103,\n",
       "  2666,\n",
       "  1104,\n",
       "  5306,\n",
       "  8697,\n",
       "  9112,\n",
       "  117,\n",
       "  1259,\n",
       "  3746,\n",
       "  1482,\n",
       "  117,\n",
       "  1841,\n",
       "  1107,\n",
       "  25629,\n",
       "  1118,\n",
       "  9640,\n",
       "  2114,\n",
       "  1219,\n",
       "  1103,\n",
       "  1669,\n",
       "  1121,\n",
       "  1492,\n",
       "  1364,\n",
       "  1410,\n",
       "  1106,\n",
       "  128,\n",
       "  1318,\n",
       "  1410,\n",
       "  113,\n",
       "  1267,\n",
       "  1126,\n",
       "  24321,\n",
       "  114,\n",
       "  119],\n",
       " [1367,\n",
       "  119,\n",
       "  11336,\n",
       "  19385,\n",
       "  5430,\n",
       "  1115,\n",
       "  2833,\n",
       "  1105,\n",
       "  11659,\n",
       "  3552,\n",
       "  1118,\n",
       "  1103,\n",
       "  2341,\n",
       "  1106,\n",
       "  1138,\n",
       "  15079,\n",
       "  1103,\n",
       "  8939,\n",
       "  1104,\n",
       "  6021,\n",
       "  2459,\n",
       "  113,\n",
       "  1349,\n",
       "  114,\n",
       "  117,\n",
       "  1259,\n",
       "  1103,\n",
       "  1739,\n",
       "  9712,\n",
       "  6824,\n",
       "  2758,\n",
       "  117,\n",
       "  1137,\n",
       "  6842,\n",
       "  1639,\n",
       "  1107,\n",
       "  1833,\n",
       "  1177,\n",
       "  117,\n",
       "  1132,\n",
       "  2548,\n",
       "  1106,\n",
       "  7970,\n",
       "  117,\n",
       "  1105,\n",
       "  3697,\n",
       "  1115,\n",
       "  1142,\n",
       "  2075,\n",
       "  1343,\n",
       "  1150,\n",
       "  6043,\n",
       "  1107,\n",
       "  1103,\n",
       "  11574,\n",
       "  1104,\n",
       "  1103,\n",
       "  6661,\n",
       "  16020,\n",
       "  1105,\n",
       "  3201,\n",
       "  8214,\n",
       "  1107,\n",
       "  6021,\n",
       "  2459,\n",
       "  113,\n",
       "  1349,\n",
       "  114,\n",
       "  132],\n",
       " [1695,\n",
       "  119,\n",
       "  8704,\n",
       "  1116,\n",
       "  1103,\n",
       "  3268,\n",
       "  1104,\n",
       "  1103,\n",
       "  20914,\n",
       "  3912,\n",
       "  1106,\n",
       "  8680,\n",
       "  4381,\n",
       "  1940,\n",
       "  3202,\n",
       "  8661,\n",
       "  12736,\n",
       "  22356,\n",
       "  1223,\n",
       "  1103,\n",
       "  154,\n",
       "  3556,\n",
       "  2328,\n",
       "  8702,\n",
       "  6716,\n",
       "  1105,\n",
       "  117,\n",
       "  1107,\n",
       "  1142,\n",
       "  7328,\n",
       "  117,\n",
       "  17233,\n",
       "  1103,\n",
       "  20914,\n",
       "  3912,\n",
       "  1105,\n",
       "  3844,\n",
       "  1311,\n",
       "  1115,\n",
       "  1138,\n",
       "  7958,\n",
       "  6661,\n",
       "  23609,\n",
       "  1733,\n",
       "  27280,\n",
       "  1106,\n",
       "  22446,\n",
       "  2459,\n",
       "  113,\n",
       "  1349,\n",
       "  114,\n",
       "  1105,\n",
       "  2478,\n",
       "  113,\n",
       "  1349,\n",
       "  114,\n",
       "  1112,\n",
       "  5847,\n",
       "  1118,\n",
       "  6021,\n",
       "  1371,\n",
       "  113,\n",
       "  1349,\n",
       "  114,\n",
       "  1106,\n",
       "  27231,\n",
       "  1114,\n",
       "  1296,\n",
       "  1168,\n",
       "  4423,\n",
       "  3711,\n",
       "  1104,\n",
       "  1940,\n",
       "  3202,\n",
       "  8661,\n",
       "  12736,\n",
       "  22356,\n",
       "  4381,\n",
       "  1105,\n",
       "  2272,\n",
       "  2492,\n",
       "  1104,\n",
       "  5582,\n",
       "  132]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ISIL itself has published videos depicting people being subjected to a range of abhorrent punishments, including stoning, being pushed - off buildings, decapitation and crucifixion.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode = tokenizer.decode(id[1])\n",
    "decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7439th meeting, held on 11 May 2015.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.723 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "test_zh = load_data(test_cn_file, 'cn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 647. GiB for an array with shape (452561528,) and data type <U384",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_14462/1143556178.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mid_to_word\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mk\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mword_to_id\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mword_to_id\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mid_to_word\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0men_wtoi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0men_itow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_en\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;31m#zh_wtoi, zh_itow = build_dict(train_zh)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_14462/1143556178.py\u001B[0m in \u001B[0;36mbuild_dict\u001B[0;34m(sentences, max_words)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mbuild_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m50000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m     \u001B[0mvocab\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCounter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msentences\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmost_common\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmax_words\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m#最大单词数是50000\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m     \u001B[0mword_to_id\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mindex\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mw\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m     \u001B[0mword_to_id\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'UNK'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mUNK_IDX\u001B[0m  \u001B[0;31m#0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mMemoryError\u001B[0m: Unable to allocate 647. GiB for an array with shape (452561528,) and data type <U384"
     ]
    }
   ],
   "source": [
    "#构建词汇表\n",
    "def build_dict(sentences, max_words = 50000):\n",
    "\n",
    "    vocab = Counter(np.concatenate(sentences)).most_common(max_words)#最大单词数是50000\n",
    "    word_to_id = {w[0]: index + 2 for index, w in enumerate(vocab)}\n",
    "    word_to_id['UNK'] = UNK_IDX  #0\n",
    "    word_to_id['PAD'] = PAD_IDX  #1\n",
    "    id_to_word = {v: k for k, v in word_to_id.items()}\n",
    "    return word_to_id,id_to_word\n",
    "en_wtoi, en_itow = build_dict(train_en)\n",
    "#zh_wtoi, zh_itow = build_dict(train_zh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 到此为止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "en_itow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 利用词典对原始句子编码 单词->数字\n",
    "def encode(en_sentences, ch_sentences, en_wtoi, zh_wtoi, sort_by_len=True):\n",
    "\n",
    "\n",
    "    out_en_sentences = [[en_wtoi.get(w, UNK_IDX) for w in sent] for sent in en_sentences]\n",
    "    out_ch_sentences = [[zh_wtoi.get(w, UNK_IDX) for w in sent] for sent in ch_sentences]\n",
    "    #返回w对应的值，否则返回UNK_IDX\n",
    "    def len_argsort(seq):#按照长度进行排序\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_ch_sentences = [out_ch_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_ch_sentences\n",
    "\n",
    "train_en_encode, train_zh_encode = encode(train_en, train_zh, en_wtoi, zh_wtoi)\n",
    "#dev_en_encode, dev_zh_encode = encode(dev_en, dev_zh, en_wtoi, zh_wtoi)\n",
    "test_en_encode, test_zh_encode = encode(test_en, test_zh, en_wtoi, zh_wtoi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def encode(sentences, wtoi, sort_by_len=True):\n",
    "\n",
    "\n",
    "    out_sentences = [[wtoi.get(w, UNK_IDX) for w in sent] for sent in sentences]\n",
    "    #返回w对应的值，否则返回UNK_IDX\n",
    "    def len_argsort(seq):#按照长度进行排序\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "       \n",
    "    # 把中文和英文按照同样的顺序排序\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_en_sentences)\n",
    "        out_en_sentences = [out_en_sentences[i] for i in sorted_index]\n",
    "        out_ch_sentences = [out_ch_sentences[i] for i in sorted_index]\n",
    "        \n",
    "    return out_en_sentences, out_ch_sentences\n",
    "\n",
    "train_en_encode, train_zh_encode = encode(train_en, train_zh, en_wtoi, zh_wtoi)\n",
    "#dev_en_encode, dev_zh_encode = encode(dev_en, dev_zh, en_wtoi, zh_wtoi)\n",
    "test_en_encode, test_zh_encode = encode(test_en, test_zh, en_wtoi, zh_wtoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_en_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#返回每个batch的id\n",
    "def get_minibatches(n, minibatch_size, shuffle=True):\n",
    "    idx_list = np.arange(0, n, minibatch_size) \n",
    "    if shuffle:\n",
    "        np.random.shuffle(idx_list)\n",
    "    minibatches = []\n",
    "    for idx in idx_list:\n",
    "        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_minibatches(50, 10, shuffle=True)#得到每一个batch对应的id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#将句子对划分到batch\n",
    "def get_batches(en_encode, ch_encode):\n",
    "    batch_indexs = get_minibatches(len(en_encode),BATCH_SIZE)\n",
    "\n",
    "    batches = []\n",
    "    for batch_index in batch_indexs:\n",
    "        batch_en = [torch.tensor(en_encode[index]).long() for index in batch_index] #每一个idx对应的句子，转为tensor格式\n",
    "        batch_zh = [torch.tensor(ch_encode[index]).long() for index in batch_index]\n",
    "        length_en = torch.tensor([len(en)  for en in batch_en]).long()#每一个句子的长度\n",
    "        length_zh = torch.tensor([len(zh)  for zh in batch_zh]).long()\n",
    "\n",
    "        batch_en = pad_sequence(batch_en, padding_value=PAD_IDX, batch_first=True)#讲一个batch中的句子padding为相同长度\n",
    "        batch_zh = pad_sequence(batch_zh, padding_value=PAD_IDX, batch_first=True)\n",
    "\n",
    "        batches.append((batch_en, batch_zh,length_en,length_zh))\n",
    "    return batches\n",
    "train_data = get_batches(train_en_encode, train_zh_encode)\n",
    "dev_data = get_batches(dev_en_encode, dev_zh_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![](https://cdn.mathpix.com/snip/images/8Es5WLO-mn8kY7GO-T7o1IxWUBPbZeLrz3JbqY5U2Vw.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "其中 $\\bar{h}_{s}$ 表示encoder每个hidden_state的输出， $h_{t}$ 表示decoder每个hidden_state的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a_{t}(s) \n",
    "&=\\frac{\\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s}\\right)\\right)}{\\sum_{s^{\\prime}} \\exp \\left(\\operatorname{score}\\left(h_{t}, \\bar{h}_{s^{\\prime}}\\right)\\right)} \\\\\n",
    "c_{t} &= \\sum a_{t} \\bar{h}_{s} \\\\\n",
    "\\tilde{h}_{t} &=\\tanh \\left(W_{c}\\left[c_{t} ; h_{t}\\right]\\right)\\\\\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$\\operatorname{score}\\left(\\boldsymbol{h}_{t}, \\overline{\\boldsymbol{h}}_{s}\\right)= \\begin{cases}\\boldsymbol{h}_{t}^{\\top} \\overline{\\boldsymbol{h}}_{s} & \\text { dot } \\\\ \\boldsymbol{h}_{t}^{\\top} \\boldsymbol{W}_{\\boldsymbol{a}} \\overline{\\boldsymbol{h}}_{s} & \\text { general } \\\\ \\boldsymbol{v}_{a}^{\\top} \\tanh \\left(\\boldsymbol{W}_{\\boldsymbol{a}}\\left[\\boldsymbol{h}_{t} ; \\overline{\\boldsymbol{h}}_{s}\\right]\\right) & \\text { concat }\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LuongEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size,enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongEncoder,self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, bidirectional=True)#双向GRU\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size*2 , dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, x_lengths):\n",
    "        \"\"\"\n",
    "        input_seqs : batch_size,max(x_lengths)\n",
    "        input_lengths: batch_size\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(x))#batch_size,max(x_lengths),embed_size\n",
    "        packed = pack_padded_sequence(embedded, x_lengths.long().cpu().data.numpy(),batch_first=True,enforce_sorted=False)\n",
    "        #压缩填充张量,压缩掉无效的填充值\n",
    "        #enforce_sorted：如果是 True ，则输入应该是按长度降序排序的序列。如果是 False ，会在函数内部进行排序 \n",
    "        outputs, hidden = self.rnn(packed)\n",
    "        outputs, _ = pad_packed_sequence(outputs,padding_value =PAD_IDX,batch_first=True)#还原\n",
    "        \n",
    "        #hidden (2, batch_size, enc_hidden_size)\n",
    "        #outputs (batch_size,seq_len, 2 * enc_hidden_size)\n",
    "        \n",
    "        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1)\n",
    "        hidden = torch.tanh(self.fc(hidden)).unsqueeze(0)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, enc_hidden_size,dec_hidden_size):\n",
    "        super(Attn,self).__init__()\n",
    "        #general attention\n",
    "        self.linear_in = nn.Linear(enc_hidden_size*2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size*2 + dec_hidden_size, dec_hidden_size)\n",
    "\n",
    "\n",
    "    def forward(self, output, encoder_out,mask):\n",
    "        \"\"\"\n",
    "        output:batch_size, max(y_lengths), dec_hidden_size  #(h_t)\n",
    "        encoder_out:batch_size, max(x_lengths), 2 * enc_hidden_size  #(h_s)\n",
    "        \"\"\"\n",
    "        batch_size = output.shape[0]\n",
    "        output_len = output.shape[1]\n",
    "        input_len = encoder_out.shape[1]\n",
    "\n",
    "        encoder_out1 = self.linear_in(encoder_out.view(batch_size*input_len, -1)).view(batch_size, input_len, -1) \n",
    "        #Wh_s \n",
    "        #batch_size,max(x_lengths),dec_hidden_size\n",
    "        score = torch.bmm(output, encoder_out1.transpose(1,2))#实现三维数组的乘法，而不用拆成二维数组使用for循环解决\n",
    "        #[batch_size,max(y_lengths),dec_hidden_size] * [batch_size,dec_hidden_size,max(x_lengths)]\n",
    "        #batch_size,max(y_lengths),max(x_lengths)#score = h_t W h_s\n",
    "        score.data.masked_fill(mask,-1e16)\n",
    "        attn = F.softmax(score, dim=2) #attention系数矩阵\n",
    "\n",
    "        ct = torch.bmm(attn, encoder_out) #ct = aths\n",
    "        #[batch_size,max(y_lengths),max(x_lengths)] * [batch_size, max(x_lengths), 2 * enc_hidden_size]\n",
    "        #batch_size, max(y_lengths), enc_hidden_size*2\n",
    "        output = torch.cat((ct, output), dim=2) \n",
    "\n",
    "        output = output.view(batch_size*output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        #batch_size, max(y_lengths), dec_hidden_size\n",
    "\n",
    "        return output,attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class LuongDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout):\n",
    "        super(LuongDecoder,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embed_size)\n",
    "        self.attention = Attn(enc_hidden_size,dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size,dec_hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "    def creat_mask(self,x,y):\n",
    "\n",
    "        x_mask = x.data != PAD_IDX #batch_size,max(x_lengths)\n",
    "        y_mask = y.data != PAD_IDX #batch_size,max(y_lengths)\n",
    "        mask = (1 - (x_mask.unsqueeze(2) * y_mask.unsqueeze(1)).float()).bool()\n",
    "        #batch_size,max(x_lengths),max(y_lengths)\n",
    "        return mask \n",
    "    \n",
    "    def forward(self,encoder_out,x,y,y_lengths,hid):\n",
    "        mask = self.creat_mask(y,x)\n",
    "        y = self.dropout(self.embedding(y))\n",
    "        packed = pack_padded_sequence(y, y_lengths.long().cpu().data.numpy(),batch_first=True,enforce_sorted=False)\n",
    "        out, hid = self.rnn(packed,hid)\n",
    "       \n",
    "        out, _ = pad_packed_sequence(out,padding_value =PAD_IDX,batch_first=True)\n",
    "        \n",
    "        output,attn = self.attention(out,encoder_out,mask)\n",
    "        output = self.out(output)\n",
    "        #batch_size, max(y_lengths), vocab_size\n",
    "        return output,hid,attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class seq2seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self,x,x_lengths,y,y_lengths):\n",
    "        encoder_out,hid = self.encoder(x,x_lengths)\n",
    "        output,hid,attn = self.decoder(encoder_out,  #这里输出的hid是decoder_rnn的hid\n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    y_lengths=y_lengths,\n",
    "                    hid=hid)#encoder的hid\n",
    "        return output,attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=15):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for _ in range(max_length):\n",
    "            output,hid,attn = self.decoder(encoder_out, \n",
    "                    x=x,\n",
    "                    y=y,\n",
    "                    y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                    hid=hid)\n",
    "\n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "\n",
    "            preds.append(y)\n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1),torch.cat(attns, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Define Model\n",
    "encoder = LuongEncoder(vocab_size = len(en_itow), embed_size = EMBED_SIZE, enc_hidden_size = ENC_HIDDEN_SIZE, dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "decoder = LuongDecoder(vocab_size = len(zh_itow), embed_size = EMBED_SIZE, enc_hidden_size  = ENC_HIDDEN_SIZE, dec_hidden_size = DEC_HIDDEN_SIZE, dropout = DROPOUT)\n",
    "model = seq2seq(encoder, decoder)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)#忽略padding位置的损失\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer,train_data):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:,:-1]#将前seq-1个单词作为输入\n",
    "        y_output = y[:,1:]#将后seq-1个单词作为输出，相当于前一个单词预测后一个单词\n",
    "        y_lengths = (y_lengths-1).to(DEVICE)\n",
    "\n",
    "\n",
    "        logits, _ = model(x,x_lengths,y_input,y_lengths)#batch_size, max(y_lengths), vocab_size\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)\n",
    "\n",
    "\n",
    "def evaluate(model,dev_data):\n",
    "\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for x, y, x_lengths, y_lengths in train_data:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        x_lengths = x_lengths.to(DEVICE)\n",
    "\n",
    "        y_input = y[:,:-1]\n",
    "        y_output = y[:,1:]\n",
    "        y_lengths = (y_lengths-1).to(DEVICE)\n",
    "        logits, _ = model(x,x_lengths,y_input,y_lengths)\n",
    "        loss = loss_fn(logits.reshape(-1,logits.shape[-1]), y_output.reshape(-1))\n",
    "\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(model, optimizer,train_data)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model,dev_data)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def translate_dev(i):\n",
    "    model.eval()\n",
    "    \n",
    "    en_sent = \" \".join([en_itow[word] for word in test_en_encode[i]])\n",
    "    print('英文原句：',en_sent)\n",
    "    print('标准中文翻译：',\" \".join([zh_itow[word] for word in test_zh_encode[i]]))\n",
    "\n",
    "    bos = torch.Tensor([[zh_wtoi[\"BOS\"]]]).long().to(DEVICE)\n",
    "    x = torch.Tensor(test_en_encode[i]).long().to(DEVICE).reshape(1, -1)\n",
    "    x_len = torch.Tensor([len(test_en_encode[i])]).long().to(DEVICE)\n",
    "    \n",
    "    translation,_ = model.translate(x, x_len, bos)\n",
    "    translation = [zh_itow[i] for i in translation.data.cpu().numpy().reshape(-1)]\n",
    "\n",
    "    trans = []\n",
    "    for word in translation:\n",
    "        if word != \"EOS\":\n",
    "            trans.append(word)\n",
    "        else:\n",
    "            break\n",
    "    print('模型翻译结果：',\" \".join(trans))\n",
    "for i in range(50,100):\n",
    "    translate_dev(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 练习一：Bi-LSTM + attention 用于情感分类\n",
    "\n",
    "补全代码：我们使用构建一个Bi-LSTM + attention模型完成文本分类任务，数据使用IMDb电影评论数据集，检测一段文字的情感是正面还是负面。\n",
    "\n",
    "[论文](https://aclanthology.org/P16-2034.pdf)\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence \n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = IMDB(split='train')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "vocab.insert_token(\"<pad>\",1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x : 0 if x == 'neg' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def collate_batch(batch):#自定义的batch输出\n",
    "    label_list, text_list,lengths = [], [],[]\n",
    "    batch.sort(key=lambda x: len(text_pipeline(x[1])), reverse=True)#按照长度的大小进行排序\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text))\n",
    "        text_list.append(processed_text)\n",
    "        lengths.append(len(processed_text))\n",
    "    text_list = pad_sequence(text_list, padding_value=vocab.get_stoi()[\"<pad>\"], batch_first=True)#进行填充，每个batch中的句子需要有相同的长度\n",
    "    return torch.tensor(label_list), text_list, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_iter,test_iter = IMDB(root='data', split=('train', 'test'))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ =  random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=128,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=128,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=128,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[0]#标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[1]#batch_size,max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data[2]#lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "模型分为五个部分\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/pvB4-X5G9OAFQ_A2wYqjCZxoOOMu_u1PpkkrJUlTbQ8.original.fullsize.png)\n",
    "\n",
    "- 输入层（Input layer）：将句子输入模型\n",
    "- 嵌入层（Embedding layer）：将每个词映射到一个低维向量\n",
    "- LSTM层（LSTM layer）：利用BiLSTM从词向量中获得特征\n",
    "- Attention层（Attention layer）：生成权重向量，将每个时间步长的单词级特征与权重向量相乘，合并成句子级特征向量（补全代码）\n",
    "- 输出层（Output layer）： 对句子进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class bilstm_attn(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "                 dropout_rate, pad_id):\n",
    "        super(bilstm_attn,self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = True,\n",
    "                            dropout=dropout_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \"\"\"\n",
    "            Write your code here.\n",
    "        \"\"\"\n",
    "        \n",
    "    def forward(self, x,lengths):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(x))# [batch size,seq len] -> [batch size,seq len,embedding_dim]\n",
    "\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden = [n layers *2, batch size, hidden dim]最后一个step的hidden\n",
    "        # cell = [n layers * 2, batch size, hidden dim]最终一个step的cell\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)\n",
    "        # output = [batch size, seq len, hidden dim * 2]#每一个step下的最后一层的output\n",
    "        output = output.reshape(output.shape[0],output.shape[1],2,-1)\n",
    "        # output = [batch size, seq len, 2,hidden dim]\n",
    "        output =  torch.sum(output, dim=2)\n",
    "        # output = [batch size, seq len, hidden dim]\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        \"\"\"\n",
    "            Write your code here.\n",
    "        \"\"\"\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = 2\n",
    "n_layers = 1\n",
    "dropout_rate = 0.5\n",
    "pad_id = vocab.get_stoi()[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = bilstm_attn(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,dropout_rate, pad_id).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, loss_fn):\n",
    "    \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "    model.train() #model.train()代表了训练模式    \n",
    "    for label, text, lengths in train_loader: \n",
    "        label = label.to(device)\n",
    "        text = text.to(device)  \n",
    "        \n",
    "        out = model(text,lengths)\n",
    "        loss = loss_fn(out,label)\n",
    "        \n",
    "        _,pred = torch.max(out.data,1)       \n",
    "        corrects += (pred == label).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad() #加这步防止梯度叠加\n",
    "        loss.backward() #反向传播\n",
    "        optimizer.step() #梯度下降\n",
    "        \n",
    "        epoch_loss += loss.item() * len(label)\n",
    "        #loss.item()已经本身除以了len(batch.label)\n",
    "        #所以得再乘一次，得到一个batch的损失，累加得到所有样本损失。\n",
    "\n",
    "        \n",
    "        total_len += len(label)\n",
    "        #计算train_iterator所有样本的数量，不出意外应该是17500\n",
    "        \n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, valid_loader):\n",
    "     \n",
    "    \n",
    "    epoch_loss = 0\n",
    "    corrects = 0\n",
    "    total_len = 0\n",
    "    \n",
    "    model.eval()\n",
    "    #转换成测试模式，冻结dropout层或其他层。\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for label, text, lengths in valid_loader: \n",
    "            #iterator为valid_iterator\n",
    "            label = label.to(device)\n",
    "            text = text.to(device) \n",
    "            \n",
    "            out = model(text,lengths)\n",
    "            loss = loss_fn(out,label)\n",
    "\n",
    "            _,pred = torch.max(out.data,1)       \n",
    "            corrects += (pred == label).sum().item()\n",
    "            \n",
    "            \n",
    "            epoch_loss += loss.item() * len(label)\n",
    "            total_len += len(label)\n",
    "    model.train() #调回训练模式   \n",
    "    \n",
    "    return epoch_loss / total_len, corrects / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, loss_fn)\n",
    "    print(\"epoch:\",epoch,\"train_loss:\",train_loss,\"train_acc\",train_acc)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_dataloader)  \n",
    "    print(\"epoch:\",epoch,\"valid_loss:\",valid_loss,\"valid_acc\",valid_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'bilstm_attn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    text = text_pipeline(text)\n",
    "    \n",
    "    length = torch.LongTensor([len(text)])\n",
    "    tensor = torch.LongTensor(text).unsqueeze(0).to(device)\n",
    "    \n",
    "    out = model(tensor, length)\n",
    "    _,pred = torch.max(out.data,1)\n",
    "    return pred.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(\"This film is terrible\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(\"This film is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}