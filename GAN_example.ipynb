{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# DCGAN Tutorial\n",
    "\n",
    "**参考**https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "使用DCGAN生成人脸，人脸数据来自于[celeba—A](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#首先导入需要的模块\n",
    "#%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataroot = \"data/celeba\"  #图片路径\n",
    "workers = 2\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "\n",
    "#number of color channels in the input images. For color images this is 3\n",
    "nc = 3\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "num_epochs = 10\n",
    "lr = 0.0002\n",
    "beta1 = 0.5  # Beta1 hyperparam for Adam optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "数据集为[celeba—A](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)人脸数据，只使用了其中的2万张人脸。使用torchvision.datasets.ImageFolder加载数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/celeba'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_6599/2928486148.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m#使用ImageFolder加载图片\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m dataset = torchvision.datasets.ImageFolder(root = dataroot,\n\u001B[0m\u001B[1;32m      3\u001B[0m                                            transform = transforms.Compose([\n\u001B[1;32m      4\u001B[0m                                                \u001B[0mtransforms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mResize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m                                                \u001B[0mtransforms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCenterCrop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001B[0m\n\u001B[1;32m    308\u001B[0m         \u001B[0mis_valid_file\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mCallable\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    309\u001B[0m     ):\n\u001B[0;32m--> 310\u001B[0;31m         super().__init__(\n\u001B[0m\u001B[1;32m    311\u001B[0m             \u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    312\u001B[0m             \u001B[0mloader\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001B[0m\n\u001B[1;32m    143\u001B[0m     ) -> None:\n\u001B[1;32m    144\u001B[0m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtransform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_transform\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtarget_transform\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 145\u001B[0;31m         \u001B[0mclasses\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_to_idx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfind_classes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    146\u001B[0m         \u001B[0msamples\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mroot\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_to_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mextensions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mis_valid_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    147\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001B[0m in \u001B[0;36mfind_classes\u001B[0;34m(self, directory)\u001B[0m\n\u001B[1;32m    217\u001B[0m             \u001B[0;34m(\u001B[0m\u001B[0mTuple\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mList\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mDict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mList\u001B[0m \u001B[0mof\u001B[0m \u001B[0mall\u001B[0m \u001B[0mclasses\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mdictionary\u001B[0m \u001B[0mmapping\u001B[0m \u001B[0meach\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0mto\u001B[0m \u001B[0man\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    218\u001B[0m         \"\"\"\n\u001B[0;32m--> 219\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfind_classes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdirectory\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    220\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mTuple\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/miniconda3/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001B[0m in \u001B[0;36mfind_classes\u001B[0;34m(directory)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0mSee\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;32mclass\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0mDatasetFolder\u001B[0m\u001B[0;31m`\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mdetails\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \"\"\"\n\u001B[0;32m---> 41\u001B[0;31m     \u001B[0mclasses\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mentry\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mentry\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscandir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdirectory\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mentry\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_dir\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mclasses\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mFileNotFoundError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"Couldn't find any class folder in {directory}.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/celeba'"
     ]
    }
   ],
   "source": [
    "#使用ImageFolder加载图片\n",
    "dataset = torchvision.datasets.ImageFolder(root = dataroot,\n",
    "                                           transform = transforms.Compose([\n",
    "                                               transforms.Resize(image_size),\n",
    "                                               transforms.CenterCrop(image_size),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                                           ]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batch_size,\n",
    "                                         shuffle = True, num_workers = workers)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize = (8, 8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0][:64], padding = 2, normalize = True), (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 参数初始化 \n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Generator\n",
    "\n",
    "![](https://cdn.mathpix.com/snip/images/9xvZOv3tUKiFczpUOKVALxnKMI9OsnAILTEXGxTC_UE.original.fullsize.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "#number of color channels in the input images. For color images this is 3\n",
    "#nc = 3\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "#nz = 100\n",
    "# Size of feature maps in generator\n",
    "#ngf = 64\n",
    "# Size of feature maps in discriminator\n",
    "#ndf = 64\n",
    "\n",
    "#torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n",
    "#H_out =(H_in −1)×stride[0]−2×padding[0]+dilation[0]×(kernel_size[0]−1)+output_padding[0]+1\n",
    "\n",
    "\n",
    "#H_out = (H_in - 1) *stride -2*padding + kernel_size\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Z:batch_size. nz x 1 x 1 \n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),  #ReLU激活函数\n",
    "            # batch_size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # batch_size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # batch_size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # batch_size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()  #Tanh激活函数\n",
    "            #batch_size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "netG = Generator().to(device)\n",
    "netG.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            #batch_size,(nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),  #LeakyReLU激活函数\n",
    "            #batch_size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            # batch_size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            # batch_size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            # batch_size (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias = False),\n",
    "            #batch_size ,1,1,1\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "netD.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize BCELoss function交叉熵损失函数\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device = device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.\n",
    "fake_label = 0.\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas = (beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas = (beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "BCELoss——交叉熵损失函数\n",
    "\n",
    "$L\\left(\\hat{y}_{i}, y_{i}\\right)=-\\left[y_{i} \\log \\left(\\hat{y}_{i}\\right)+\\left(1-y_{i}\\right) \\log \\left(1-\\hat{y}_{i}\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Part 1 - 训练Discriminator\n",
    "\n",
    "最大化$[log(D(x))+log(1−D(G(z)))]$\n",
    "\n",
    "- 从训练集中选择一组真实的样本，喂入D，计算损失。\n",
    "- 使用G生成一组假样本，喂入D，计算损失。\n",
    "\n",
    "\n",
    "Part 2 - 训练Generator\n",
    "\n",
    "最大化$log(D(G(z))) $\n",
    "\n",
    "使用D对G生成的假样本进行分类（这些样本赋予real_label），计算损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []  #储存图片\n",
    "G_losses = []  #G损失\n",
    "D_losses = []  #D损失\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()  #清空梯度\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)  #真实的一组图片\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, dtype = torch.float, device = device)  #真实的标签 1\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device = device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)  #G(z)\n",
    "        label.fill_(fake_label)  #假的图片 0\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch, accumulated (summed) with previous gradients\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs - 1) and (i == len(dataloader) - 1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(torchvision.utils.make_grid(fake, padding = 2, normalize = True))\n",
    "\n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses, label = \"G\")\n",
    "plt.plot(D_losses, label = \"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8, 8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated = True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval = 1000, repeat_delay = 1000, blit = True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%capture\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Real Images vs. Fake Images**\n",
    "\n",
    "Finally, lets take a look at some real images and fake images side by\n",
    "side.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64], padding = 5, normalize = True).cpu(),\n",
    "                        (1, 2, 0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 练习二\n",
    "\n",
    "基于GAN的手写数字生成，补全代码，自己编写生成器和判别器网络架构（例如，多层感知机），实现手写数字的生成。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(128, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 784),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "def show_images(images):\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(sqrtn, sqrtn, index + 1)\n",
    "        plt.imshow(image.reshape(28, 28))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def d_loss_function(inputs, targets):\n",
    "    return nn.BCELoss()(inputs, targets)\n",
    "\n",
    "\n",
    "def g_loss_function(inputs):\n",
    "    targets = torch.ones([inputs.shape[0], 1])\n",
    "    targets = targets.to(device)\n",
    "    return nn.BCELoss()(inputs, targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n",
    "\n",
    "# Model\n",
    "G = generator().to(device)\n",
    "D = discriminator().to(device)\n",
    "print(G)\n",
    "print(D)\n",
    "\n",
    "# Settings\n",
    "epochs = 20\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "g_optimizer = optim.Adam(G.parameters(), lr = lr, betas = (0.5, 0.999))\n",
    "d_optimizer = optim.Adam(D.parameters(), lr = lr, betas = (0.5, 0.999))\n",
    "\n",
    "# Transform\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load data\n",
    "train_set = datasets.MNIST('data/', train = True, download = True, transform = transform)\n",
    "test_set = datasets.MNIST('data/', train = False, download = True, transform = transform)\n",
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = batch_size, shuffle = False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "\n",
    "    for times, data in enumerate(train_loader):\n",
    "        times += 1\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # Zero the parameter gradients\n",
    "        d_optimizer.zero_grad()\n",
    "        real_inputs = data[0].to(device)\n",
    "\n",
    "        real_inputs = real_inputs.view(-1, 784)\n",
    "        real_outputs = D(real_inputs)\n",
    "        real_label = torch.ones(real_inputs.shape[0], 1).to(device)\n",
    "\n",
    "        noise = (torch.rand(real_inputs.shape[0], 128) - 0.5) / 0.5\n",
    "        noise = noise.to(device)\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs.detach())\n",
    "        fake_label = torch.zeros(fake_inputs.shape[0], 1).to(device)\n",
    "\n",
    "        outputs = torch.cat((real_outputs, fake_outputs), 0)\n",
    "        targets = torch.cat((real_label, fake_label), 0)\n",
    "\n",
    "        # Backward propagation\n",
    "        d_loss = d_loss_function(outputs, targets)\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Generator\n",
    "\n",
    "        fake_inputs = G(noise)\n",
    "        fake_outputs = D(fake_inputs)\n",
    "\n",
    "        g_loss = g_loss_function(fake_outputs)\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if times % 100 == 0 or times == len(train_loader):\n",
    "            print('[{}/{}, {}/{}] D_loss: {:.3f} G_loss: {:.3f}'.format(epoch, epochs, times, len(train_loader),\n",
    "                                                                        d_loss.item(), g_loss.item()))\n",
    "\n",
    "    imgs_numpy = (fake_inputs.data.cpu().numpy() + 1.0) / 2.0\n",
    "    show_images(imgs_numpy[:16])\n",
    "    plt.show()\n",
    "\n",
    "print('Training Finished.')\n",
    "print('Cost Time: {}s'.format(time.time() - start_time))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "def show_images(images):\n",
    "    sqrtn = int(np.ceil(np.sqrt(images.shape[0])))\n",
    "\n",
    "    for index, image in enumerate(images):\n",
    "        plt.subplot(sqrtn, sqrtn, index + 1)\n",
    "        plt.imshow(image.reshape(28, 28))\n",
    "\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print('GPU State:', device)\n",
    "\n",
    "# Model\n",
    "G.eval()\n",
    "\n",
    "# Generator\n",
    "noise = (torch.rand(16, 128) - 0.5) / 0.5\n",
    "noise = noise.to(device)\n",
    "\n",
    "fake_image = G(noise)\n",
    "imgs_numpy = (fake_image.data.cpu().numpy() + 1.0) / 2.0\n",
    "show_images(imgs_numpy)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}